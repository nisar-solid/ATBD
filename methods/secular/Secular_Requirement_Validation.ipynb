{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow to Validate NISAR L2 Secular Displacement Requirement\n",
    "\n",
    "**Original code authored by:** David Bekaert, Heresh Fattahi, Eric Fielding, and Zhang Yunjun \n",
    "\n",
    "Extensive modifications by Adrian Borsa and Amy Whetter and other NISAR team members 2022\n",
    "\n",
    "Reorganized and modified by Ekaterina Tymofyeyeva, March 2024\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Both the initial setup (<b>Prep A</b> section) and download of the data (<b>Prep B</b> section) should be run at the start of the notebook. And all subsequent sections NEED to be run in order.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define CalVal Site and directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose a site and track direction\n",
    "site='MojaveD173' \n",
    "\n",
    "# Define your directory structure - you won't need to change this line\n",
    "start_directory = '/scratch/nisar-st-calval-solidearth' \n",
    "\n",
    "# What dataset are you processing?\n",
    "dataset = 'ARIA_S1' # For Sentinel-1 testing with aria-tools\n",
    "\n",
    "# The date and version of this Cal/Val run\n",
    "today = '20240429'\n",
    "version = '1'\n",
    "\n",
    "# The file where you keep your customized list of sites.\n",
    "custom_sites = '/home/jovyan/my_sites.txt'\n",
    "\n",
    "# Please enter a name or username that will determine where your outputs are stored\n",
    "import os\n",
    "if os.path.exists('/home/jovyan/me.txt'): # if OpenTopo API key already installed\n",
    "    with open('/home/jovyan/me.txt') as m:\n",
    "        you = m.readline().strip()\n",
    "    print('You are', you)\n",
    "    print('Using this as the name of the directory where your outputs will be stored.')\n",
    "    print('Directory structure: start_directory / dataset/ requirement / site / you / today / version ')\n",
    "\n",
    "else:\n",
    "    print('We need a name or username (determines where your outputs will be stored)')\n",
    "    print('Directory structure: start_directory / dataset/ requirement / site / you / today / version ')\n",
    "\n",
    "    you = input('Please type your name:')\n",
    "    with open ('/home/jovyan/me.txt', 'w') as m: \n",
    "        m.write(you)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of Contents:\n",
    "<a id='secular_TOC'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "[**1. Generation of Time Series from Interferograms**](#secular_gen_ts)\n",
    "- [1.1. Validate/Modify Interferogram Network](#secular_validate_network)\n",
    "- [1.2. Generate Quality Control Mask](#secular_generate_mask)\n",
    "- [1.3. Reference Interferograms To Common Lat/Lon](#secular_common_latlon)\n",
    "- [1.4. Invert for SBAS Line-of-Sight Timeseries](#secular_invert_SBAS)\n",
    "\n",
    "[**2. Optional Corrections**](#secular_opt_correction)\n",
    "- [2.1. Solid Earth Tide Correction](#secular_solid_earth)\n",
    "- [2.2. Tropospheric Delay Correction](#secular_tropo_corr)\n",
    "- [2.3. Phase Deramping ](#secular_phase_deramp)\n",
    "- [2.4. Topographic Residual Correction ](#secular_topo_corr) \n",
    "\n",
    "[**3. Estimate InSAR and GNSS Velocities**](#secular_decomp_ts)\n",
    "- [3.1. Estimate InSAR LOS Velocities](#secular_insar_vel1)\n",
    "- [3.2. Find Collocated GNSS Stations](#secular_co_gps)  \n",
    "- [3.3. Get GNSS Position Time Series](#secular_gps_ts) \n",
    "- [3.4. Make GNSS LOS Velocities](#secular_gps_los)\n",
    "- [3.5. Re-Reference GNSS and InSAR Velocities](#secular_gps_insar)\n",
    "\n",
    "[**4. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison**](#secular_nisar_validation)\n",
    "- [4.1. Make Velocity Residuals at GNSS Locations](#secular_make_vel)\n",
    "- [4.2. Make Double-differenced Velocity Residuals](#secular_make_velres)\n",
    "- [4.3. Secular Requirement Validation: Method 1](#secular_valid_method1)\n",
    "\n",
    "[**5. NISAR Validation Approach 2: InSAR-only Structure Function**](#secular_nisar_validation2)\n",
    "- [5.1. Read Array and Mask Pixels with no Data](#secular_array_mask)\n",
    "- [5.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend](#secular_remove_trend)\n",
    "- [5.3. Amplitude vs. Distance of Relative Measurements (pair differences)](#secular_M2ampvsdist2)\n",
    "- [5.4. Bin Sample Pairs by Distance Bin and Calculate Statistics](#secular_M2RelMeasTable)\n",
    "\n",
    "[**Appendix: Supplementary Comparisons and Plots**](#secular_appendix1)\n",
    "- [A.1. Compare Raw Velocities](#secular_compare_raw)\n",
    "- [A.2. Plot Velocity Residuals](#secular_plot_vel)\n",
    "- [A.3. Plot Double-differenced Residuals](#secular_plot_velres)\n",
    "- [A.4. GPS Position Plot](#secular_appendix_gps)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################# You should not need to change any code in this cell ##########################################\n",
    "\n",
    "#Load Packages\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from mintpy.cli import view, plot_network\n",
    "from mintpy.objects import gnss, timeseries\n",
    "from mintpy.smallbaselineApp import TimeSeriesAnalysis\n",
    "from mintpy.utils import ptime, readfile, utils as ut\n",
    "from scipy import signal\n",
    "\n",
    "from solid_utils.sampling import load_geo, samp_pair, profile_samples, haversine_distance\n",
    "from solid_utils.plotting import display_validation, display_validation_table\n",
    "\n",
    "#Set Global Plot Parameters\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "################# Set Directories ##########################################\n",
    "requirement = 'Secular'\n",
    "work_dir = os.path.join(start_directory,dataset,requirement,site,you,today,'v'+version)\n",
    "print(\"Work directory:\", work_dir)\n",
    "\n",
    "gunw_dir = os.path.join(work_dir,'products')\n",
    "print(\"   GUNW    dir:\", gunw_dir) \n",
    "\n",
    "mintpy_dir = os.path.join(work_dir,'MintPy')\n",
    "print(\"   MintPy  dir:\", mintpy_dir)\n",
    "\n",
    "### Change to MintPy workdir\n",
    "if not os.path.exists(mintpy_dir):\n",
    "    print()\n",
    "    print('ERROR: Stop! Your MintPy processing directory does not exist for this requirement, site, version, or date of your ATBD run.')\n",
    "    print('You may need to run the prep notebook first!')\n",
    "    print()\n",
    "else:\n",
    "    os.chdir(mintpy_dir)\n",
    "\n",
    "vel_file = os.path.join(mintpy_dir, 'velocity.h5')\n",
    "msk_file = os.path.join(mintpy_dir, 'maskConnComp.h5')  # maskTempCoh.h5 maskConnComp.h5\n",
    "\n",
    "with open(custom_sites,'r') as fid:\n",
    "    sitedata = json.load(fid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Generation of Time Series from Interferograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_validate_network'></a>\n",
    "## 1.1. Validate/Modify Interferogram Network\n",
    "\n",
    "Add additional parameters to config_file in order to remove selected interferograms, change minimum coherence, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_file = os.path.join(mintpy_dir,site + '.cfg')\n",
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep modify_network'\n",
    "process = subprocess.run(command, shell=True)\n",
    "\n",
    "plot_network.main(['inputs/ifgramStack.h5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_generate_mask'></a>\n",
    "## 1.2. Generate Quality Control Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask files can be can be used to mask pixels in the time-series processing. Below we generate a mask file based on the connected components, which is a metric for unwrapping quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command='generate_mask.py inputs/ifgramStack.h5  --nonzero  -o maskConnComp.h5  --update'\n",
    "process = subprocess.run(command, shell=True)\n",
    "view.main(['maskConnComp.h5', 'mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_common_latlon'></a>\n",
    "## 1.3. Reference Interferograms To Common Lat/Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep reference_point'\n",
    "process = subprocess.run(command, shell=True)\n",
    "os.system('info.py inputs/ifgramStack.h5 | egrep \"REF_\"');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_invert_SBAS'></a>\n",
    "## 1.4. Invert for SBAS Line-of-Sight Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep invert_network'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_opt_correction'></a>\n",
    "# 2. Optional Corrections\n",
    "\n",
    "Phase distortions related to solid earth and ocean tidal effects, as well as those due to temporal variations in the vertical stratification of the atmosphere, can be mitigated using the approaches described below. At this point, it is expected that these corrections will not be needed to validate the mission requirements, but they may be used to produce the highest quality data products. Typically, these are applied to the estimated time series product rather than to the individual interferograms, since they are a function of the time of each radar acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_solid_earth'></a>\n",
    "## 2.1. Solid Earth Tide Correction\n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_tropo_corr'></a>\n",
    "## 2.2. Tropospheric Delay Correction\n",
    "\n",
    "Optional atmospheric correction utilizes the PyAPS (Jolivet et al., 2011, Jolivet and Agram, 2012) module within GIAnT (or eventually a merged replacement for GIAnT and MintPy). PyAPS is well documented, maintained and can be freely downloaded. PyAPS is included in GIAnT distribution). PyAPS currently includes support for ECMWF’s ERA-Interim, NOAA’s NARR and NASA’s MERRA weather models. A final selection of atmospheric models to be used for operational NISAR processing will be done during Phase C.\n",
    "\n",
    "[T]ropospheric delay maps are produced from atmospheric data provided by Global Atmospheric Models. This method aims to correct differential atmospheric delay correlated with the topography in interferometric phase measurements. Global Atmospheric Models (hereafter GAMs)... provide estimates of the air temperature, the atmospheric pressure and the humidity as a function of elevation on a coarse resolution latitude/longitude grid. In PyAPS, we use this 3D distribution of atmospheric variables to determine the atmospheric phase delay on each pixel of each interferogram.\n",
    "\n",
    "The absolute atmospheric delay is computed at each SAR acquisition date. For a pixel a_i at an elevation z at acquisition date i, the four surrounding grid points are selected and the delays for their respective elevations are computed. The resulting delay at the pixel a_i is then the bilinear interpolation between the delays at the four grid points. Finally, we combine the absolute delay maps of the InSAR partner images to produce the differential delay maps used to correct the interferograms.\n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "do_tropo_correction = False\n",
    "########################################################################\n",
    "'''\n",
    "REFERENCE : https://github.com/insarlab/pyaps#2-account-setup-for-era5\n",
    "Read Section 2 for ERA5 [link above] to create an account on the CDS website.\n",
    "'''\n",
    "\n",
    "if do_tropo_correction:\n",
    "    if not Use_Staged_Data and not os.path.exists(Path.home()/'.cdsapirc'):\n",
    "        print('NEEDED to download ERA5, link: https://cds.climate.copernicus.eu/user/register')\n",
    "        UID = input('Please type your CDS_UID:')\n",
    "        CDS_API = input('Please type your CDS_API:')\n",
    "        \n",
    "        cds_tmp = '''url: https://cds.climate.copernicus.eu/api/v2\n",
    "        key: {UID}:{CDS_API}'''.format(UID=UID, CDS_API=CDS_API)\n",
    "        os.system('echo \"{cds_tmp}\" > ~/.cdsapirc; chmod 600 ~/.cdsapirc'.format(cds_tmp = str(cds_tmp)))\n",
    "    \n",
    "    command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep correct_troposphere'\n",
    "    process = subprocess.run(command, shell=True)\n",
    "    \n",
    "    view.main(['inputs/ERA5.h5'])\n",
    "    timeseries_filename = 'timeseries_ERA5.h5'\n",
    "else:\n",
    "    timeseries_filename = 'timeseries.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_phase_deramp'></a>\n",
    "## 2.3. Phase Deramping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep deramp'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_topo_corr'></a>\n",
    "## 2.4. Topographic Residual Correction \n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep correct_topography'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_decomp_ts'></a>\n",
    "# 3. Estimate InSAR and GNSS Velocities\n",
    "The approach that will be used for the generation of NISAR L3 products for Requirements 660 and 663 allows for an explicit inclusion of key basis functions (e.g., Heaviside functions, secular rate, etc.) in the InSAR inversion. Modifications to this algorithm may be identified and implemented in response to NISAR Phase C activities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_insar_vel1'></a>\n",
    "## 3.1. Estimate InSAR LOS Velocities\n",
    "\n",
    "Given a time series of InSAR LOS displacements, the observations for a given pixel, $U(t)$, can be parameterized as:\n",
    "\n",
    "$$U(t) = a \\;+\\; vt \\;+\\; c_1 cos (\\omega_1t - \\phi_{1,}) \\;+\\; c_2 cos (\\omega_2t - \\phi_2) \\;+\\; \\sum_{j=1}^{N_{eq}} \\left( h_j+f_j F_j (t-t_j) \\right)H(t - t_j) \\;+\\; \\frac{B_\\perp (t)}{R sin \\theta}\\delta z \\;+\\; residual$$ \n",
    "\n",
    "which includes a constant offset $(a)$, velocity $(v)$, and amplitudes $(c_j)$ and phases $(\\phi_j)$ of annual $(\\omega_1)$ and semiannual $(\\omega_2)$ sinusoidal terms.  Where needed we can include additional complexity, such as coseismic and postseismic processes parameterized by Heaviside (step) functions $H$ and postseismic functions $F$ (the latter typically exponential and/or logarithmic).   $B_\\perp(t)$, $R$, $\\theta$, and $\\delta z$ are, respectively, the perpendicular component of the interferometric baseline relative to the first date, slant range distance, incidence angle and topography error correction for the given pixel. \n",
    "\n",
    "Thus, given either an ensemble of interferograms or the output of SBAS (displacement vs. time), we can write the LSQ problem as \n",
    "\n",
    "$$ \\textbf{G}\\textbf{m} = \\textbf{d}$$\n",
    "\n",
    "where $\\textbf{G}$ is the design matrix (constructed out of the different functional terms in Equation 2 evaluated either at the SAR image dates for SBAS output, or between the dates spanned by each pair for interferograms), $\\textbf{m}$ is the vector of model parameters (the coefficients in Equation 2) and $\\textbf{d}$ is the vector of observations.  For GPS time series, $\\textbf{G}, \\textbf{d}, \\textbf{m}$ are constructed using values evaluated at single epochs corresponding to the GPS solution times, as for SBAS InSAR input. \n",
    "\n",
    "With this formulation, we can obtain InSAR velocity estimates and their formal uncertainties (including in areas where the expected answer is zero). \n",
    "\n",
    "The default InSAR velocity fit in MintPy is to estimate a mean linear velocity $(v)$ in in the equation, which we do below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep velocity'\n",
    "process = subprocess.run(command, shell=True)\n",
    "\n",
    "# load velocity file\n",
    "insar_velocities,_ = readfile.read(vel_file, datasetName = 'velocity')  # read velocity file\n",
    "insar_velocities = insar_velocities * 1000.  # convert velocities from m to mm\n",
    "\n",
    "# set masked pixels to NaN\n",
    "msk,_ = readfile.read(msk_file)\n",
    "insar_velocities[msk == 0] = np.nan\n",
    "insar_velocities[insar_velocities == 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the mean linear velocity fit. The MintPy `view` module automatically reads the temporal coherence mask `maskTempCoh.h5` and applies that to mask out pixels with unreliable velocities (white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scp_args = 'velocity.h5 velocity -v -25 25 --colormap RdBu_r --figtitle LOS_Velocity --unit mm/yr -m ' + msk_file\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Note :</b> \n",
    "Negative values indicates that target is moving away from the radar (i.e., Subsidence in case of vertical deformation).\n",
    "Positive values indicates that target is moving towards the radar (i.e., uplift in case of vertical deformation). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_co_gps'></a>\n",
    "## 3.2. Find Collocated GNSS Stations\n",
    "\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get analysis metadata from InSAR velocity file\n",
    "insar_metadata = readfile.read_attribute(vel_file)\n",
    "lat_step = float(insar_metadata['Y_STEP'])\n",
    "lon_step = float(insar_metadata['X_STEP'])\n",
    "(S,N,W,E) = ut.four_corners(insar_metadata)\n",
    "start_date = insar_metadata.get('START_DATE', None).split('T')[0]\n",
    "end_date = insar_metadata.get('END_DATE', None).split('T')[0]\n",
    "start_date_gnss = dt.strptime(start_date, \"%Y%m%d\")\n",
    "end_date_gnss = dt.strptime(end_date, \"%Y%m%d\")\n",
    "#inc_angle = int(float(insar_metadata.get('incidenceAngle', None)))\n",
    "#az_angle = int(float(insar_metadata.get('azimuthAngle', None)))\n",
    "# Just testing!\n",
    "inc_angle = 32.5\n",
    "az_angle = 0.0\n",
    "\n",
    "#Set GNSS Parameters\n",
    "gnss_completeness_threshold = 0.9    #0.9  #percent of data timespan with valid GNSS epochs\n",
    "gnss_residual_stdev_threshold = 10.  #0.03  #0.03  #max threshold standard deviation of residuals to linear GNSS fit\n",
    "\n",
    "# search for collocated GNSS stations\n",
    "site_names, site_lats, site_lons = gnss.search_gnss(SNWE=(S,N,W,E), start_date=start_date, end_date=end_date)\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "insar_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gps_ts'></a>\n",
    "## 3.3. Get GNSS Position Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get daily position solutions for GNSS stations\n",
    "use_stn = []  #stations to keep\n",
    "bad_stn = []  #stations to toss\n",
    "use_lats = [] \n",
    "use_lons = []\n",
    "\n",
    "for counter, stn in enumerate(site_names):\n",
    "    gnss_obj = gnss.get_gnss_class('UNR')(site = stn, data_dir = os.path.join(mintpy_dir,'GNSS'))\n",
    "    gnss_obj.open(print_msg=False)\n",
    "    \n",
    "    # count number of dates in time range\n",
    "    dates = gnss_obj.dates\n",
    "    range_days = (end_date_gnss - start_date_gnss).days\n",
    "    gnss_count = np.histogram(dates, bins=[start_date_gnss, end_date_gnss])\n",
    "    gnss_count = int(gnss_count[0])\n",
    "    \n",
    "    # for this quick screening check of data quality, we use the constant incidence and azimuth angles \n",
    "    # get standard deviation of residuals to linear fit\n",
    "    disp_los = ut.enu2los(gnss_obj.dis_e, gnss_obj.dis_n, gnss_obj.dis_u, inc_angle, az_angle)\n",
    "    disp_detrended = signal.detrend(disp_los)\n",
    "    stn_stdv = np.std(disp_detrended)\n",
    "   \n",
    "    # select GNSS stations based on data completeness and scatter of residuals\n",
    "    disp_detrended = signal.detrend(disp_los)\n",
    "    if range_days * gnss_completeness_threshold <= gnss_count:\n",
    "        if stn_stdv > gnss_residual_stdev_threshold:\n",
    "            bad_stn.append(stn)\n",
    "        else:\n",
    "            use_stn.append(stn)\n",
    "            use_lats.append(site_lats[counter])\n",
    "            use_lons.append(site_lons[counter])\n",
    "    else:\n",
    "        bad_stn.append(stn)\n",
    "\n",
    "site_names = use_stn\n",
    "site_lats = use_lats\n",
    "site_lons = use_lons\n",
    "\n",
    "# [optional] manually remove additional stations\n",
    "gnss_to_remove=[]\n",
    "\n",
    "for i, gnss_site in enumerate(gnss_to_remove):\n",
    "    if gnss_site in site_names:\n",
    "        site_names.remove(gnss_site)\n",
    "    if gnss_site not in bad_stn:\n",
    "        bad_stn.append(gnss_site)\n",
    "\n",
    "print(\"\\nFinal list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)\n",
    "print(\"List of {} stations removed from analysis\".format(len(bad_stn)))\n",
    "print(bad_stn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gps_los'></a>\n",
    "## 3.4. Project GNSS to LOS Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_velocities = gnss.get_los_obs(insar_metadata, \n",
    "                            'velocity', \n",
    "                            site_names, \n",
    "                            start_date=start_date,\n",
    "                            end_date=end_date,\n",
    "                            gnss_comp='enu2los', \n",
    "                            redo=True)\n",
    "\n",
    "# scale site velocities from m/yr to mm/yr\n",
    "gnss_velocities *= 1000.\n",
    "\n",
    "print('\\n site   vel_los [mm/yr]')\n",
    "print(np.array([site_names, gnss_velocities]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gps_insar'></a>\n",
    "## 3.5. Re-Reference GNSS and InSAR LOS Velocities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reference GNSS stations to GNSS reference site\n",
    "ref_site_ind = site_names.index(sitedata['sites'][site]['gps_ref_site_name'])\n",
    "gnss_velocities = gnss_velocities - gnss_velocities[ref_site_ind]\n",
    "\n",
    "# reference InSAR to GNSS reference site\n",
    "ref_site_lat = float(site_lats[ref_site_ind])\n",
    "ref_site_lon = float(site_lons[ref_site_ind])\n",
    "ref_y, ref_x = ut.coordinate(insar_metadata).geo2radar(ref_site_lat, ref_site_lon)[:2]\n",
    "insar_velocities = insar_velocities - insar_velocities[ref_y, ref_x]\n",
    "\n",
    "# plot GNSS stations on InSAR velocity field\n",
    "vmin, vmax = -20, 20\n",
    "cmap = plt.get_cmap('RdBu_r')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "cax = ax.imshow(insar_velocities, cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest', extent=(W, E, S, N))\n",
    "cbar = fig.colorbar(cax, ax=ax)\n",
    "cbar.set_label('LOS velocity [mm/year]')\n",
    "\n",
    "for lat, lon, obs in zip(site_lats, site_lons, gnss_velocities):\n",
    "    color = cmap((obs - vmin)/(vmax - vmin))\n",
    "    ax.scatter(lon, lat, color=color, s=8**2, edgecolors='k')\n",
    "for i, label in enumerate(site_names):\n",
    "     plt.annotate(label, (site_lons[i], site_lats[i]), color='black')\n",
    "\n",
    "out_fig = os.path.abspath('vel_insar_vs_gnss.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_nisar_validation'></a>\n",
    "# 4. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_make_vel'></a>\n",
    "## 4.1. Make Velocity Residuals at GNSS Locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set Parameters\n",
    "pixel_radius = 5   #number of InSAR pixels to average for comparison with GNSS\n",
    "\n",
    "#Create dictionary with the stations as the key and all their info as an array \n",
    "stn_dict = {}\n",
    "\n",
    "#Loop over GNSS station locations\n",
    "for i in range(len(site_names)): \n",
    "    # convert GNSS station lat/lon information to InSAR x/y grid\n",
    "    stn_lat = site_lats[i]\n",
    "    stn_lon = site_lons[i]\n",
    "    x_value = round((stn_lon - W)/lon_step)\n",
    "    y_value = round((stn_lat - N)/lat_step)\n",
    "    \n",
    "    # get velocities and residuals\n",
    "    gnss_site_vel = gnss_velocities[i]\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    vel_px_rad = insar_velocities[y_value-pixel_radius:y_value+1+pixel_radius, \n",
    "                     x_value-pixel_radius:x_value+1+pixel_radius]\n",
    "    insar_site_vel = np.median(vel_px_rad)\n",
    "    residual = gnss_site_vel - insar_site_vel\n",
    "\n",
    "    # populate data structure\n",
    "    values = [x_value, y_value, insar_site_vel, gnss_site_vel, residual, stn_lat, stn_lon]\n",
    "    stn = site_names[i]\n",
    "    stn_dict[stn] = values\n",
    "\n",
    "# extract data from structure\n",
    "res_list = []\n",
    "insar_site_vels = []\n",
    "gnss_site_vels = []\n",
    "lat_list = []\n",
    "lon_list = []\n",
    "for i in range(len(site_names)): \n",
    "    stn = site_names[i]\n",
    "    insar_site_vels.append(stn_dict[stn][2])\n",
    "    gnss_site_vels.append(stn_dict[stn][3])\n",
    "    res_list.append(stn_dict[stn][4])\n",
    "    lat_list.append(stn_dict[stn][5])\n",
    "    lon_list.append(stn_dict[stn][6])\n",
    "num_stn = len(site_names) \n",
    "print('Finish creating InSAR residuals at GNSS sites')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_make_velres'></a>\n",
    "## 4.2. Make Double-Differenced Velocity Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_gnss_sites = len(site_names)\n",
    "diff_res_list = []\n",
    "stn_dist_list = []\n",
    "\n",
    "# loop over stations\n",
    "for i in range(n_gnss_sites-1):\n",
    "    stn1 = site_names[i]\n",
    "    for j in range(i + 1, n_gnss_sites):\n",
    "        stn2 = site_names[j]\n",
    "\n",
    "        # calculate GNSS and InSAR velocity differences between stations\n",
    "        gnss_vel_diff = stn_dict[stn1][3] - stn_dict[stn2][3]\n",
    "        insar_vel_diff = stn_dict[stn1][2] - stn_dict[stn2][2]\n",
    "\n",
    "        # calculate GNSS vs InSAR differences (double differences) between stations\n",
    "        diff_res = gnss_vel_diff - insar_vel_diff\n",
    "        diff_res_list.append(diff_res)\n",
    "\n",
    "        # get distance (km) between stations using Haversine formula\n",
    "        # index 5 is lat, 6 is lon\n",
    "        stn_dist = haversine_distance(stn_dict[stn1][6], stn_dict[stn1][5], stn_dict[stn2][6], stn_dict[stn2][5])\n",
    "        stn_dist_list.append(stn_dist)\n",
    "\n",
    "# Write data for statistical tests\n",
    "gnss_site_dist = np.array(stn_dist_list)\n",
    "double_diff_rel_measure = np.array(np.abs(diff_res_list))\n",
    "ndx = np.argsort(gnss_site_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_valid_method1'></a>\n",
    "## 4.3. Secular Requirement Validation: Method 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define secular requirement\n",
    "secular_gnss_rqmt = 2  # mm/yr for 3 years of data over length scales of 0.1-50 km\n",
    "gnss_dist_rqmt = [0.1, 50.0]  # km\n",
    "\n",
    "# Statistics\n",
    "n_bins = 10\n",
    "threshold = 0.683  \n",
    "#  we assume that the distribution of residuals is Gaussian and \n",
    "#  that the threshold represents a 1-sigma limit within which \n",
    "#  we expect 68.3% of residuals to lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_table, fig = display_validation(gnss_site_dist,                 # binned distance for point\n",
    "                                           double_diff_rel_measure,        # binned double-difference velocities mm/yr\n",
    "                                           site,                           # cal/val site name\n",
    "                                           start_date,                     # start date of InSAR dataset\n",
    "                                           end_date,                       # end date of InSAR dataset \n",
    "                                           requirement=secular_gnss_rqmt,  # measurement requirement to meet, e.g 2 mm/yr for 3 years of data over 0.1-50km\n",
    "                                           distance_rqmt=gnss_dist_rqmt,   # distance over requirement is to meet, e.g. over length scales of 0.1-50 km [0.1, 50] \n",
    "                                           n_bins=n_bins,                  # number of bins, to collect statistics \n",
    "                                           threshold=threshold,            # quantile threshold for point-pairs that pass requirement, e.g. 0.683 - we expect 68.3% of residuals to lie. \n",
    "                                           sensor='Sentinel-1',            # sensor that is validated, Sentinel-1 or NISAR\n",
    "                                           validation_type='secular',      # validation for: secular, transient, coseismic requirement\n",
    "                                           validation_data='GNSS')         # validation method: GNSS - Method 1, InSAR - Method 2\n",
    "\n",
    "out_fig = os.path.abspath('secular_insar-gnss_velocity_vs_distance.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1—Successful when 68% of points below requirements line\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_validation_table(validation_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1 table by distance bin—successful when greater than 0.683\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_nisar_validation2'></a>\n",
    "# 5. NISAR Validation Approach 2: InSAR-only Structure Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Validation approach 2, we use a time interval and area where we assume no deformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot velocity map\n",
    "scp_args = 'velocity.h5 velocity -v -20 20 --colormap RdBu_r --figtitle LOS_Velocity --unit mm/yr -m ' + msk_file\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_array_mask'></a>\n",
    "## 5.1. Read Array and Mask Pixels with no Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the assumed non-earthquake displacement as the insar_displacment for statistics and convert to mm\n",
    "insar_velocities,_ = readfile.read(vel_file, datasetName = 'velocity')  #read velocity\n",
    "velStart = sitedata['sites'][site]['download_start_date']\n",
    "insar_velocities = insar_velocities * 1000.  # convert velocities from m to mm\n",
    "\n",
    "# set masked pixels to NaN\n",
    "msk,_ = readfile.read(msk_file)\n",
    "insar_velocities[msk == 0] = np.nan\n",
    "insar_velocities[insar_velocities == 0] = np.nan\n",
    "\n",
    "# display map of data after masking\n",
    "cmap = plt.get_cmap('RdBu_r')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.imshow(insar_velocities, cmap=cmap, vmin=-20, vmax=20, interpolation='nearest', extent=(W, E, S, N))\n",
    "ax.set_title(\"Secular \\n Date \"+velStart)\n",
    "cbar1 = fig.colorbar(img1, ax=ax)\n",
    "cbar1.set_label('LOS velocity [mm/year]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_remove_trend'></a>\n",
    "## 5.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define requirement\n",
    "secular_insar_rqmt = 2  # mm/yr\n",
    "insar_dist_rqmt = [0.1, 50.0]  # km\n",
    "\n",
    "sample_mode = 'profile'  # 'points' or 'profile'\n",
    "# note that the 'profile' method may take significantly longer\n",
    "\n",
    "# Collect samples using the specified method\n",
    "if sample_mode in ['points']:\n",
    "    X0,Y0 = load_geo(insar_metadata)\n",
    "    X0_2d,Y0_2d = np.meshgrid(X0,Y0)\n",
    "\n",
    "    insar_sample_dist, insar_rel_measure = samp_pair(X0_2d, Y0_2d, insar_velocities, num_samples=1000000)\n",
    "\n",
    "elif sample_mode in ['profile']:\n",
    "    # Sample grid setup\n",
    "    length, width = int(insar_metadata['LENGTH']), int(insar_metadata['WIDTH'])\n",
    "    X = np.linspace(W+lon_step, E-lon_step, width)  # longitudes\n",
    "    Y = np.linspace(N+lat_step, S-lat_step, length)  # latitudes\n",
    "    X_coords, Y_coords = np.meshgrid(X, Y)\n",
    "\n",
    "    # Draw random samples from map (without replacement)\n",
    "    num_samples = 20000\n",
    "    \n",
    "    # Retrieve profile samples\n",
    "    insar_sample_dist, insar_rel_measure = profile_samples(\\\n",
    "                    x=X_coords.reshape(-1,1),\n",
    "                    y=Y_coords.reshape(-1,1),\n",
    "                    data=insar_velocities,\n",
    "                    metadata=insar_metadata,\n",
    "                    len_rqmt=insar_dist_rqmt,\n",
    "                    num_samples=num_samples)\n",
    "\n",
    "print('Finished sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_M2ampvsdist2'></a>\n",
    "## 5.3. Amplitude vs. Distance of Relative Measurements (pair differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "n_bins = 10\n",
    "threshold = 0.683  \n",
    "#  we assume that the distribution of residuals is Gaussian and \n",
    "#  that the threshold represents a 1-sigma limit within which \n",
    "#  we expect 68.3% of residuals to lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_table, fig = display_validation(insar_sample_dist,              # binned distance for point\n",
    "                                           insar_rel_measure,              # binned relative velocities mm/yr\n",
    "                                           site,                           # cal/val site name\n",
    "                                           start_date,                     # start date of InSAR dataset\n",
    "                                           end_date,                       # end date of InSAR dataset \n",
    "                                           requirement=secular_insar_rqmt,  # measurement requirement to meet, e.g 2 mm/yr for 3 years of data over 0.1-50km\n",
    "                                           distance_rqmt=insar_dist_rqmt,   # distance over requirement is to meet, e.g. over length scales of 0.1-50 km [0.1, 50] \n",
    "                                           n_bins=n_bins,                  # number of bins, to collect statistics \n",
    "                                           threshold=threshold,            # quantile threshold for point-pairs that pass requirement, e.g. 0.683 - we expect 68.3% of residuals to lie. \n",
    "                                           sensor='Sentinel-1',            # sensor that is validated, Sentinel-1 or NISAR\n",
    "                                           validation_type='secular',      # validation for: secular, transient, coseismic requirement\n",
    "                                           validation_data='InSAR')         # validation method: GNSS - Method 1, InSAR - Method 2\n",
    "\n",
    "out_fig = os.path.abspath('secular_insar-only_vs_distance_'+site+'_date'+velStart+'.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2—\n",
    "    68% of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_M2RelMeasTable'></a>\n",
    "## 5.4. Bin Sample Pairs by Distance Bin and Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_validation_table(validation_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2 table of distance bins—\n",
    "    68% (0.683) of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_appendix1'></a>\n",
    "# Appendix: Supplementary Comparisons and Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_compare_raw'></a>\n",
    "## A.1. Compare Raw Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin, vmax = -25, 25\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(insar_site_vels, range=[vmin, vmax], bins=50, color=\"green\", edgecolor='grey', label='V_InSAR')\n",
    "plt.hist(gnss_site_vels, range=[vmin, vmax], bins=50, color=\"orange\", edgecolor='grey', label='V_gnss', alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Velocities \\n Date range {start_date}-{end_date} \\n Reference stn: {sitedata['sites'][site]['gps_ref_site_name']} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('LOS Velocity (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.ylim(0,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_plot_vel'></a>\n",
    "## A.2. Plot Velocity Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin, vmax = -10, 10\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(res_list, bins = 40, range=[vmin,vmax], edgecolor='grey', color=\"darkblue\", linewidth=1, label='V_gnss - V_InSAR (area average)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Residuals \\n Date range {start_date}-{end_date} \\n Reference stn: {sitedata['sites'][site]['gps_ref_site_name']} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('Velocity Residual (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_plot_velres'></a>\n",
    "## A.3. Plot Double Difference Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(diff_res_list, range = [vmin, vmax],bins = 40, color = \"darkblue\",edgecolor='grey',label='V_gnss_(s1-s2) - V_InSAR_(s1-s2)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Difference Residualts \\n Date range {start_date}-{end_date} \\n Reference stn: {sitedata['sites'][site]['gps_ref_site_name']} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('Double Differenced Velocity Residual (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_appendix_gps'></a>\n",
    "## A.4. GNSS Timeseries Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grab the time-series file used for time function estimation given the template setup\n",
    "template = readfile.read_template(os.path.join(mintpy_dir, 'smallbaselineApp.cfg'))\n",
    "template = ut.check_template_auto_value(template)\n",
    "insar_ts_file = TimeSeriesAnalysis.get_timeseries_filename(template, mintpy_dir)['velocity']['input']\n",
    "\n",
    "# read the time-series file\n",
    "insar_ts, atr = readfile.read(insar_ts_file, datasetName='timeseries')\n",
    "mask = readfile.read(os.path.join(mintpy_dir, 'maskTempCoh.h5'))[0]\n",
    "print(f'reading timeseries from file: {insar_ts_file}')\n",
    "\n",
    "# Get date list\n",
    "date_list = timeseries(insar_ts_file).get_date_list()\n",
    "num_date = len(date_list)\n",
    "date0, date1 = date_list[0], date_list[-1]\n",
    "insar_dates = ptime.date_list2vector(date_list)[0]\n",
    "\n",
    "# spatial reference\n",
    "coord = ut.coordinate(atr)\n",
    "ref_site = sitedata['sites'][site]['gps_ref_site_name']\n",
    "ref_gnss_obj = gnss.get_gnss_class('UNR')(site = ref_site, data_dir = os.path.join(mintpy_dir,'GNSS'))\n",
    "ref_lat, ref_lon = ref_gnss_obj.get_site_lat_lon()\n",
    "ref_y, ref_x = coord.geo2radar(ref_lat, ref_lon)[:2]\n",
    "if not mask[ref_y, ref_x]:\n",
    "    raise ValueError(f'Given reference GNSS site ({ref_site}) is in mask-out unrelible region in InSAR! Change to a different site.')\n",
    "ref_insar_dis = insar_ts[:, ref_y, ref_x]\n",
    "\n",
    "# Plot displacements and velocity timeseries at GNSS station locations\n",
    "num_site = len(site_names)\n",
    "prog_bar = ptime.progressBar(maxValue=num_site)\n",
    "for i, site_name in enumerate(site_names):\n",
    "    prog_bar.update(i+1, suffix=f'{site_name} {i+1}/{num_site}')\n",
    "\n",
    "    ## read data\n",
    "    # read GNSS\n",
    "    gnss_obj = gnss.get_gnss_class('UNR')(site = site_name, data_dir = os.path.join(mintpy_dir,'GNSS'))\n",
    "    gnss_dates, gnss_dis, _, gnss_lalo = gnss_obj.get_los_displacement(atr, start_date=date0, end_date=date1, ref_site=ref_site)[:4]\n",
    "    # shift GNSS to zero-mean in time [for plotting purpose]\n",
    "    gnss_dis -= np.nanmedian(gnss_dis)\n",
    "\n",
    "    # read InSAR\n",
    "    y, x = coord.geo2radar(gnss_lalo[0], gnss_lalo[1])[:2]\n",
    "    insar_dis = insar_ts[:, y, x] - ref_insar_dis\n",
    "    # apply a constant shift in time to fit InSAR to GNSS\n",
    "    comm_dates = sorted(list(set(gnss_dates) & set(insar_dates)))\n",
    "    if comm_dates:\n",
    "        insar_flag = [x in comm_dates for x in insar_dates]\n",
    "        gnss_flag = [x in comm_dates for x in gnss_dates]\n",
    "        insar_dis -= np.nanmedian(insar_dis[insar_flag] - gnss_dis[gnss_flag])\n",
    "\n",
    "    ## plot figure\n",
    "    if gnss_dis.size > 0 and np.any(~np.isnan(insar_dis)):\n",
    "        fig, ax = plt.subplots(figsize=(12, 3))\n",
    "        ax.axhline(color='grey',linestyle='dashed', linewidth=2)\n",
    "        ax.scatter(gnss_dates, gnss_dis*100, s=2**2, label=\"GNSS Daily Positions\")\n",
    "        ax.scatter(insar_dates, insar_dis*100, label=\"InSAR Positions\")\n",
    "        # axis format\n",
    "        ax.set_title(f\"Station Name: {site_name}\") \n",
    "        ax.set_ylabel('LOS displacement [cm]')\n",
    "        ax.legend()\n",
    "prog_bar.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
