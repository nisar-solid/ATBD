{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow to Validate NISAR L2 Secular Displacement Requirement\n",
    "\n",
    "**Original code authored by:** David Bekaert, Heresh Fattahi, Eric Fielding, and Zhang Yunjun \n",
    "\n",
    "Extensive modifications by Adrian Borsa and Amy Whetter 2022 <br>\n",
    "Subsequent updates and reformatting by Rob Zinke, Katia Tymofyeveva and Adrian Borsa 2024 <br>\n",
    "Optional corrections and plots by Emre Havazli, April 2025<br>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Step 1 of the validation workflow is executed in the <b>ARIA_prep</b> notebook. All processing steps in this notebook MUST be run sequentially, although several are optional and can be skipped.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Table of Contents: <a id='TOC'></a>\n",
    "\n",
    "[**Environment Setup**](#setup)\n",
    "- [Load Python Packages](#load_packages)\n",
    "- [Define CalVal Site and Parameters](#set_calval_params)\n",
    "- [Set Directories and Files](#set_directories)\n",
    "\n",
    "[**1. Download and Prepare Interferograms**](#prep_ifg)\n",
    "[Executed in ARIA_prep]\n",
    "\n",
    "[**2. Generate Time Series from Interferograms**](#gen_ts)\n",
    "- [2.1. Validate/Modify Interferogram Network](#validate_network)\n",
    "- [2.2. Generate Quality Control Mask](#generate_mask)\n",
    "- [2.3. Reference Interferograms To Common Lat/Lon](#common_latlon)\n",
    "- [2.4. Invert for SBAS Line-of-Sight Timeseries](#invert_SBAS)\n",
    "\n",
    "[**3. Optional Corrections**](#opt_correction)\n",
    "- [3.1. Solid Earth Tide Correction](#solid_earth)\n",
    "- [3.2. Tropospheric Delay Correction](#tropo_corr)\n",
    "- [3.3. Phase Deramping ](#phase_deramp)\n",
    "- [3.4. Topographic Residual Correction ](#topo_corr) \n",
    "\n",
    "[**4. Decompose InSAR and GNSS Time Series**](#decomp_ts)\n",
    "- [4.1. Estimate InSAR LOS Velocities](#insar_vel)\n",
    "- [4.2. Not Used](#empty)\n",
    "- [4.3. Find Collocated GNSS Stations](#find_gps)  \n",
    "- [4.4. Get GNSS Position Time Series](#gps_ts) \n",
    "- [4.5. Estimate GNSS LOS Velocities](#gps_vel)\n",
    "- [4.6. Re-reference GNSS and InSAR](#reference)\n",
    "\n",
    "[**5. Validation Method 1: GNSS-InSAR Direct Comparison**](#validation1)\n",
    "- [5.1. Make GNSS-InSAR Velocity Residuals at GNSS Station Locations](#make_resid)\n",
    "- [5.2. Make Double-differenced Velocity Residuals](#make_ddiff)\n",
    "- [5.3. Secular Requirement Validation: Method 1](#secular_validation_method1)\n",
    "\n",
    "[**6. Validation Method 2: InSAR-only Structure Function**](#validation2)\n",
    "- [6.1. Read InSAR Array and Mask Pixels with No Data](#array_mask)\n",
    "- [6.2. Randomly Sample and Pair Pixels](#array_sample)\n",
    "- [6.3. Secular Requirement Validation: Method 2](#secular_validation_method2)\n",
    "\n",
    "[**Appendix: Supplementary Comparisons and Plots**](#appendix)\n",
    "- [A.1. Compare Raw Velocities](#plot_vel)\n",
    "- [A.2. Plot Velocity Residuals](#plot_residual)\n",
    "- [A.3. Plot Double-differenced Residuals](#plot_ddiff)\n",
    "- [A.4. GNSS Timeseries Plots](#plot_gps_tseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='#setup'></a>\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Python Packages <a id='#load_packages'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import warnings\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from mintpy.cli import view, plot_network\n",
    "from mintpy.objects import gnss, timeseries\n",
    "from mintpy.smallbaselineApp import TimeSeriesAnalysis\n",
    "from mintpy.utils import ptime, readfile, utils as ut\n",
    "from scipy import signal\n",
    "\n",
    "from solid_utils.gnss_utils import scale_gnss_m_to_mm\n",
    "from solid_utils.fitting import IterativeOutlierFit\n",
    "from solid_utils.plotting import display_validation, display_validation_table\n",
    "from solid_utils.sampling import SiteVelocity, load_geo, samp_pair, profile_samples, haversine_distance\n",
    "\n",
    "#Set Global Plot Parameters\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Calval Site and Parameters <a id='set_calval_params'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify a calval location ID from my_sites.txt\n",
    "site = 'test' \n",
    "\n",
    "# Choose the requirement to validate\n",
    "# Options: 'Secular' 'Coseismic' 'Transient'\n",
    "requirement = 'Secular' \n",
    "\n",
    "# What dataset are you processing?\n",
    "# Options: \n",
    "#'ARIA_S1' (old directory structure for Sentinel-1 testing with aria-tools)\n",
    "#'ARIA_S1_new' (new directory structure for Sentinel-1 testing with aria-tools)\n",
    "dataset = 'ARIA_S1_new'\n",
    "aria_gunw_version = '3_0_1'\n",
    "\n",
    "# The date and version of this Cal/Val run\n",
    "rundate = '20250430'\n",
    "version = '1'\n",
    "\n",
    "# Provide the file where you keep your customized list of sites.\n",
    "custom_sites = '/home/jovyan/solid_earth_atbd_dev/ATBD_main/my_sites.txt'\n",
    "\n",
    "# Enter a username for storing your outputs\n",
    "if os.path.exists('/home/jovyan/solid_earth_atbd_dev/ATBD_main/me.txt'):\n",
    "    with open('/home/jovyan/solid_earth_atbd_dev/ATBD_main/me.txt') as m:\n",
    "        you = m.readline().strip()\n",
    "else:\n",
    "    you = input('Please type a username for your calval outputs:')\n",
    "    with open ('/home/jovyan/me.txt', 'w') as m: \n",
    "        m.write(you)\n",
    "\n",
    "# Load metadata for calval locations\n",
    "with open('/home/jovyan/solid_earth_atbd_dev/ATBD_main/my_sites.txt','r') as fid:\n",
    "    sitedata = json.load(fid)\n",
    "\n",
    "# Plot parameters\n",
    "vmin, vmax = -100, 100  # mm/yr\n",
    "cmap = plt.get_cmap('RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Directories and Files <a id='set_directories'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directory location for Cal/Val data (do not change)\n",
    "start_directory = '/scratch/nisar-st-calval-solidearth' \n",
    "\n",
    "# Site directory\n",
    "site_dir = os.path.join(start_directory, dataset, site)\n",
    "\n",
    "# Working directory for calval processing\n",
    "work_dir = os.path.join(site_dir, requirement, you, rundate, 'v' + version)\n",
    "print(\"  Work directory:\", work_dir)\n",
    "\n",
    "# Directory for storing GUNW interferograms\n",
    "gunw_dir = os.path.join(site_dir,'products')\n",
    "print(\"  GUNW directory:\", gunw_dir) \n",
    "\n",
    "# Directory for storing MintPy outputs\n",
    "mintpy_dir = os.path.join(work_dir,'MintPy')\n",
    "print(\"MintPy directory:\", mintpy_dir)\n",
    "\n",
    "# Flag missing MintPy directory\n",
    "if not os.path.exists(mintpy_dir):\n",
    "    print()\n",
    "    print('ERROR: Stop! Your MintPy processing directory does not exist for this requirement, site, version, or date of your ATBD run.')\n",
    "    print('You may need to run the prep notebook first!')\n",
    "    print()\n",
    "else:\n",
    "    os.chdir(mintpy_dir)\n",
    "\n",
    "# Set MintPy filenames\n",
    "geom_file = os.path.join(mintpy_dir, 'inputs', 'geometryGeo.h5')\n",
    "msk_file = os.path.join(mintpy_dir, 'maskTempCoh.h5')  # maskConnComp.h5\n",
    "ts_file = os.path.join(mintpy_dir, 'timeseries.h5')\n",
    "vel_file = os.path.join(mintpy_dir, 'velocity.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='#prep_ifg'></a>\n",
    "## 1. Download and Prepare Interferograms \n",
    "Executed in *ARIA_prep* notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='#gen_ts'></a>\n",
    "## 2. Generation of Time Series from Interferograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1. Validate/Modify Interferogram Network <a id='validate_network'></a>\n",
    "\n",
    "Add additional parameters to config_file in order to remove selected interferograms, change minimum coherence, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_file = os.path.join(mintpy_dir, sitedata['sites'][site]['calval_location'] + '.cfg')\n",
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep modify_network'\n",
    "process = subprocess.run(command, shell=True)\n",
    "plot_network.main(['inputs/ifgramStack.h5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2. Generate Quality Control Mask <a id='generate_mask'></a>\n",
    "\n",
    "Mask files can be can be used to mask pixels in the time-series processing. Below we generate an initial mask file `maskConnComp.h5` based on the connected components for all the interferograms, which is a metric for unwrapping quality. After time-series analysis is complete, we will calculate a mask from the temporal coherence or variation of phase or displacement with time to make `maskTempCoh.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command='generate_mask.py inputs/ifgramStack.h5  --nonzero  -o maskConnComp.h5  --update'\n",
    "process = subprocess.run(command, shell=True)\n",
    "view.main(['maskConnComp.h5', 'mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3. Reference Interferograms To Common Lat/Lon <a id='common_latlon'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep reference_point'\n",
    "process = subprocess.run(command, shell=True)\n",
    "os.system('info.py inputs/ifgramStack.h5 | egrep \"REF_\"');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Change Reference Point (Optional) <a id='common_latlon_change'></a>\n",
    "\n",
    "Use the cells below to update the reference point. Uncomment and enter the desired *new_lat* and *new_lon*, then verify the update in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_reference_point(config_file, new_lat, new_lon):\n",
    "#     # Read the file\n",
    "#     with open(config_file, \"r\") as file:\n",
    "#         lines = file.readlines()\n",
    "    \n",
    "#     # Update the specific key\n",
    "#     for i, line in enumerate(lines):\n",
    "#         if \"mintpy.reference.lalo\" in line:\n",
    "#             lines[i] = f\"mintpy.reference.lalo = {new_lat}, {new_lon}\\n\"\n",
    "#             break  # Stop after updating the line\n",
    "    \n",
    "#     # Write back the modified file\n",
    "#     with open(config_file, \"w\") as file:\n",
    "#         file.writelines(lines)\n",
    "    \n",
    "#     print(f\"Updated mintpy.reference.lalo to {new_lat}, {new_lon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_lat =\n",
    "# new_lon = \n",
    "# update_reference_point(config_file, new_lat, new_lon)  # New latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep reference_point'\n",
    "# process = subprocess.run(command, shell=True)\n",
    "# os.system('info.py inputs/ifgramStack.h5 | egrep \"REF_\"');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4. Invert for SBAS Line-of-Sight Timeseries <a id='invert_SBAS'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep invert_network'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='opt_correction'></a>\n",
    "## 3. Optional Corrections\n",
    "\n",
    "Phase distortions related to solid earth and ocean tidal effects, as well as those due to temporal variations in the vertical stratification of the atmosphere, can be mitigated using the approaches described below. At this point, it is expected that these corrections will not be needed to validate the mission requirements, but they may be used to produce the highest quality data products. Typically, these are applied to the estimated time series product rather than to the individual interferograms, since they are a function of the time of each radar acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Calculate Solid Earth Tides Correction <a id='solid_earth'></a>\n",
    "\n",
    "Optional solid Earth tide correction is included in the ARIA GUNW products and can be extracted using the <i>ARIA_prep</i> notebook.  \n",
    "Once extracted, you can calculate the correction using the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_file = f\"{mintpy_dir}/timeseries.h5\"                      # Input timeseries file to apply tropospheric correction\n",
    "cor_file = f\"{mintpy_dir}/inputs/solidEarthTide_ARIA.h5\"     # Input SET correction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if input correction file exist\n",
    "if os.path.isfile(cor_file):\n",
    "    print(f\"Found {cor_file}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Input {cor_file} not found\")\n",
    "\n",
    "# Check if the input time series file exist and read attributes\n",
    "if os.path.isfile(ts_file):\n",
    "    print(f\"Found {ts_file}\")\n",
    "    atr = readfile.read_attribute(ts_file)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Input {ts_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference correction to the timeseries reference pixel\n",
    "command = f\"reference_point.py {cor_file} --lat {float(atr['REF_LAT'])} --lon {float(atr['REF_LON'])}\"\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference correction to the timeseries reference date\n",
    "command = f\"reference_date.py {cor_file} --ref-date {atr['REF_DATE']}\"\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the corrections\n",
    "view.main([cor_file, '-m', msk_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Calculate Ionosphere Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_file = f\"{mintpy_dir}/timeseries.h5\"                        # Input timeseries file to apply tropospheric correction\n",
    "cor_stack_file = f\"{mintpy_dir}/inputs/ionStack.h5\"            # Input ionosphere stack file\n",
    "cor_file = f\"{mintpy_dir}/ion.h5\"                              # Ionosphere correction file (to be generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if input correction file exist\n",
    "if os.path.isfile(cor_stack_file):\n",
    "    print(f\"Found {cor_stack_file}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Input {cor_stack_file} not found\")\n",
    "\n",
    "# Check if the input time series file exist and read attributes\n",
    "if os.path.isfile(ts_file):\n",
    "    print(f\"Found {ts_file}\")\n",
    "    atr = readfile.read_attribute(ts_file)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Input {ts_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify ionosphere network\n",
    "command = f'modify_network.py {cor_stack_file} -t {config_file}'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reference correction to the timeseries reference pixel\n",
    "command = f\"reference_point.py {cor_stack_file} --lat {float(atr['REF_LAT'])} --lon {float(atr['REF_LON'])}\"\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimate ionospheric delay time-series via ifgram_inversion.py\n",
    "command = f'ifgram_inversion.py {cor_stack_file} --dset unwrapPhase --weight-func no --update'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference correction to the timeseries reference date\n",
    "command = f\"reference_date.py {cor_file} --ref-date {atr['REF_DATE']}\"\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the corrections\n",
    "view.main([cor_file, '-m', msk_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3. Calculate Tropospheric Delay Correction <a id='tropo_corr'></a>\n",
    "\n",
    "Optional atmospheric correction utilizes the PyAPS (Jolivet et al., 2011, Jolivet and Agram, 2012) module within GIAnT (or eventually a merged replacement for GIAnT and MintPy). PyAPS is well documented, maintained and can be freely downloaded. PyAPS is included in GIAnT distribution). PyAPS currently includes support for ECMWF’s ERA-Interim, NOAA’s NARR and NASA’s MERRA weather models. A final selection of atmospheric models to be used for operational NISAR processing will be done during Phase C.\n",
    "\n",
    "[T]ropospheric delay maps are produced from atmospheric data provided by Global Atmospheric Models. This method aims to correct differential atmospheric delay correlated with the topography in interferometric phase measurements. Global Atmospheric Models (hereafter GAMs)... provide estimates of the air temperature, the atmospheric pressure and the humidity as a function of elevation on a coarse resolution latitude/longitude grid. In PyAPS, we use this 3D distribution of atmospheric variables to determine the atmospheric phase delay on each pixel of each interferogram.\n",
    "\n",
    "The absolute atmospheric delay is computed at each SAR acquisition date. For a pixel a_i at an elevation z at acquisition date i, the four surrounding grid points are selected and the delays for their respective elevations are computed. The resulting delay at the pixel a_i is then the bilinear interpolation between the delays at the four grid points. Finally, we combine the absolute delay maps of the InSAR partner images to produce the differential delay maps used to correct the interferograms.\n",
    "\n",
    "REFERENCE : https://github.com/insarlab/pyaps#2-account-setup-for-era5\n",
    "<br> Read Section 2 for ERA5 [link above] to create an account on the CDS website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if 'do_tropo_correction' in sitedata['sites']:\n",
    "#     do_tropo_correction = sitedata['sites'][site]['do_tropo_correction']\n",
    "# else:\n",
    "#     do_tropo_correction = 'False'\n",
    "\n",
    "# if do_tropo_correction == 'True':\n",
    "#     if not Use_Staged_Data and not os.path.exists(os.path.expanduser(\"~\")+'/.cdsapirc'):\n",
    "#         print('NEEDED to download ERA5, link: https://cds.climate.copernicus.eu/user/register')\n",
    "#         UID = input('Please type your CDS_UID:')\n",
    "#         CDS_API = input('Please type your CDS_API:')\n",
    "        \n",
    "#         cds_tmp = '''url: https://cds.climate.copernicus.eu/api\n",
    "#         key: {UID}:{CDS_API}'''.format(UID=UID, CDS_API=CDS_API)\n",
    "#         os.system('echo \"{cds_tmp}\" > ~/.cdsapirc; chmod 600 ~/.cdsapirc'.format(cds_tmp = str(cds_tmp)))\n",
    "    \n",
    "#     command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep correct_troposphere'\n",
    "#     process = subprocess.run(command, shell=True)\n",
    "    \n",
    "#     view.main(['inputs/ERA5.h5'])\n",
    "#     timeseries_filename = 'timeseries_ERA5.h5'\n",
    "# else:\n",
    "#     timeseries_filename = 'timeseries.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note for HRRR:</b> The HRRR dataset is included in ARIA GUNW products and can be extracted using the <i>ARIA_prep</i> notebook with the <code>solid_earth_atbd_dev</code> environment. <br><br>\n",
    "    If you already have the <code>HRRR_ARIA.h5</code> file, you can apply HRRR corrections using the cells below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_file = f\"{mintpy_dir}/timeseries.h5\"                      # Input timeseries file to apply tropospheric correction\n",
    "cor_file = f\"{mintpy_dir}/inputs/HRRR_ARIA.h5\"               # Input SET correction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if input correction file exist\n",
    "if os.path.isfile(cor_file):\n",
    "    print(f\"Found {cor_file}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Input {cor_file} not found\")\n",
    "\n",
    "# Check if the input time series file exist and read attributes\n",
    "if os.path.isfile(ts_file):\n",
    "    print(f\"Found {ts_file}\")\n",
    "    atr = readfile.read_attribute(ts_file)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Input {ts_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference correction to the timeseries reference pixel\n",
    "command = f\"reference_point.py {cor_file} --lat {float(atr['REF_LAT'])} --lon {float(atr['REF_LON'])}\"\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference correction to the timeseries reference date\n",
    "command = f\"reference_date.py {cor_file} --ref-date {atr['REF_DATE']}\"\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the corrections\n",
    "view.main([cor_file, '-m', msk_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Optional Corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solid Earth Tides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files:\n",
    "input_timeseries = f\"{mintpy_dir}/timeseries.h5\"\n",
    "input_cor = f\"{mintpy_dir}/inputs/solidEarthTide_ARIA.h5\"\n",
    "\n",
    "# Output files:\n",
    "timeseries_filename = f\"{mintpy_dir}/timeseries_SET.h5\"\n",
    "velocity_filename = f\"{mintpy_dir}/velocity_SET.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply SET Correction\n",
    "command = f'diff.py {input_timeseries} {input_cor} -o {timeseries_filename} --force'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Estimate velocity\n",
    "command = f'timeseries2velocity.py {timeseries_filename} -o {velocity_filename}'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view.main([velocity_filename, 'velocity', '-m', msk_file, '-u', 'mm', '-v', str(vmin), str(vmax), '--title', f\"{velocity_filename.split('/')[-1]}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files:\n",
    "input_timeseries = f\"{mintpy_dir}/timeseries_SET.h5\"\n",
    "input_cor = f\"{mintpy_dir}/ion.h5\"\n",
    "\n",
    "# Output file:\n",
    "timeseries_filename = f\"{mintpy_dir}/timeseries_SET_iono.h5\"\n",
    "velocity_filename = f\"{mintpy_dir}/velocity_SET_iono.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ionopshere correction\n",
    "command = f'diff.py {input_timeseries} {input_cor} -o {timeseries_filename} --force'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Estimate velocity\n",
    "command = f'timeseries2velocity.py {timeseries_filename} -o {velocity_filename}'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view.main([velocity_filename, 'velocity', '-m', msk_file, '-u', 'mm', '-v', str(vmin), str(vmax), '--title', f\"{velocity_filename.split('/')[-1]}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Troposphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files:\n",
    "input_timeseries = f\"{mintpy_dir}/timeseries_SET_iono.h5\"\n",
    "input_cor = f\"{mintpy_dir}/inputs/HRRR_ARIA.h5\"\n",
    "\n",
    "# Output file:\n",
    "timeseries_filename = f\"{mintpy_dir}/timeseries_SET_iono_HRRR.h5\"\n",
    "velocity_filename = f\"{mintpy_dir}/velocity_SET_iono_HRRR.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply troposphere correction\n",
    "command = f'diff.py {input_timeseries} {input_cor} -o {timeseries_filename} --force'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Estimate velocity\n",
    "command = f'timeseries2velocity.py {timeseries_filename} -o {velocity_filename}'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view.main([velocity_filename, 'velocity', '-m', msk_file, '-u', 'mm', '-v', str(vmin), str(vmax), '--title', f\"{velocity_filename.split('/')[-1]}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4. Phase Deramping <a id='phase_deramp'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep deramp'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.5. Topographic Residual Correction <a id='topo_corr'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep correct_topography'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> The rest of the notebook is using the variable <code>timeseries_filename</code> as InSAR result. <b>Make sure <code>timeseries_filename</code> is set to the correct file</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timeseries_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='decomp_ts'></a>\n",
    "## 4. Estimate InSAR and GNSS Velocities\n",
    "The approach that will be used for the generation of NISAR L3 products for Requirements 660 and 663 allows for an explicit inclusion of key basis functions (e.g., Heaviside functions, secular rate, etc.) in the InSAR inversion. Modifications to this algorithm may be identified and implemented in response to NISAR Phase C activities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1. Estimate InSAR LOS Velocities <a id='insar_vel'></a>\n",
    "\n",
    "Given a time series of InSAR LOS displacements, the observations for a given pixel, $U(t)$, can be parameterized as:\n",
    "\n",
    "$$U(t) = a \\;+\\; vt \\;+\\; c_1 cos (\\omega_1t - \\phi_{1,}) \\;+\\; c_2 cos (\\omega_2t - \\phi_2) \\;+\\; \\sum_{j=1}^{N_{eq}} \\left( h_j+f_j F_j (t-t_j) \\right)H(t - t_j) \\;+\\; \\frac{B_\\perp (t)}{R sin \\theta}\\delta z \\;+\\; residual$$ \n",
    "\n",
    "which includes a constant offset $(a)$, velocity $(v)$, and amplitudes $(c_j)$ and phases $(\\phi_j)$ of annual $(\\omega_1)$ and semiannual $(\\omega_2)$ sinusoidal terms.  Where needed we can include additional complexity, such as coseismic and postseismic processes parameterized by Heaviside (step) functions $H$ and postseismic functions $F$ (the latter typically exponential and/or logarithmic).   $B_\\perp(t)$, $R$, $\\theta$, and $\\delta z$ are, respectively, the perpendicular component of the interferometric baseline relative to the first date, slant range distance, incidence angle and topography error correction for the given pixel. \n",
    "\n",
    "Thus, given either an ensemble of interferograms or the output of SBAS (displacement vs. time), we can write the LSQ problem as \n",
    "\n",
    "$$ \\textbf{G}\\textbf{m} = \\textbf{d}$$\n",
    "\n",
    "where $\\textbf{G}$ is the design matrix (constructed out of the different functional terms in Equation 2 evaluated either at the SAR image dates for SBAS output, or between the dates spanned by each pair for interferograms), $\\textbf{m}$ is the vector of model parameters (the coefficients in Equation 2) and $\\textbf{d}$ is the vector of observations.  (In the case of GNSS displacement timeseries, $\\textbf{G}, \\textbf{d}, \\textbf{m}$ are constructed using values evaluated at single epochs corresponding to the GNSS solution times, otherwise the estimate is the same.) \n",
    "\n",
    "With this formulation, we can obtain InSAR and GNSS velocity estimates and their formal uncertainties (including in areas where zero deformation is expected). The default InSAR (and GNSS) velocity fit in MintPy is to estimate a mean linear velocity $(v)$ in in the equation. To estimate a secular deformation rate, one can also estimate additional parameters for annual and semi-annual terms to account for seasonal periodic fluctuations in position. Below, we define a set of basis functions that MintPy will invert for, for both InSAR and GNSS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "If the time-series span less than one year, then annual and semi-annual fuctions might not be reliably fit for noisy data. In that case, the periodic functions will not be included in these analyses.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify basis functions\n",
    "# Changing these might change the meaning of this exercise! Use caution if altering.\n",
    "ts_functions = {\n",
    "                'polynomial': 1,\n",
    "                'periodic': [1.0, 0.5],  # Periodic terms are in units of years\n",
    "                'stepDate': [],\n",
    "                'polyline': [],\n",
    "                'exp': {},\n",
    "                'log': {}\n",
    "}\n",
    "\n",
    "# Determine time span to check whether fitting of periodic functions is appropriate\n",
    "ts_metadata = readfile.read_attribute(timeseries_filename)\n",
    "start_date = dt.strptime(sitedata['sites'][site]['download_start_date'], '%Y%m%d')\n",
    "end_date = dt.strptime(sitedata['sites'][site]['download_end_date'], '%Y%m%d')\n",
    "ts_timespan = (end_date - start_date).days\n",
    "\n",
    "if ts_timespan < 365*2:\n",
    "    print('Time span is {:d} days'.format(ts_timespan))\n",
    "    if ts_timespan < 365:\n",
    "        ts_functions['periodic'] = []\n",
    "        print('No periodic functions will be fit')\n",
    "    else:\n",
    "        warnings.warn('Warning: Periodic fits might be unreliable for series less than 2 years')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the TS model into argument string for command line execution\n",
    "function_str = ''\n",
    "\n",
    "# Loop through basis functions\n",
    "for fcn in ts_functions.keys():\n",
    "    arg = ts_functions[fcn]\n",
    "    \n",
    "    if fcn == 'polynomial':\n",
    "        function_str += f' --polynomial {arg:d}'\n",
    "    \n",
    "    else:\n",
    "        # Check whether values are passed\n",
    "        if len(arg) > 0:\n",
    "            # Append basis function name\n",
    "            function_str += f' --{fcn:s}'\n",
    "\n",
    "            # Append function arguments (e.g, period, decay, etc.)\n",
    "            if type(arg) == list:\n",
    "                function_str += ' {:s}'.format(' '.join([str(a) for a in arg]))\n",
    "            elif type(arg) == dict:\n",
    "                for event_date in arg.keys():\n",
    "                    function_str += f' {event_date:s}'\n",
    "                    for val in arg[event_date]:\n",
    "                        function_str += f' {val:f}'\n",
    "\n",
    "# Run command\n",
    "command = 'timeseries2velocity.py ' + timeseries_filename + function_str\n",
    "process = subprocess.run(command, shell=True)\n",
    "\n",
    "# Load velocity file\n",
    "insar_velocities,_ = readfile.read(vel_file, datasetName = 'velocity')  # read velocity file\n",
    "insar_velocities = insar_velocities * 1000.  # convert InSAR velocities from m/yr to mm/yr\n",
    "\n",
    "# Set masked pixels to NaN\n",
    "msk,_ = readfile.read(msk_file)\n",
    "insar_velocities[msk == 0] = np.nan\n",
    "insar_velocities[insar_velocities == 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the mean linear velocity fit. The MintPy `view` module automatically reads the temporal coherence mask `maskTempCoh.h5` and applies that to mask out pixels with unreliable velocities (white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp_args = f\"velocity.h5 velocity -v {vmin} {vmax} --colormap RdBu_r --figtitle LOS_Velocity --unit mm/yr -m {msk_file}\"\n",
    "view.main(scp_args.split());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Not Used <a id='empty'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.3. Find Collocated GNSS Stations <a id='find_gps'></a>\n",
    "\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks (e.g., NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand), located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales.\n",
    "\n",
    "MintPy supports several options for downloading and handling GNSS data. Currently supported data sets are:\n",
    "* UNR National Geodetic Laboratory\n",
    "* SIO/JPL MEaSUREs ESESES\n",
    "* JPL SIDESHOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GNSS processing source\n",
    "if 'gnss_source' in sitedata['sites'][site]:\n",
    "    gnss_source = sitedata['sites'][site]['gnss_source']\n",
    "else:\n",
    "    gnss_source = 'UNR'\n",
    "print(f\"GNSS processing source: {gnss_source:s}\")\n",
    "\n",
    "# Get analysis metadata from InSAR velocity file\n",
    "insar_metadata = readfile.read_attribute(vel_file)\n",
    "lat_step = float(insar_metadata['Y_STEP'])\n",
    "lon_step = float(insar_metadata['X_STEP'])\n",
    "(S,N,W,E) = ut.four_corners(insar_metadata)\n",
    "start_date = insar_metadata.get('START_DATE', None).split('T')[0]\n",
    "end_date = insar_metadata.get('END_DATE', None).split('T')[0]\n",
    "start_date_gnss = dt.strptime(start_date, \"%Y%m%d\")\n",
    "end_date_gnss = dt.strptime(end_date, \"%Y%m%d\")\n",
    "\n",
    "# Get center lat/lon of analysis region\n",
    "bbox = sitedata['sites'][site]['analysis_region'].replace(\"'\",\"\").split()\n",
    "lat0 = (float(bbox[0]) + float(bbox[1]))/2\n",
    "lon0 = (float(bbox[2]) + float(bbox[3]))/2\n",
    "# Identify geometry file x/y location associated with center lat/lon\n",
    "geom_obj = mintpy_dir + '/inputs/geometryGeo.h5'\n",
    "atr = readfile.read_attribute(geom_obj)\n",
    "coord = ut.coordinate(atr, lookup_file=geom_obj)\n",
    "y, x = coord.geo2radar(lat0, lon0)[0:2]\n",
    "y = max(0, y);  y = min(int(atr['LENGTH'])-1, y)\n",
    "x = max(0, x);  x = min(int(atr['WIDTH'])-1, x)\n",
    "# Write out inclination/azimuth at specified x/y location\n",
    "kwargs = dict(box=(x,y,x+1,y+1))\n",
    "inc_angle = readfile.read(geom_obj, datasetName='incidenceAngle', **kwargs)[0][0,0]\n",
    "az_angle  = readfile.read(geom_obj, datasetName='azimuthAngle',   **kwargs)[0][0,0]\n",
    "\n",
    "# Set GNSS Parameters\n",
    "gnss_completeness_threshold = 0.9    # percent of data timespan with valid GNSS epochs\n",
    "gnss_residual_stdev_threshold = 10.  # max threshold standard deviation of residuals to linear GNSS fit (mm/yr)\n",
    "\n",
    "# Search for collocated GNSS stations\n",
    "site_names, _, _ = gnss.search_gnss(SNWE=(S,N,W,E),\n",
    "                                    start_date=start_date,\n",
    "                                    end_date=end_date,\n",
    "                                    source=gnss_source)\n",
    "site_names = [str(stn) for stn in site_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.4. Get GNSS Position Time Series <a id='gps_ts'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the GNSS station data, and evaluate the station data quality based on data completeness and scatter of residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Empty dicts and lists to store GNSS data\n",
    "gnss_stns = {}\n",
    "bad_stns = {}\n",
    "\n",
    "for site_name in site_names:\n",
    "    gnss_stn = gnss.get_gnss_class(gnss_source)(site = site_name)\n",
    "    gnss_stn.open(print_msg=False)\n",
    "\n",
    "    # check if station is in masked out area\n",
    "    site_lat, site_lon = gnss_stn.get_site_lat_lon()\n",
    "    site_y, site_x = ut.coordinate(insar_metadata).geo2radar(site_lat, site_lon)[:2]\n",
    "    masked_loc = np.isnan(insar_velocities[site_y, site_x])\n",
    "\n",
    "    # count number of dates in time range\n",
    "    dates = gnss_stn.dates\n",
    "    range_days = (end_date_gnss - start_date_gnss).days\n",
    "    gnss_count = np.histogram(dates, bins=[start_date_gnss, end_date_gnss])\n",
    "    gnss_count = int(gnss_count[0][0])\n",
    "\n",
    "    # get standard deviation of residuals to linear fit\n",
    "    disp_los = ut.enu2los(gnss_stn.dis_e, gnss_stn.dis_n, gnss_stn.dis_u, inc_angle, az_angle)\n",
    "    disp_detrended = signal.detrend(disp_los)\n",
    "    stn_stdv = np.std(disp_detrended)\n",
    "\n",
    "    # select GNSS stations based on data completeness, scatter of residuals\n",
    "    # and whether they are in masked areas\n",
    "    if (range_days*gnss_completeness_threshold <= gnss_count) \\\n",
    "        and (stn_stdv <= gnss_residual_stdev_threshold) \\\n",
    "        and (masked_loc == False):\n",
    "        gnss_stns[site_name] = gnss_stn\n",
    "    else:\n",
    "        bad_stns[site_name] = gnss_stn\n",
    "\n",
    "# [optional] manually remove additional stations\n",
    "gnss_to_remove=[]\n",
    "\n",
    "for site_name in gnss_to_remove:\n",
    "    bad_stns[site_name] = gnss_stns[site_name]\n",
    "    del gnss_stns[site_name]\n",
    "\n",
    "# Final list of site names\n",
    "site_names = [*gnss_stns]\n",
    "bad_site_names = [*bad_stns]\n",
    "\n",
    "print(\"\\nFinal list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)\n",
    "print(\"List of {} stations removed from analysis\".format(len(bad_site_names)))\n",
    "print(bad_site_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stations in map view\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "cax = ax.imshow(insar_velocities, cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest', extent=(W, E, S, N))\n",
    "cbar = fig.colorbar(cax, ax=ax)\n",
    "cbar.set_label('LOS velocity [mm/year]')\n",
    "\n",
    "# Plot valid stations\n",
    "for i, gnss_stn in enumerate(gnss_stns.values()):\n",
    "    # Plot station\n",
    "    ax.scatter(gnss_stn.site_lon, gnss_stn.site_lat, s=8**2, c='k',\n",
    "               label=('valid' if i == 0 else None))\n",
    "    ax.annotate(gnss_stn.site, (gnss_stn.site_lon, gnss_stn.site_lat))\n",
    "\n",
    "# Plot rejected stations\n",
    "for i, gnss_stn in enumerate(bad_stns.values()):\n",
    "    # Plot station\n",
    "    ax.scatter(gnss_stn.site_lon, gnss_stn.site_lat, s=8**2,\n",
    "               marker='X', facecolor='none', edgecolor='k',\n",
    "              label=('rejected' if i == 0 else None))\n",
    "    ax.annotate(gnss_stn.site, (gnss_stn.site_lon, gnss_stn.site_lat))\n",
    "\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.5. Estimate GNSS LOS Velocities <a id='gps_vel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the GNSS velocities in InSAR line of sight, by projecting the 3D displacements into LOS and fitting the resulting time-series using the same basis functions (i.e., linear and seasonal terms) that were used to fit the InSAR time-series. Outliers are removed from the GNSS position data and the functional fits are recomputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Outlier ID and removal parameters\n",
    "outlier_thresh = 3  # standard deviations\n",
    "outlier_iters = 1  # iterations\n",
    "\n",
    "# Loop through valid GNSS stations\n",
    "gnss_fits = {}  # empty dict to store fits\n",
    "gnss_sites = {}  # empty dict to store site measurements\n",
    "\n",
    "for i, site_name in enumerate(site_names):\n",
    "    # Retrieve station information\n",
    "    gnss_stn = gnss_stns[site_name]\n",
    "    gnss_stn.get_los_displacement(insar_metadata,\n",
    "                                  start_date=start_date,\n",
    "                                  end_date=end_date)\n",
    "\n",
    "    # Scale displacement values from m to mm\n",
    "    scale_gnss_m_to_mm(gnss_stn)\n",
    "    \n",
    "    # Outlier detection and removal\n",
    "    gnss_fit = IterativeOutlierFit(gnss_stn.dates, gnss_stn.dis_los,\n",
    "                                   model=ts_functions, threshold=outlier_thresh,\n",
    "                                   max_iter=outlier_iters)\n",
    "    gnss_fits[site_name] = gnss_fit  # record for posterity\n",
    "\n",
    "    # Record GNSS site velocity\n",
    "    gnss_site = SiteVelocity(site=site_name,\n",
    "                             site_lon=gnss_stn.site_lon,\n",
    "                             site_lat=gnss_stn.site_lat,\n",
    "                             vel=gnss_fit.m_hat[1],\n",
    "                             vel_err=gnss_fit.mhat_se[1],\n",
    "                             unit='mm/yr')\n",
    "    gnss_sites[site_name] = gnss_site\n",
    "\n",
    "    # Report\n",
    "    if i == 0 :\n",
    "        print('site velocity(mm/yr)')\n",
    "    print(gnss_site)\n",
    "\n",
    "    # Plotting parameters\n",
    "    n_dates = len(gnss_stn.dates)\n",
    "    label_skips = n_dates//6\n",
    "    ax_nb = i % 2\n",
    "\n",
    "    # Spawn figure for even numbers\n",
    "    if ax_nb == 0:\n",
    "        fig, axes = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "\n",
    "    # Plot outliers\n",
    "    if gnss_fit.n_outliers > 0:\n",
    "        axes[ax_nb].scatter(gnss_fit.outlier_dates,\n",
    "                            gnss_fit.outlier_dis,\n",
    "                            2**2, 'firebrick', label='outliers')\n",
    "\n",
    "    # Plot filtered data and model fit\n",
    "    axes[ax_nb].scatter(gnss_fit.dates, gnss_fit.dis, 3**2,\n",
    "                        'dimgrey', zorder=1)\n",
    "    axes[ax_nb].plot(gnss_fit.dates, gnss_fit.dis_hat,\n",
    "                     'c', linewidth=3, label='model fit', zorder=2)\n",
    "    axes[ax_nb].plot(gnss_fit.dates, gnss_fit.err_envelope[0],\n",
    "                     'b--', zorder=3)\n",
    "    axes[ax_nb].plot(gnss_fit.dates, gnss_fit.err_envelope[1],\n",
    "                     'b--', zorder=3, label='95% conf')\n",
    "    \n",
    "    # Format plot\n",
    "    axes[ax_nb].legend()\n",
    "    axes[ax_nb].set_xticks(gnss_stn.dates[::label_skips])\n",
    "    axes[ax_nb].set_xticklabels([date.strftime('%Y-%m-%d') \\\n",
    "                                 for date in gnss_stn.dates[::label_skips]],\n",
    "                                rotation=80)\n",
    "    axes[ax_nb].set_ylabel('LOS displacement (mm)')\n",
    "    axes[ax_nb].set_title(gnss_site)\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.6. Re-reference GNSS and InSAR <a id='reference'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference GNSS stations to reference site\n",
    "ref_site = sitedata['sites'][site]['gps_ref_site_name']\n",
    "if sitedata['sites'][site]['gps_ref_site_name'] == 'auto':\n",
    "    ref_site = [*gnss_sites.keys()][0]\n",
    "\n",
    "gnss_ref_vel = gnss_sites[ref_site].vel\n",
    "print(f\"Using GNSS reference station: {ref_site}\")\n",
    "\n",
    "for site_name in site_names:\n",
    "    gnss_sites[site_name].vel -= gnss_ref_vel\n",
    "    print(str(gnss_sites[site_name]))\n",
    "\n",
    "# Reference InSAR to GNSS reference site\n",
    "(ref_y,\n",
    " ref_x,\n",
    " _, _) = ut.coordinate(insar_metadata).geo2radar(gnss_sites[ref_site].site_lat,\n",
    "                                                 gnss_sites[ref_site].site_lon)\n",
    "insar_ref_velocity = insar_velocities[ref_y, ref_x]\n",
    "insar_velocities = insar_velocities - insar_ref_velocity\n",
    "print(f\"Insar reference velocity {insar_ref_velocity:.1f} mm/yr\")\n",
    "\n",
    "# Plot GNSS stations on InSAR velocity field\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "cax = ax.imshow(insar_velocities, cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                interpolation='nearest', extent=(W, E, S, N))\n",
    "cbar = fig.colorbar(cax, ax=ax)\n",
    "cbar.set_label('LOS velocity [mm/year]')\n",
    "\n",
    "for site_name in site_names:\n",
    "    gnss_site = gnss_sites[site_name]\n",
    "    color = cmap((gnss_site.vel - vmin)/(vmax-vmin))\n",
    "    ax.scatter(gnss_site.site_lon, gnss_site.site_lat, color=color, s=8**2, edgecolors='k')\n",
    "    ax.text(gnss_site.site_lon, gnss_site.site_lat, gnss_site.site)\n",
    "    ax.set_title(f\"{gnss_source} GNSS velocities on InSAR velocities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Note :</b> \n",
    "Negative values indicates that target is moving away from the radar (i.e., subsidence in case of vertical deformation).\n",
    "Positive values indicates that target is moving towards the radar (i.e., uplift in case of vertical deformation). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='validation1'></a>\n",
    "## 5. Validation Method 1: GNSS-InSAR Direct Comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1. Make GNSS-InSAR Velocity Residuals at GNSS Station Locations <a id='make_resid'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "pixel_radius = 5   #number of InSAR pixels to average for comparison with GNSS\n",
    "\n",
    "# Loop over InSAR measurements at GNSS station locations\n",
    "insar_sites = {}  # empty dict to store site measurements\n",
    "\n",
    "for i, site_name in enumerate(site_names):\n",
    "    gnss_site = gnss_sites[site_name]\n",
    "    \n",
    "    # Convert GNSS station lat/lon information to InSAR x/y grid\n",
    "    stn_y, stn_x, _, _ = coord.geo2radar(gnss_site.site_lat,\n",
    "                                         gnss_site.site_lon)\n",
    "\n",
    "    # Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    # To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    vel_px_rad = insar_velocities[stn_y-pixel_radius:stn_y+1+pixel_radius, \n",
    "                                  stn_x-pixel_radius:stn_x+1+pixel_radius]\n",
    "    insar_site_vel = np.nanmedian(vel_px_rad)\n",
    "\n",
    "    # Assign to object\n",
    "    insar_site = SiteVelocity(site=site_name,\n",
    "                              site_lon=gnss_site.site_lon,\n",
    "                              site_lat=gnss_site.site_lat,\n",
    "                              vel=insar_site_vel,\n",
    "                              unit='mm/yr')\n",
    "    insar_sites[site_name] = insar_site\n",
    "\n",
    "    # Report\n",
    "    if i == 0 :\n",
    "        print('site velocity(mm/yr)')\n",
    "    print(insar_site)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.2. Make Double-Differenced Velocity Residuals <a id='make_ddiff'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the InSAR velocity field to a single reference site, as done in Section 4.9, leaves room for discrepancies between the InSAR and GNSS data sets. Instead, this validation exercise focuses on the ability to accurately measure relative displacement rates across a scene. To eliminate bias introduced by imprecise registration between the InSAR and GNSS data, we difference the InSAR and GNSS relative velocities between pairs of stations, i.e., the \"double-difference\" between site pairs.\n",
    "\n",
    "We compute the double-differences below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute double differences between site pairs\n",
    "n_sites = len(site_names)\n",
    "\n",
    "# Loop over stations\n",
    "double_diffs = {}  # empty dict to populate with InSAR - GNSS diffs\n",
    "for i in range(n_sites - 1):\n",
    "    stn1 = site_names[i]\n",
    "    for j in range(i + 1, n_sites):\n",
    "        stn2 = site_names[j]\n",
    "\n",
    "        # Compute distance (km) between site locations\n",
    "        site_dist = haversine_distance(gnss_sites[stn1].site_lon,\n",
    "                                       gnss_sites[stn1].site_lat,\n",
    "                                       gnss_sites[stn2].site_lon,\n",
    "                                       gnss_sites[stn2].site_lat)\n",
    "        \n",
    "        # Calculate site-to-site differences\n",
    "        diff_name = f\"{stn1}-{stn2}\"\n",
    "        gnss_diff = gnss_sites[stn1] - gnss_sites[stn2]\n",
    "        insar_diff = insar_sites[stn1] - insar_sites[stn2]\n",
    "        diff_res = gnss_diff - insar_diff\n",
    "        diff_res.dist = site_dist\n",
    "        diff_res.site = diff_name\n",
    "\n",
    "        # Record to dictionary\n",
    "        double_diffs[diff_name] = diff_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.3. Secular Requirement Validation: Method 1 <a id='secular_validation_method1'></a>\n",
    "\n",
    "We assume that the distribution of residuals is Gaussian and that the requirement success threshold represents a 1-sigma limit within which we expect 68.3% of residuals to lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define requirement\n",
    "secular_threshold_rqmt = 2  # mm/yr\n",
    "secular_distance_rqmt = [0.1, 50.0]  # km\n",
    "\n",
    "# Validation parameters\n",
    "n_bins = 10\n",
    "threshold = 0.683  \n",
    "\n",
    "# Write data for statistical tests\n",
    "site_dist = np.array([diff_res.dist for diff_res in double_diffs.values()])\n",
    "double_diff_rel_measure = np.abs(np.array([diff_res.vel for diff_res in double_diffs.values()]))\n",
    "\n",
    "# Validation figure and assessment\n",
    "validation_table, fig = display_validation(site_dist,                           # binned distance for point\n",
    "                                           double_diff_rel_measure,             # binned double-difference velocities mm/yr\n",
    "                                           site,                                # cal/val site name\n",
    "                                           start_date,                          # start date of InSAR dataset\n",
    "                                           end_date,                            # end date of InSAR dataset \n",
    "                                           requirement=secular_threshold_rqmt,  # measurement requirement to meet, e.g 2 mm/yr for 3 years of data over 0.1-50km\n",
    "                                           distance_rqmt=secular_distance_rqmt, # distance over requirement is to meet, e.g. over length scales of 0.1-50 km [0.1, 50] \n",
    "                                           n_bins=n_bins,                       # number of bins, to collect statistics \n",
    "                                           threshold=threshold,                 # quantile threshold for point-pairs that pass requirement, e.g. 0.683 - we expect 68.3% of residuals to lie. \n",
    "                                           sensor='Sentinel-1',                 # sensor that is validated, Sentinel-1 or NISAR\n",
    "                                           validation_type='secular',           # validation for: secular, transient, coseismic requirement\n",
    "                                           validation_data='GNSS')              # validation method: GNSS - Method 1, InSAR - Method 2\n",
    "\n",
    "out_fig = os.path.abspath('secular_insar-gnss_velocity_vs_distance.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_validation_table(validation_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Validation Method 1: success for a baseline distance bin occurs when the percentage of residuals within the requirement success threshold is >0.683\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='validation2'></a>\n",
    "## 6. Validation Method 2: InSAR-only Structure Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Validation Method 2, we use a time interval and area where we assume no deformation.  As with Method 1, we assume that the distribution of residuals is Gaussian and that the requirement success threshold represents a 1-sigma limit within which we expect 68.3% of residuals to lie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Read InSAR Array and Mask Pixels with no Data <a id='array_mask'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the assumed non-earthquake displacement as the insar_displacment for statistics and convert to mm\n",
    "insar_velocities,_ = readfile.read(vel_file, datasetName = 'velocity')  #read velocity\n",
    "velStart = sitedata['sites'][site]['download_start_date']\n",
    "insar_velocities = insar_velocities * 1000.  # convert velocities from m to mm\n",
    "\n",
    "# Set masked pixels to NaN\n",
    "msk,_ = readfile.read(msk_file)\n",
    "insar_velocities[msk == 0] = np.nan\n",
    "insar_velocities[insar_velocities == 0] = np.nan\n",
    "\n",
    "# Display map of velocity data after masking\n",
    "cmap = plt.get_cmap('RdBu_r')\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.imshow(insar_velocities, cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest', extent=(W, E, S, N))\n",
    "ax.set_title(\"Secular \\n Date \"+velStart)\n",
    "cbar1 = fig.colorbar(img1, ax=ax)\n",
    "cbar1.set_label('LOS velocity [mm/year]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend <a id='remove_trend'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define requirement\n",
    "secular_threshold_rqmt = 2  # mm/yr\n",
    "secular_distance_rqmt = [0.1, 50.0]  # km\n",
    "\n",
    "sample_mode = 'points'  # 'points' or 'profile'\n",
    "# note that the 'profile' method may take significantly longer\n",
    "\n",
    "# Collect samples using the specified method\n",
    "if sample_mode in ['points']:\n",
    "    X0,Y0 = load_geo(insar_metadata)\n",
    "    X0_2d,Y0_2d = np.meshgrid(X0,Y0)\n",
    "\n",
    "    insar_sample_dist, insar_rel_measure = samp_pair(X0_2d, Y0_2d, insar_velocities, num_samples=1000000)\n",
    "\n",
    "elif sample_mode in ['profile']:\n",
    "    # Sample grid setup\n",
    "    length, width = int(insar_metadata['LENGTH']), int(insar_metadata['WIDTH'])\n",
    "    X = np.linspace(W+lon_step, E-lon_step, width)  # longitudes\n",
    "    Y = np.linspace(N+lat_step, S-lat_step, length)  # latitudes\n",
    "    X_coords, Y_coords = np.meshgrid(X, Y)\n",
    "\n",
    "    # Draw random samples from map (without replacement)\n",
    "    num_samples = 20000\n",
    "    \n",
    "    # Retrieve profile samples\n",
    "    insar_sample_dist, insar_rel_measure = profile_samples(\\\n",
    "                    x=X_coords.reshape(-1,1),\n",
    "                    y=Y_coords.reshape(-1,1),\n",
    "                    data=insar_velocities,\n",
    "                    metadata=insar_metadata,\n",
    "                    len_rqmt=secular_distance_rqmt,\n",
    "                    num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Secular Requirement Validation: Method 2  <a id='secular_validation_method2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation parameters\n",
    "n_bins = 10\n",
    "threshold = 0.683  \n",
    "\n",
    "# Validation figure and assessment\n",
    "validation_table, fig = display_validation(insar_sample_dist,              # binned distance for point\n",
    "                                           insar_rel_measure,              # binned relative velocities mm/yr\n",
    "                                           site,                           # cal/val site name\n",
    "                                           start_date,                     # start date of InSAR dataset\n",
    "                                           end_date,                       # end date of InSAR dataset \n",
    "                                           requirement=secular_threshold_rqmt,  # measurement requirement to meet, e.g 2 mm/yr for 3 years of data over 0.1-50km\n",
    "                                           distance_rqmt=secular_distance_rqmt,  # distance over requirement is to meet, e.g. over length scales of 0.1-50 km [0.1, 50] \n",
    "                                           n_bins=n_bins,                  # number of bins, to collect statistics \n",
    "                                           threshold=threshold,            # quantile threshold for point-pairs that pass requirement, e.g. 0.683 - we expect 68.3% of residuals to lie. \n",
    "                                           sensor='Sentinel-1',            # sensor that is validated, Sentinel-1 or NISAR\n",
    "                                           validation_type='secular',      # validation for: secular, transient, coseismic requirement\n",
    "                                           validation_data='InSAR')        # validation method: GNSS - Method 1, InSAR - Method 2\n",
    "\n",
    "out_fig = os.path.abspath('secular_insar-only_vs_distance_'+site+'_date'+velStart+'.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_validation_table(validation_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2—\n",
    "    68% of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='appendix'></a>\n",
    "## Appendix: Supplementary Comparisons and Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A.1. Compare Raw Velocities <a id='compare_raw'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.hist([insar_site.vel for insar_site in insar_sites.values()],\n",
    "         range=[vmin, vmax], bins=50, color=\"green\", edgecolor='grey', label='V_InSAR')\n",
    "plt.hist([gnss_site.vel for gnss_site in gnss_sites.values()],\n",
    "         range=[vmin, vmax], bins=50, color=\"orange\", edgecolor='grey', label='V_gnss', alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f'''Velocities \\n Date range {start_date}-{end_date}\n",
    "Reference stn: {sitedata['sites'][site]['gps_ref_site_name']}\n",
    "Number of stations used: {len(site_names)}''')\n",
    "plt.xlabel('LOS Velocity (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.ylim(0, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A.2. Plot Velocity Residuals <a id='plot_residuals'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.hist([(insar_sites[site_name] - gnss_sites[site_name]).vel for site_name in site_names],\n",
    "         bins = 40, range=[vmin,vmax], edgecolor='grey', color=\"darkblue\", linewidth=1,\n",
    "         label='V_gnss - V_InSAR (area average)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f'''Residuals \\n Date range {start_date}-{end_date}\n",
    "Reference stn: {sitedata['sites'][site]['gps_ref_site_name']}\n",
    "Number of stations used: {len(site_names)}''')\n",
    "plt.xlabel('Velocity Residual (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A.3. Plot Double Difference Residuals <a id='plot_ddiff'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.hist([diff_res.vel for diff_res in double_diffs.values()],\n",
    "         range = [vmin, vmax],bins = 40, color = \"darkblue\",edgecolor='grey',\n",
    "         label='V_gnss_(s1-s2) - V_InSAR_(s1-s2)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f'''Difference Residualts \\n Date range {start_date}-{end_date}\n",
    "Reference stn: {sitedata['sites'][site]['gps_ref_site_name']}\n",
    "Number of stations used: {len(site_names)}''')\n",
    "plt.xlabel('Double Differenced Velocity Residual (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A.4. GNSS Timeseries Plots <a id='plot_gps_tseries'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab the time-series file used for time function estimation given the template setup\n",
    "template = readfile.read_template(os.path.join(mintpy_dir, 'smallbaselineApp.cfg'))\n",
    "template = ut.check_template_auto_value(template)\n",
    "ts_file = TimeSeriesAnalysis.get_timeseries_filename(template, mintpy_dir)['velocity']['input']\n",
    "\n",
    "# Read the time-series file\n",
    "insar_ts, ts_metadata = readfile.read(ts_file, datasetName='timeseries')\n",
    "mask = readfile.read(os.path.join(mintpy_dir, 'maskTempCoh.h5'))[0]\n",
    "print(f\"reading timeseries from file: {ts_file}\")\n",
    "\n",
    "# Get date list\n",
    "date_list = timeseries(ts_file).get_date_list()\n",
    "num_date = len(date_list)\n",
    "date0, date1 = date_list[0], date_list[-1]\n",
    "insar_dates = ptime.date_list2vector(date_list)[0]\n",
    "\n",
    "# Spatial reference\n",
    "coord = ut.coordinate(ts_metadata)\n",
    "ref_site = sitedata['sites'][site]['gps_ref_site_name']\n",
    "ref_gnss_obj = gnss_stns[ref_site]\n",
    "ref_lat, ref_lon = ref_gnss_obj.get_site_lat_lon()\n",
    "ref_y, ref_x = coord.geo2radar(ref_lat, ref_lon)[:2]\n",
    "if not mask[ref_y, ref_x]:\n",
    "    raise ValueError(f'Given reference GNSS site ({ref_site}) is in mask-out unrelible region in InSAR! Change to a different site.')\n",
    "ref_insar_dis = insar_ts[:, ref_y, ref_x]\n",
    "\n",
    "# Plot displacements and velocity timeseries at GNSS station locations\n",
    "num_site = len(site_names)\n",
    "prog_bar = ptime.progressBar(maxValue=num_site)\n",
    "for i, site_name in enumerate(site_names):\n",
    "    prog_bar.update(i+1, suffix=f'{site_name} {i+1}/{num_site}')\n",
    "\n",
    "    ## Read data\n",
    "    # Recall gnss station displacements with outliers removed\n",
    "    gnss_obj = gnss_stns[site_name]\n",
    "    gnss_lalo = (gnss_obj.site_lat, gnss_obj.site_lon)\n",
    "\n",
    "    # Get relative LOS displacement on common dates\n",
    "    gnss_dates = np.array(sorted(list(set(gnss_obj.dates) & set(ref_gnss_obj.dates))))\n",
    "    gnss_dis = np.zeros(gnss_dates.shape, dtype=np.float32)\n",
    "    for i, date_i in enumerate(gnss_dates):\n",
    "        idx1 = np.where(gnss_obj.dates == date_i)[0][0]\n",
    "        idx2 = np.where(ref_gnss_obj.dates == date_i)[0][0]\n",
    "        gnss_dis[i] = gnss_obj.dis_los[idx1] - ref_gnss_obj.dis_los[idx2]\n",
    "    \n",
    "    # Shift GNSS to zero-mean in time [for plotting purpose]\n",
    "    gnss_dis -= np.nanmedian(gnss_dis)\n",
    "\n",
    "    # Read InSAR\n",
    "    y, x = coord.geo2radar(gnss_lalo[0], gnss_lalo[1])[:2]\n",
    "    insar_dis = insar_ts[:, y, x] - ref_insar_dis\n",
    "\n",
    "    # Apply a constant shift in time to fit InSAR to GNSS\n",
    "    comm_dates = sorted(list(set(gnss_dates) & set(insar_dates)))\n",
    "    if comm_dates:\n",
    "        insar_flag = [x in comm_dates for x in insar_dates]\n",
    "        gnss_flag = [x in comm_dates for x in gnss_dates]\n",
    "        insar_dis -= np.nanmedian(insar_dis[insar_flag] - gnss_dis[gnss_flag])\n",
    "\n",
    "    ## Plot figure\n",
    "    if gnss_dis.size > 0 and np.any(~np.isnan(insar_dis)):\n",
    "        fig, ax = plt.subplots(figsize=(10, 3))\n",
    "        ax.axhline(color='grey',linestyle='dashed', linewidth=2)\n",
    "        ax.scatter(gnss_dates, gnss_dis*1000, s=2**2, label=\"GNSS Daily Positions\")\n",
    "        ax.scatter(insar_dates, insar_dis*1000, label=\"InSAR Positions\")\n",
    "        # axis format\n",
    "        ax.set_title(f\"Station Name: {site_name}\") \n",
    "        ax.set_ylabel('LOS displacement [mm]')\n",
    "        ax.legend()\n",
    "prog_bar.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:.local-solid_earth_atbd_dev]",
   "language": "python",
   "name": "conda-env-.local-solid_earth_atbd_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
