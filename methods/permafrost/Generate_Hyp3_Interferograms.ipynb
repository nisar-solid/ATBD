{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edfb36c0-c9dc-4f1f-8e9a-95af2ac7dea4",
   "metadata": {},
   "source": [
    "## Generate Hyp3 interferogram stack\n",
    "\n",
    "This notebook will queue up processing jobs in Hyp3 and \n",
    "then download and prepare them for use in Mintpy. \n",
    "Code adapted from [Hyp3 tutorial](https://nbviewer.org/github/ASFHyP3/hyp3-docs/blob/main/docs/tutorials/hyp3_insar_stack_for_ts_analysis.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad819ee-520c-4eba-811e-56c029b91170",
   "metadata": {},
   "source": [
    "### Prepare environment\n",
    "\n",
    "Import functions and load metadata for processing. The environment requires several packages from the Alaska Satellite Facility to function, specifically [asf_search](https://docs.asf.alaska.edu/asf_search/basics/) and [hyp3_sdk](https://hyp3-docs.asf.alaska.edu/using/sdk/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4d218-7be9-44df-991f-695860da7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dateutil.parser import parse as parse_date\n",
    "from typing import List, Union\n",
    "from osgeo import gdal\n",
    "from solid_utils import permafrost_utils as pu\n",
    "import asf_search as asf\n",
    "import hyp3_sdk as sdk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658aa85e-27d0-4738-bee9-e2e3628e68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Set Directories, get site info ##############################\n",
    "site = 'NorthSlopeEastD102'\n",
    "requirement = 'permafrost'\n",
    "dataset = 'Hyp3_S1'\n",
    "year = 2025\n",
    "start_directory = 'default'\n",
    "\n",
    "custom_sites = \"/home/jovyan/my_sites.txt\"  # Path to custom site metadata\n",
    "try:\n",
    "    with open(custom_sites, \"r\") as f:\n",
    "        sitedata = json.load(f)\n",
    "    site_info = sitedata[\"sites\"][site]\n",
    "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "    raise RuntimeError(f\"Failed to load site metadata from {custom_sites}: {e}\")\n",
    "except KeyError:\n",
    "    raise ValueError(f\"Site ID '{site}' not found in {custom_sites}\")\n",
    "\n",
    "scratch_path = \"/scratch/nisar-st-calval-solidearth/permafrost/Hyp3_S1\"\n",
    "work_dir = Path(f'{scratch_path}/{site}/{str(year)}')\n",
    "print(\"Work directory:\", work_dir)\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Change to Workdir   \n",
    "os.chdir(work_dir)\n",
    "       \n",
    "hyp3_dir = work_dir/'products'               #aka gunwdir\n",
    "hyp3_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"   Hyp3  dir:\", hyp3_dir) \n",
    "    \n",
    "mintpy_dir = work_dir/'MintPy' \n",
    "mintpy_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"   MintPy  dir:\", mintpy_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7aaa29-ff20-4cdf-909a-6b9716d65a40",
   "metadata": {},
   "source": [
    "### Generate SBAS image pairs\n",
    "\n",
    "Identify stack of SLC images and determine SBAS pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b78039-a20c-4e72-8660-c80b6cf78cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat = site_info['region_identifier']\n",
    "frame = site_info['sentinel_frame']\n",
    "if site_info['sentinel_direction']=='ASCENDING':\n",
    "    direction = asf.constants.ASCENDING\n",
    "elif site_info['sentinel_direction']=='DESCENDING':\n",
    "    direction = asf.constants.DESCENDING\n",
    "\n",
    "stack_start = parse_date(f'{year}-05-20 00:00:00Z')\n",
    "stack_end = parse_date(f'{year}-09-20 00:00:00Z')\n",
    "\n",
    "search_results = asf.geo_search(platform=asf.constants.SENTINEL1, \n",
    "                                intersectsWith=lonlat, \n",
    "                                start=stack_start,\n",
    "                                end=stack_end,\n",
    "                                processingLevel=asf.constants.SLC,\n",
    "                                beamMode=asf.constants.IW,\n",
    "                                flightDirection=direction,\n",
    "                                asfFrame=int(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927ef76-8002-4dd0-b44f-8cfd48033f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = asf.baseline_search.stack_from_product(search_results[-1])\n",
    "\n",
    "columns = list(baseline_results[0].properties.keys()) + ['geometry', ]\n",
    "data = [list(scene.properties.values()) + [scene.geometry, ] \n",
    "        for scene in baseline_results]\n",
    "\n",
    "stack = pd.DataFrame(data, columns=columns)\n",
    "stack['startTime'] = stack.startTime.apply(parse_date)\n",
    "\n",
    "stack = stack.loc[(stack_start <= stack.startTime) & (stack.startTime <= stack_end)]\n",
    "\n",
    "stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a5947-61f5-494f-a59f-2917286a432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbas_pairs = set()\n",
    "\n",
    "for reference, rt in stack.loc[::-1, ['sceneName', 'temporalBaseline']].itertuples(\n",
    "    index=False):\n",
    "    secondaries = stack.loc[\n",
    "        (stack.sceneName != reference)\n",
    "        & (stack.temporalBaseline - rt <= int(site_info['tempBaseMax']))\n",
    "        & (stack.temporalBaseline - rt > 0)\n",
    "    ]\n",
    "    for secondary in secondaries.sceneName:\n",
    "        sbas_pairs.add((reference, secondary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acece7fe-01d7-4538-8408-e8ee80cc7c6e",
   "metadata": {},
   "source": [
    "### Do Hyp3 Processing\n",
    "\n",
    "This step requires NASA Earthdata credentials. See [Hyp3 documentation](https://hyp3-docs.asf.alaska.edu/) for more information about using Hyp3 processing. Each user has an allotted Hyp3 processing quota, for which some of this processing is counted against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a87c3d-5704-4142-9bd6-8e60cd3c5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyp3 = sdk.HyP3(prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86270ef1-dd29-42bb-9e1e-e2e4e5b4d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jobs = sdk.Batch()\n",
    "h3projname = f\"{site}_NCV_{year}\"\n",
    "for reference, secondary in sbas_pairs:\n",
    "    jobs += hyp3.submit_insar_job(reference, secondary, name=h3projname,\n",
    "                                  include_dem=True, include_look_vectors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28afea4-b108-4559-9f26-7557465789ec",
   "metadata": {},
   "source": [
    "This processing will likely take about an hour to complete. You cannot download data until the processing is finished. The following cell will check to see if the processing is still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cbadf-6264-446f-b720-eb463a592ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "running = len(hyp3.find_jobs(status_code=[\"RUNNING\"]))\n",
    "pending = len(hyp3.find_jobs(status_code=[\"PENDING\"]))\n",
    "\n",
    "if running+pending ==0:\n",
    "    print('No jobs running, all clear')\n",
    "else:\n",
    "    print(f'Jobs still running!\\n{pending} pending, {running} running')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ba561-b415-4663-b45d-a1aad18059a9",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0586588a-94ee-47e2-a2f5-dc1cfa7b8008",
   "metadata": {},
   "source": [
    "\n",
    "h3projname = f\"{site}_NCV_{year}\"\n",
    "batch = hyp3.find_jobs(name=h3projname)\n",
    "insar_products = batch.download_files(hyp3_dir)\n",
    "insar_products = [sdk.util.extract_zipped_product(ii) for ii in insar_products]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54e84a-d404-42dc-91f6-d56e74a383bd",
   "metadata": {},
   "source": [
    "### Crop Interferograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659c967-bfe5-4325-a39d-e63a8a63ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop to common overlap\n",
    "files = hyp3_dir.glob('*/*_dem.tif')\n",
    "overlap = pu.get_common_overlap(files)\n",
    "pu.clip_hyp3_products_to_common_overlap(hyp3_dir, overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1192d9a-2716-4093-b3ae-a34df331fa13",
   "metadata": {},
   "source": [
    "## Generate MintPy Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a86d3-7a04-4c4a-925c-8998103fad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writenumpyconfig(config_file,data_dir,reflalo = None, subset = None):\n",
    "    \"\"\"Write a mintpy config file.\n",
    "    config_file: file name to write to\n",
    "    data_dir: directory of .tif files\n",
    "    reflalo (optional): 'y,x' of reference point in tif proj\n",
    "    subset (optional): '[ymin:ymax,xmin:xmax]' in tif proj\n",
    "    \n",
    "    Note: hyp3 provides images in a utm projection, so \n",
    "    lalo here refers to y,x in map coords\"\"\"\n",
    "    \n",
    "    cfgtext = f\"\"\"\n",
    "    mintpy.load.processor        = hyp3\n",
    "    ##---------interferogram datasets:\n",
    "    mintpy.load.unwFile          = {data_dir}/*/*_unw_phase_clipped.tif\n",
    "    mintpy.load.corFile          = {data_dir}/*/*_corr_clipped.tif\n",
    "    ##---------geometry datasets:\n",
    "    mintpy.load.demFile          = {data_dir}/*/*_dem_clipped.tif\n",
    "    mintpy.load.incAngleFile     = {data_dir}/*/*_lv_theta_clipped.tif\n",
    "    mintpy.load.azAngleFile      = {data_dir}/*/*_lv_phi_clipped.tif\n",
    "    mintpy.load.waterMaskFile    = {data_dir}/*/*_water_mask_clipped.tif\"\"\"\n",
    "\n",
    "    if reflalo:\n",
    "            cfgtext+=f\"\"\"    \n",
    "    mintpy.reference.lalo         = {reflalo}\"\"\"  #should be 'y,x' in map coords\n",
    "    \n",
    "    if not subset:\n",
    "        subset = 'no'\n",
    "    cfgtext+=f\"\"\"\n",
    "    mintpy.subset.lalo            = {subset}\"\"\" #should be '[ymin:ymax,xmin:xmax]'\n",
    "                                                #in map coords\n",
    "        \n",
    "    mintpy_config = config_file\n",
    "    mintpy_config.write_text(cfgtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dcce3c-9773-4d3a-bc08-d44915833a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = mintpy_dir/f\"{site_info['calval_location']}_{str(year)}.cfg\"\n",
    "writenumpyconfig(config_file,\n",
    "                 hyp3_dir,\n",
    "                 reflalo=site_info['mintpy_ref_loc'],\n",
    "                 subset = site_info['subset_region'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solid_earth_atbd",
   "language": "python",
   "name": "solid_earth_atbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
