{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Workflow to Validate NISAR L2 Coseismic Displacement Requirement\n",
    "\n",
    "**Original code authored by:** David Bekaert, Heresh Fattahi, Eric Fielding, and Zhang Yunjun \n",
    "\n",
    "Extensive modifications by Adrian Borsa and Amy Whetter 2022\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Both the initial setup (<b>Prep A</b> section) and download of the data (<b>Prep B</b> section) should be run at the start of the notebook. And all subsequent sections NEED to be run in order.\n",
    "</div>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## Define CalVal Site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose a site and track direction\n",
    "site='MojaveD173' \n",
    "\n",
    "# What dataset are you processing?\n",
    "dataset = 'ARIA_S1' # For Sentinel-1 testing with aria-tools\n",
    "\n",
    "# The date and version of this Cal/Val run\n",
    "today = '20240429'\n",
    "version = '1'\n",
    "\n",
    "# Define your directory structure - you won't need to change this line\n",
    "start_directory = '/scratch/nisar-st-calval-solidearth' \n",
    "\n",
    "# The file where you keep your customized list of sites.\n",
    "custom_sites = '/home/jovyan/my_sites.txt'\n",
    "\n",
    "# Please enter a name or username that will determine where your outputs are stored\n",
    "import os\n",
    "if os.path.exists('/home/jovyan/me.txt'): # if OpenTopo API key already installed\n",
    "    with open('/home/jovyan/me.txt') as m:\n",
    "        you = m.readline().strip()\n",
    "    print('You are', you)\n",
    "    print('Using this as the name of the directory where your outputs will be stored.')\n",
    "    print('Directory structure: start_directory / dataset/ requirement / site / you / today / version ')\n",
    "    print('For example: /scratch/nisar-st-calval-solidearth/ARIA_S1/Coseismic/MojaveD173/aborsa/20240424/v1/')\n",
    "else:\n",
    "    print('We need a name or username (determines where your outputs will be stored)')\n",
    "    print('Directory structure: start_directory / dataset/ requirement / site / you / today / version ')\n",
    "    print('For example: /scratch/nisar-st-calval-solidearth/ARIA_S1/Coseismic/MojaveD173/aborsa/20240424/v1/')\n",
    "    you = input('Please type your name:')\n",
    "    with open ('/home/jovyan/me.txt', 'w') as m: \n",
    "        m.write(you)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of Contents:\n",
    "<a id='example_TOC'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr/>\n",
    "\n",
    "[**Environment Setup**](#prep_a)\n",
    "\n",
    "[**1. Generation of Time Series from Interferograms**](#gen_ts)\n",
    "- [1.1. Validate/Modify Interferogram Network](#validate_network)\n",
    "- [1.2. Generate Quality Control Mask](#generate_mask)\n",
    "- [1.3. Reference Interferograms To Common Lat/Lon](#common_latlon)\n",
    "- [1.4. Invert for SBAS Line-of-Sight Timeseries](#invert_SBAS)\n",
    "\n",
    "[**2. Optional Corrections**](#opt_correction)\n",
    "- [2.1. Solid Earth Tides Correction](#solid_earth)\n",
    "- [2.2. Tropospheric Delay Correction](#tropo_corr)\n",
    "- [2.3. Phase Deramping ](#phase_deramp)\n",
    "- [2.4. Topographic Residual Correction ](#topo_corr) \n",
    "\n",
    "[**3. Decomposition of InSAR and GNSS Time Series into Basis Functions**](#decomp_ts)\n",
    "- [3.1. Estimate InSAR LOS Velocities](#insar_vel1)\n",
    "- [3.2. Estimate InSAR Coseismic Displacement](#co_step1)\n",
    "- [3.3. Find Collocated GNSS Stations](#co_gps)  \n",
    "- [3.4. Get GNSS Position Time Series](#gps_ts) \n",
    "- [3.5. Make GNSS LOS Velocities](#gps_los)\n",
    "- [3.6. Re-reference GNSS and InSAR LOS Coseismic Step](#gps_insar)\n",
    "\n",
    "[**4. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison**](#nisar_validation)\n",
    "- [4.1. Make Displacement Residuals at GNSS Locations](#make_vel)\n",
    "- [4.2. Make Double-differenced Displacement Residuals](#make_velres1)\n",
    "- [4.3. Amplitude vs. Distance of Double-differences (not quite a structure function)](#amp_vs_dist)\n",
    "- [4.4. GNSS-InSAR Residuals Analysis with Step Function](#nisar_anal)\n",
    "- [4.5. Make Double-differenced Displacement Residuals Step](#make_velres2)\n",
    "- [4.6. Amplitude vs. Distance of Double-differences Step (not quite a structure function)](#ampvsdist2)\n",
    "- [4.7. Coseismic Requirement Validation: Method 1](#coseismic_valid_method1)\n",
    "\n",
    "[**5. NISAR Validation Approach 2: InSAR-only Structure Function**](#nisar_validation2)\n",
    "- [5.1. Read Array and Mask Pixels with no Data](#array_mask)\n",
    "- [5.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend](#remove_trend)\n",
    "- [5.3. Amplitude vs. Distance of Relative Measurements (pair differences)](#M2ampvsdist2)\n",
    "- [5.4. Bin Sample Pairs by Distance Bin and Calculate Statistics](#M2RelMeasTable)\n",
    "\n",
    "[**Appendix: Supplementary Comparisons and Plots**](#appendix)\n",
    "- [A.1. Compare Raw Velocities](#compare_raw)\n",
    "- [A.2. Plot Velocity Residuals](#plot_vel)\n",
    "- [A.3. Plot Double-differenced Residuals](#plot_velres)\n",
    "- [A.4. GPS Position Plot](#appendix_gps)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Environment setup\n",
    "\n",
    "Find your workspace and the MintPy-readable stack you created previously with one of the data prep notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load Packages\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from mintpy.cli import view, plot_network\n",
    "from mintpy.objects import gnss, timeseries\n",
    "from mintpy.smallbaselineApp import TimeSeriesAnalysis\n",
    "from mintpy.utils import ptime, readfile, time_func, utils as ut\n",
    "from scipy import signal\n",
    "\n",
    "from solid_utils.sampling import load_geo, samp_pair, profile_samples, haversine_distance\n",
    "\n",
    "#Set Global Plot Parameters\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "################# Set Directories ##########################################\n",
    "requirement='Coseismic'\n",
    "work_dir = os.path.join(start_directory,dataset,requirement,site,you,today,'v'+version)\n",
    "print(\"Work directory:\", work_dir)\n",
    "\n",
    "gunw_dir = os.path.join(work_dir,'products')\n",
    "print(\"   GUNW    dir:\", gunw_dir) \n",
    "\n",
    "mintpy_dir = os.path.join(work_dir,'MintPy')\n",
    "print(\"   MintPy  dir:\", mintpy_dir)\n",
    "### Change to MintPy workdir\n",
    "if not os.path.exists(mintpy_dir):\n",
    "    print()\n",
    "    print('ERROR: Stop! Your MintPy processing directory does not exist for this requirement, site, version, or date of your ATBD run.')\n",
    "    print('You may need to run the prep notebook first!')\n",
    "    print()\n",
    "else:\n",
    "    os.chdir(mintpy_dir)\n",
    "\n",
    "vel_file = os.path.join(mintpy_dir, 'velocity.h5')\n",
    "msk_file = os.path.join(mintpy_dir, 'maskConnComp.h5')  # maskTempCoh.h5 maskConnComp.h5\n",
    "\n",
    "with open(custom_sites,'r') as fid:\n",
    "    sitedata = json.load(fid)\n",
    "\n",
    "sitedata['sites'][site]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Generation of Time Series from Interferograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='validate_network'></a>\n",
    "## 1.1. Validate/Modify Interferogram Network\n",
    "\n",
    "Add additional parameters to config_file in order to remove selected interferograms, change minimum coherence, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_file = os.path.join(mintpy_dir,site + '.cfg')\n",
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep modify_network'\n",
    "process = subprocess.run(command, shell=True)\n",
    "\n",
    "plot_network.main(['inputs/ifgramStack.h5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='generate_mask'></a>\n",
    "## 2.4. Generate Quality Control Mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask files can be can be used to mask pixels in the time-series processing. Below we generate an initial mask file `maskConnComp.h5` based on the connected components for all the interferograms, which is a metric for unwrapping quality. After time-series analysis is complete, we will calculate a mask from the temporal coherence or variation of phase or displacement with time to make `maskTempCoh.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command='generate_mask.py inputs/ifgramStack.h5  --nonzero  -o maskConnComp.h5  --update'\n",
    "process = subprocess.run(command, shell=True)\n",
    "view.main(['maskConnComp.h5', 'mask'])\n",
    "msk_file = os.path.join(mintpy_dir, 'maskTempCoh.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='common_latlon'></a>\n",
    "## 1.2. Reference Interferograms To Common Lat/Lon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep reference_point'\n",
    "process = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "os.system('info.py inputs/ifgramStack.h5 | egrep \"REF_\"');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='invert_SBAS'></a>\n",
    "## 1.3. Invert for SBAS Line-of-Sight Timeseries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep invert_network'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='opt_correction'></a>\n",
    "# 2. Optional Corrections\n",
    "\n",
    "Phase distortions related to solid earth and ocean tidal effects as well as those due to temporal variations in the vertical stratification of the atmosphere can be mitigated using the approaches described below. At this point, it is expected that these corrections will not be needed to validate the mission requirements, but they may be used to produce the highest quality data products. Typically, these are applied to the estimated time series product rather than to the individual interferograms, since they are a function of the time of each radar acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='solid_earth'></a>\n",
    "## 2.1. Solid Earth Tides Correction\n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='tropo_corr'></a>\n",
    "## 2.2. Tropospheric Delay Correction\n",
    "\n",
    "Optional atmospheric correction utilizes the PyAPS (Jolivet et al., 2011, Jolivet and Agram, 2012) module within GIAnT (or eventually a merged replacement for GIAnT and MintPy). PyAPS is well documented, maintained and can be freely downloaded. PyAPS is included in GIAnT distribution). PyAPS currently includes support for ECMWF’s ERA-Interim, NOAA’s NARR and NASA’s MERRA weather models. A final selection of atmospheric models to be used for operational NISAR processing will be done during Phase C.\n",
    "\n",
    "[T]ropospheric delay maps are produced from atmospheric data provided by Global Atmospheric Models. This method aims to correct differential atmospheric delay correlated with the topography in interferometric phase measurements. Global Atmospheric Models (hereafter GAMs)... provide estimates of the air temperature, the atmospheric pressure and the humidity as a function of elevation on a coarse resolution latitude/longitude grid. In PyAPS, we use this 3D distribution of atmospheric variables to determine the atmospheric phase delay on each pixel of each interferogram.\n",
    "\n",
    "The absolute atmospheric delay is computed at each SAR acquisition date. For a pixel a_i at an elevation z at acquisition date i, the four surrounding grid points are selected and the delays for their respective elevations are computed. The resulting delay at the pixel a_i is then the bilinear interpolation between the delays at the four grid points. Finally, we combine the absolute delay maps of the InSAR partner images to produce the differential delay maps used to correct the interferograms.\n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "do_tropo_correction = False\n",
    "########################################################################\n",
    "'''\n",
    "REFERENCE : https://github.com/insarlab/pyaps#2-account-setup-for-era5\n",
    "Read Section 2 for ERA5 [link above] to create an account on the CDS website.\n",
    "'''\n",
    "\n",
    "if do_tropo_correction:\n",
    "    if not Use_Staged_Data and not os.path.exists(Path.home()/'.cdsapirc'):\n",
    "        print('NEEDED to download ERA5, link: https://cds.climate.copernicus.eu/user/register')\n",
    "        UID = input('Please type your CDS_UID:')\n",
    "        CDS_API = input('Please type your CDS_API:')\n",
    "        \n",
    "        cds_tmp = '''url: https://cds.climate.copernicus.eu/api/v2\n",
    "        key: {UID}:{CDS_API}'''.format(UID=UID, CDS_API=CDS_API)\n",
    "        os.system('echo \"{cds_tmp}\" > ~/.cdsapirc; chmod 600 ~/.cdsapirc'.format(cds_tmp = str(cds_tmp)))\n",
    "    \n",
    "    command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep correct_troposphere'\n",
    "    process = subprocess.run(command, shell=True)\n",
    "    \n",
    "    view.main(['inputs/ERA5.h5'])\n",
    "    timeseries_filename = 'timeseries_ERA5.h5'\n",
    "else:\n",
    "    timeseries_filename = 'timeseries.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='phase_deramp'></a>\n",
    "## 2.3. Phase Deramping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep deramp'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='topo_corr'></a>\n",
    "## 2.4. Topographic Residual Correction\n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep correct_topography'\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='decomp_ts'></a>\n",
    "# 3. Decomposition of InSAR and GNSS Time Series into Basis Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='insar_vel1'></a>\n",
    "## 3.1. Estimate InSAR LOS Velocities\n",
    "\n",
    "Given a time series of InSAR LOS displacements, the observations for a given pixel, $U(t)$, can be parameterized as:\n",
    "\n",
    "$$ U(t) = a \\;+\\; vt \\;+\\; c_1 cos (\\omega_1t - \\phi_{1,}) \\;+\\; c_2 cos (\\omega_2t - \\phi_2) \\;+\\; \\sum_{j=1}^{N_{eq}} \\left( h_j+f_j F_j (t-t_j) \\right)H(t - t_j) \\;+\\; \\frac{B_\\perp (t)}{R sin \\theta}\\delta z \\;+\\; residual$$ \n",
    "\n",
    "which includes a constant offset $(a)$, velocity $(v)$, and amplitudes $(c_j)$ and phases $(\\phi_j)$ of annual $(\\omega_1)$ and semiannual $(\\omega_2)$ sinusoidal terms.  Where needed we can include additional complexity, such as coseismic and postseismic processes parameterized by Heaviside (step) functions $H$ and postseismic functions $F$ (the latter typically exponential and/or logarithmic).   $B_\\perp(t)$, $R$, $\\theta$, and $\\delta z$ are, respectively, the perpendicular component of the interferometric baseline relative to the first date, slant range distance, incidence angle and topography error correction for the given pixel. \n",
    "\n",
    "Thus, given either an ensemble of interferograms or the output of SBAS (displacement vs. time), we can write the LSQ problem as \n",
    "\n",
    "$$ \\textbf{G}\\textbf{m} = \\textbf{d}$$\n",
    "\n",
    "where $\\textbf{G}$ is the design matrix (constructed out of the different functional terms in Equation 2 evaluated either at the SAR image dates for SBAS output, or between the dates spanned by each pair for interferograms), $\\textbf{m}$ is the vector of model parameters (the coefficients in Equation 2) and $\\textbf{d}$ is the vector of observations.  For GPS time series, $\\textbf{G}, \\textbf{d}, \\textbf{m}$ are constructed using values evaluated at single epochs corresponding to the GPS solution times, as for SBAS InSAR input. \n",
    "\n",
    "With this formulation, we can obtain InSAR velocity estimates and their formal uncertainties (including in areas where the expected answer is zero). \n",
    "\n",
    "The default InSAR velocity fit in MintPy is to estimate a mean linear velocity $(v)$ in in the equation, which we do below. This is the same linear velocity used in the Secular notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(mintpy_dir)\n",
    "#command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep velocity'\n",
    "command = 'timeseries2velocity.py ' + timeseries_filename \n",
    "process = subprocess.run(command, shell=True)\n",
    "vel_file = os.path.join(mintpy_dir, 'velocity.h5')\n",
    "vel = readfile.read(vel_file, datasetName = 'velocity')[0] * 100.  #read and convert velocities from m to cm\n",
    "\n",
    "# optionally set masked pixels to NaN\n",
    "#msk = readfile.read(msk_file)[0]\n",
    "#vel[msk == 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the mean linear velocity fit. The MintPy `view` module automatically reads the temporal coherence mask `maskTempCoh.h5` and applies that to mask out pixels with unreliable velocities (white). Because there is a large coseismic displacement at the time of the earthquake but we are only estimating a single linear velocity, it absorbs the coseismic signal in this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scp_args = 'velocity.h5 velocity -v -25 25 --ref-yx 200 950 --colormap RdBu --figtitle LOS_Velocity' # --plot-setting ' + plot_config_file\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Note :</b> \n",
    "Negative values indicates that target is moving away from the radar (i.e., Subsidence in case of vertical deformation).\n",
    "Positive values indicates that target is moving towards the radar (i.e., uplift in case of vertical deformation). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='co_step1'></a>\n",
    "## 3.2. Estimate InSAR Coseismic Displacement\n",
    "\n",
    "We can use the same time series to estimate the coseismic displacement as a Heaviside $H$ or step function at the time of an earthquake or a number $N_{eq}$ of earthquakes. In the above equation this is the $$\\sum_{j=1}^{N_{eq}} \\left( h_j+f_j F_j (t-t_j) \\right)H(t - t_j) \\;$$ set of terms. For simplicity, we consider only one earthquake and we assume the postseismic displacement functions $F$ are small compared to the coseismic displacements, so we only need to solve for the coefficient $h$ of each interferogram pixel.\n",
    "\n",
    "We call the MintPy `timeseries2velocity.py` program again and specify the time of the earthquake $t_j$. The fit will also include the linear velocity rate separated from the step function amplitude. Both estimated coefficients have their associated uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'timeseries2velocity.py ' + timeseries_filename + ' --step ' + sitedata['sites'][site]['earthquakeDate']\n",
    "process = subprocess.run(command, shell=True)\n",
    "\n",
    "EQdataset = 'step' + sitedata['sites'][site]['earthquakeDate']\n",
    "EQstep = readfile.read(vel_file, datasetName = EQdataset)[0] * 100.  #read and convert coseismic step from m to cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can view the step function amplitude and the new linear velocity estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scp_args = 'velocity.h5 step' + sitedata['sites'][site]['earthquakeDate'] + ' -v -40 40 --ref-yx 200 950 --colormap RdBu --figtitle LOS_Coseismic' # --plot-setting ' + plot_config_file\n",
    "view.main(scp_args.split())\n",
    "scp_args = 'velocity.h5 velocity -v -25 25 --ref-yx 200 950 --colormap RdBu --figtitle LOS_Velocity' # --plot-setting ' + plot_config_file\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='co_gps'></a>\n",
    "## 3.3. Find Collocated GNSS Stations\n",
    "\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get analysis metadata from InSAR velocity file\n",
    "atr = readfile.read_attribute(vel_file)\n",
    "length, width = int(atr['LENGTH']), int(atr['WIDTH'])\n",
    "lat_step = float(atr['Y_STEP'])\n",
    "lon_step = float(atr['X_STEP'])\n",
    "N = float(atr['Y_FIRST'])\n",
    "W = float(atr['X_FIRST'])\n",
    "S = N + lat_step * length\n",
    "E = W + lon_step * width\n",
    "start_date = atr.get('START_DATE', None)\n",
    "end_date = atr.get('END_DATE', None)\n",
    "start_date_gnss = dt.strptime(start_date, \"%Y%m%d\")\n",
    "end_date_gnss = dt.strptime(end_date, \"%Y%m%d\")\n",
    "#inc_angle = int(float(atr.get('incidenceAngle', None)))\n",
    "#az_angle = int(float(atr.get('azimuthAngle', None))) \n",
    "inc_angle = 32.5\n",
    "az_angle = 0.0\n",
    "\n",
    "#Set GNSS Parameters\n",
    "gnss_completeness_threshold = 0.9     #0.9  #percent of data timespan with valid GNSS epochs\n",
    "gnss_residual_stdev_threshold = 20.   #0.03  #0.03  #max threshold standard deviation of residuals to linear GNSS fit\n",
    "\n",
    "# search for collocated GNSS stations\n",
    "site_names, site_lats, site_lons = gnss.search_gnss(SNWE=(S,N,W,E), \n",
    "                                                  start_date=start_date_gnss.strftime('%Y%m%d'),\n",
    "                                                  end_date=end_date_gnss.strftime('%Y%m%d'))\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='gps_ts'></a>\n",
    "## 3.4. Get GNSS Position Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get daily position solutions for GNSS stations\n",
    "use_stn = []  #stations to keep\n",
    "bad_stn = []  #stations to toss\n",
    "use_lats = [] \n",
    "use_lons = []\n",
    "counter = 0\n",
    "\n",
    "for stn in site_names:\n",
    "    gnss_obj = gnss.get_gnss_class('UNR')(site = stn, data_dir = os.path.join(mintpy_dir,'GNSS'))\n",
    "    gnss_obj.open(print_msg='False')\n",
    "    \n",
    "    # count number of dates in time range\n",
    "    dates = gnss_obj.dates\n",
    "    range_days = (end_date_gnss - start_date_gnss).days\n",
    "    gnss_count = np.histogram(dates, bins=[start_date_gnss,end_date_gnss])\n",
    "    gnss_count = int(gnss_count[0])\n",
    "    \n",
    "    # for this quick screening check of data quality, we use the constant incidence and azimuth angles \n",
    "    # get standard deviation of residuals to linear fit\n",
    "    disp_los = ut.enu2los(gnss_obj.dis_e, gnss_obj.dis_n, gnss_obj.dis_u, inc_angle, az_angle)\n",
    "    #disp_los = ut.enu2los(gps_obj.dis_e, gps_obj.dis_n, gps_obj.dis_u, inc_angle, head_angle, az_angle)\n",
    "    disp_detrended = signal.detrend(disp_los)\n",
    "    stn_stdv = np.std(disp_detrended)\n",
    "   \n",
    "    # select GNSS stations based on data completeness and scatter of residuals\n",
    "    disp_detrended = signal.detrend(disp_los)\n",
    "    if range_days*gps_completeness_threshold <= gnss_count:\n",
    "        \n",
    "        if stn_stdv > gps_residual_stdev_threshold:\n",
    "            bad_stn.append(stn)\n",
    "        else:\n",
    "            use_stn.append(stn)\n",
    "            use_lats.append(site_lats[counter])\n",
    "            use_lons.append(site_lons[counter])\n",
    "    else:\n",
    "        bad_stn.append(stn)\n",
    "    counter+=1\n",
    "\n",
    "site_names = use_stn\n",
    "site_lats = use_lats\n",
    "site_lons = use_lons\n",
    "\n",
    "# [optional] manually remove additional stations\n",
    "gnns_to_remove=['CAL8', 'P094', 'COSO', 'CAC2']\n",
    "\n",
    "for i, gnss_site in enumerate(gnns_to_remove):\n",
    "    if gnss_site in site_names:\n",
    "        site_names.remove(gnss_site)\n",
    "    if gnss_site not in bad_stn:\n",
    "        bad_stn.append(gnss_site)\n",
    "    \n",
    "print(\"Final list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)\n",
    "print(\"List of {} stations removed from analysis\".format(len(bad_stn)))\n",
    "print(bad_stn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='gps_los'></a>\n",
    "## 3.5. Make GNSS LOS Velocities\n",
    "\n",
    "As a first approximation to estimate coseismic displacements, we take one day before and one (or three) day after earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EQdate_int = int(sitedata['sites'][site]['earthquakeDate'])\n",
    "EQpre_date = str(EQdate_int - 1)  # should convert to real date object first\n",
    "EQpost_date = str(EQdate_int + 3)\n",
    "\n",
    "gnss_comp = 'enu2los'\n",
    "meta = readfile.read_attribute('velocity.h5')\n",
    "SNWE = ut.four_corners(meta)\n",
    "vel = gnss.get_los_obs(meta, 'velocity',     site_names, start_date='20150101', end_date='20190619')\n",
    "displ = gnss.get_los_obs(meta, \n",
    "                            'displacement', \n",
    "                            site_names, \n",
    "                            start_date=EQpre_date, \n",
    "                            end_date=EQpost_date,\n",
    "                            gnss_comp=gnss_comp, \n",
    "                            redo=True) * 100.\n",
    "print('\\n site   disp-los [cm/yr]')\n",
    "print(np.array([site_names,displ]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. download & read GNSS displacement time series\n",
    "site_id = 'RAMT'\n",
    "gnss_obj = gnss.get_gnss_class('UNR')(site = stn, data_dir = os.path.join(mintpy_dir,'GNSS'))\n",
    "gnss_obj.open()\n",
    "\n",
    "# 2. fit time function\n",
    "model = {\n",
    "    'polynomial' : 1,\n",
    "    'stepDate'       : [sitedata['sites'][site]['earthquakeDate']]\n",
    "}\n",
    "G, m, e2 = time_func.estimate_time_func(model, gnss_obj.date_list, gnss_obj.dis_n)\n",
    "\n",
    "print('fit parameters (constant, linear velocity, step)',m)\n",
    "\n",
    "# 3. reconstruct time series from estimated time function parameters\n",
    "date_list_fit = ptime.get_date_range(gnss_obj.date_list[0], gnss_obj.date_list[-1])\n",
    "dates_fit = ptime.date_list2vector(date_list_fit)[0]\n",
    "G_fit = time_func.get_design_matrix4time_func(date_list_fit, model)\n",
    "dis_ts_fit = np.matmul(G_fit, m)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=[12, 4])\n",
    "ax.plot(gnss_obj.dates, gnss_obj.dis_n, '.', ms=4)\n",
    "ax.plot(dates_fit, dis_ts_fit, lw=4, alpha=0.8)\n",
    "ax.set_ylabel('North displacement [m]')\n",
    "ax.set_title(f'Site {site_id} at [{gnss_obj.site_lon:.6f}, {gnss_obj.site_lat:.6f}]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get InSAR geometry file\n",
    "geom_file = ut.get_geometry_file(['incidenceAngle','azimuthAngle'], work_dir=mintpy_dir, coord='geo')\n",
    "\n",
    "# 1. download & read GNSS displacement time series\n",
    "site_id = 'RAMT'\n",
    "gnss_obj = gnss.get_gnss_class('UNR')(site = stn, data_dir = os.path.join(mintpy_dir,'GNSS'))\n",
    "gnss_obj.open()\n",
    "\n",
    "# 2. fit time function\n",
    "model = {\n",
    "    'polynomial' : 1,\n",
    "    'stepDate'       : [sitedata['sites'][site]['earthquakeDate']]\n",
    "}\n",
    "\n",
    "# project the GNSS three components to InSAR LOS\n",
    "dis_los = ut.enu2los(gnss_obj.dis_e,gnss_obj.dis_n,gnss_obj.dis_u,inc_angle,az_angle)\n",
    "\n",
    "# fit whole time series for this station\n",
    "# date range for this station\n",
    "statStart = gnss_obj.date_list[0]\n",
    "statEnd = gnss_obj.date_list[-1]\n",
    "\n",
    "\n",
    "#dates, dis_los, std, site_lalo, ref_site_lalo = gps_obj.read_gps_los_displacement(geom_file, \n",
    "#                                                                                  start_date=statStart, \n",
    "#                                                                                  end_date=statEnd, \n",
    "#                                                                                  ref_site=None,\n",
    "#                                                                                  gps_comp=gps_comp, \n",
    "#                                                                                  print_msg=False)\n",
    "\n",
    "G, m, e2 = time_func.estimate_time_func(model, gnss_obj.date_list, dis_los)\n",
    "\n",
    "print('fit parameters (constant, linear velocity, step)',m)\n",
    "\n",
    "# 3. reconstruct time series from estimated time function parameters\n",
    "date_list_fit = ptime.get_date_range(statStart, statEnd)\n",
    "dates_fit = ptime.date_list2vector(date_list_fit)[0]\n",
    "G_fit = time_func.get_design_matrix4time_func(date_list_fit, model)\n",
    "dis_ts_fit = np.matmul(G_fit, m)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=[12, 4])\n",
    "ax.plot(gnss_obj.dates, dis_los, '.', ms=4)\n",
    "ax.plot(dates_fit, dis_ts_fit, lw=4, alpha=0.8)\n",
    "ax.set_ylabel('LOS displacement [m]')\n",
    "ax.set_title(f'Site {site_id} at [{gnss_obj.site_lon:.6f}, {gnss_obj.site_lat:.6f}]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the LOS fits for all good GPS stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_los_coseis_cm = []\n",
    "plotGNSSfit = False\n",
    "\n",
    "# need to update these to keep consistent\n",
    "site_lats = []\n",
    "site_lons = []\n",
    "\n",
    "twoQuake = False\n",
    "# fit time function\n",
    "if twoQuake:\n",
    "    modelParam = {\n",
    "        'polynomial' : 1,\n",
    "        'stepDate'       : [sitedata['sites'][site]['earthquakeDate'], sitedata['sites'][site]['earthquakeDate2']],\n",
    "    }\n",
    "else:\n",
    "    modelParam = {\n",
    "        'polynomial' : 1,\n",
    "        'stepDate'       : [sitedata['sites'][site]['earthquakeDate']],\n",
    "    }\n",
    "\n",
    "for stn in site_names:\n",
    "    gnss_obj = gnss.get_gnss_class('UNR')(site = stn, data_dir = os.path.join(mintpy_dir,'GNSS'))\n",
    "    gnss_obj.open()\n",
    "    \n",
    "    site_lats.append(gnss_obj.site_lat) # save site location in list\n",
    "    site_lons.append(gnss_obj.site_lon)\n",
    "    \n",
    "    # fit whole time series for this station\n",
    "    # date range for this station\n",
    "    statStart = gnss_obj.date_list[0]\n",
    "    statEnd = gnss_obj.date_list[-1]\n",
    "\n",
    "    dates, dis_los, std, site_lalo, ref_site_lalo = gnss_obj.get_los_displacement(geom_file, \n",
    "                                                                                      start_date=statStart, \n",
    "                                                                                      end_date=statEnd, \n",
    "                                                                                      ref_site=None,\n",
    "                                                                                      gnss_comp=gnss_comp, \n",
    "                                                                                      print_msg=False)\n",
    "\n",
    "    \n",
    "    G, m, e2 = time_func.estimate_time_func(model, gnss_obj.date_list, dis_los)\n",
    "\n",
    "    print('station',stn,'fit parameters (constant, linear velocity, step)',m)\n",
    "    gnss_los_coseis_cm.append(m[2]*100.)  # save step function and convert to cm for convenience\n",
    "    \n",
    "    if (plotGNSSfit):\n",
    "        # 3. reconstruct time series from estimated time function parameters\n",
    "        date_list_fit = ptime.get_date_range(gnss_obj.date_list[0], gnss_obj.date_list[-1])\n",
    "        dates_fit = ptime.date_list2vector(date_list_fit)[0]\n",
    "        G_fit = time_func.get_design_matrix4time_func(date_list_fit, model)\n",
    "        dis_ts_fit = np.matmul(G_fit, m)\n",
    "        \n",
    "        dates = gnss_obj.date_list\n",
    "        index_start = dates.index(gnss_obj.date_list[0])\n",
    "        index_end = dates.index(gnss_obj.date_list[-1])\n",
    "        print ('index_start',index_start,'end',index_end)\n",
    "        print('len dates',len(dates),'dis_los',len(dis_los),'dates_fit',len(dates_fit))\n",
    "        cut_dates = dates[index_start:index_end]\n",
    "        cut_dates_vec = ptime.date_list2vector(cut_dates)[0]\n",
    "        cut_dis_los = dis_los[index_start:index_end]\n",
    "\n",
    "        # plot\n",
    "        fig, ax = plt.subplots(figsize=[12, 4])\n",
    "        ax.plot(cut_dates_vec, cut_dis_los, '.', ms=4)\n",
    "        ax.plot(dates_fit, dis_ts_fit, lw=4, alpha=0.8)\n",
    "        ax.set_ylabel('LOS displacement [m]')\n",
    "        ax.set_title(f'Site {stn} at [{gnss_obj.site_lon:.6f}, {gnss_obj.site_lat:.6f}]')\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "print(gnss_los_coseis_cm)\n",
    "\n",
    "# compare the simple difference displacements to the time-series step fits\n",
    "dispDiff = displ - gnss_los_coseis_cm\n",
    "print('difference in displacement estimate (cm)\\n', dispDiff)\n",
    "print('difference in coseismic displacement estimate (%)\\n',(displ + gnss_los_coseis_cm) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='gps_insar'></a>\n",
    "## 3.6. Re-reference GNSS and InSAR LOS Coseismic Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reference GNSS stations to GNSS reference site\n",
    "ref_site_ind = site_names.index(sitedata['sites'][site]['gps_ref_site_name'])\n",
    "displRelRef = displ - displ[ref_site_ind]\n",
    "\n",
    "# reference InSAR to GNSS reference site\n",
    "ref_site_lat = float(site_lats[ref_site_ind])\n",
    "ref_site_lon = float(site_lons[ref_site_ind])\n",
    "ref_y, ref_x = ut.coordinate(atr).geo2radar(ref_site_lat, ref_site_lon)[:2]\n",
    "print ('new reference pixel for InSAR y,x:',ref_y, ref_x,'at lat, lon',ref_site_lat,ref_site_lon)\n",
    "EQstep = EQstep - EQstep[ref_y, ref_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot GNSS stations on InSAR coseismic field\n",
    "vmin, vmax = -40, 40\n",
    "cmap = plt.get_cmap('RdBu')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.imshow(EQstep, cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest', extent=(W, E, S, N))\n",
    "cbar1 = fig.colorbar(img1, ax=ax)\n",
    "cbar1.set_label('LOS EQ displacement')\n",
    "\n",
    "for lat, lon, obs in zip(site_lats, site_lons, displRelRef):\n",
    "    color = cmap((obs - vmin)/(vmax - vmin))\n",
    "    ax.scatter(lon, lat, color=color, s=8**2, edgecolors='k')\n",
    "for i, label in enumerate(site_names):\n",
    "     plt.annotate(label, (site_lons[i], site_lats[i]), color='black')\n",
    "\n",
    "out_fig = os.path.abspath('coseismic_insar_vs_gnss.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# same re-reference for the time-series GNSS fit\n",
    "ref_site_ind = site_names.index(sitedata['sites'][site]['gps_ref_site_name'])\n",
    "displFitRelRef = gnss_los_coseis_cm - gnss_los_coseis_cm[ref_site_ind]\n",
    "\n",
    "# plot GNSS stations on InSAR coseismic field\n",
    "vmin, vmax = -40, 40\n",
    "cmap = plt.get_cmap('RdBu')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.imshow(EQstep, cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest', extent=(W, E, S, N))\n",
    "cbar1 = fig.colorbar(img1, ax=ax)\n",
    "cbar1.set_label('LOS EQ displacement')\n",
    "\n",
    "for lat, lon, obs in zip(site_lats, site_lons, displFitRelRef):\n",
    "    color = cmap((obs - vmin)/(vmax - vmin))\n",
    "    ax.scatter(lon, lat, color=color, s=8**2, edgecolors='k')\n",
    "for i, label in enumerate(site_names):\n",
    "     plt.annotate(label, (site_lons[i], site_lats[i]), color='black')\n",
    "\n",
    "out_fig = os.path.abspath('coseismic_insar_vs_gnss_fit.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='nisar_validation'></a>\n",
    "# 4. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='make_vel'></a>\n",
    "## 4.1. Make Displacement Residuals at GNSS Locations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First compare the coseismic displacements estimated with the simple subtraction of pre-quake and post-quake dates with the GNSS or GPS displacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set Parameters\n",
    "pixel_radius = 5   #number of InSAR pixels to average for comparison with GNSS\n",
    "\n",
    "#Create dictionary with the stations as the key and all their info as an array \n",
    "stn_dict = {}  \n",
    "\n",
    "#Loop over GNSS station locations\n",
    "for i in range(0,len(site_names)): \n",
    "    #print(site_names[i])\n",
    "    \n",
    "    # convert GNSS station lat/lon information to InSAR x/y grid\n",
    "    stn_lat = site_lats[i]\n",
    "    stn_lon = site_lons[i]\n",
    "    x_value = round((stn_lon - W)/lon_step)\n",
    "    y_value = round((stn_lat - N)/lat_step)\n",
    "    \n",
    "    # get velocities and residuals\n",
    "    disp_GNSS = displ[i]\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    disp_px_rad = EQstep[y_value-pixel_radius:y_value+1+pixel_radius, \n",
    "                     x_value-pixel_radius:x_value+1+pixel_radius]\n",
    "    disp_InSAR = np.median(disp_px_rad)\n",
    "    residual = disp_GNSS - disp_InSAR\n",
    "\n",
    "    # populate data structure\n",
    "    values = [x_value, y_value, disp_InSAR, disp_GNSS, residual, stn_lat, stn_lon]\n",
    "    stn = site_names[i]\n",
    "    stn_dict[stn] = values\n",
    "    \n",
    "# extract data from structure\n",
    "res_list1 = []\n",
    "insar_disp1 = []\n",
    "gnss_disp1 = []\n",
    "lat_list1 = []\n",
    "lon_list1 = []\n",
    "for i in range(len(site_names)): \n",
    "    stn = site_names[i]\n",
    "    insar_disp1.append(stn_dict[stn][2])\n",
    "    gnss_disp1.append(stn_dict[stn][3])\n",
    "    res_list1.append(stn_dict[stn][4])\n",
    "    lat_list1.append(stn_dict[stn][5])\n",
    "    lon_list1.append(stn_dict[stn][6])\n",
    "num_stn = len(site_names)\n",
    "print('Finish creating InSAR residuals at GNSS sites')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='make_velres1'></a>\n",
    "## 4.2. Make Double-differenced Displacement Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff_res_list1 = []\n",
    "stn_dist_list1 = []\n",
    "dict_keys = list(stn_dict.keys())\n",
    "\n",
    "# remove reference stn\n",
    "site_names_analysis = list(site_names)\n",
    "#site_names_analysis.remove(gps_ref_site_name)\n",
    "\n",
    "# loop over stations\n",
    "for i in range(len(site_names_analysis)-1):\n",
    "    stn1 = dict_keys[i]\n",
    "    print(site_names_analysis[i])\n",
    "    for l in range(i + 1, len(dict_keys)):\n",
    "        stn2 = dict_keys[l]\n",
    "        # calculate between-station velocity residual\n",
    "        # stn_dict values = [x_value, y_value, vel_InSAR, vel_GPS, residual, stn_lat, stn_lon]\n",
    "        # index 3 = gps vel\n",
    "        gnss_vel_diff = stn_dict[stn1][3] - stn_dict[stn2][3]\n",
    "        # index 2 = insar vel \n",
    "        insar_vel_diff = stn_dict[stn1][2]-stn_dict[stn2][2]\n",
    "        # calculate double-difference\n",
    "        diff_res = gnss_vel_diff - insar_vel_diff\n",
    "        diff_res_list1.append(diff_res)\n",
    "        # get distance between selected station\n",
    "        # index 5 is lat, 6 is lon\n",
    "        dlat = (stn_dict[stn1][5]-stn_dict[stn2][5])\n",
    "        dlon = (stn_dict[stn1][6]-stn_dict[stn2][6])*np.sin(stn_dict[stn1][5])\n",
    "        #convert degrees to km\n",
    "        stn_dist = math.sqrt(dlat**2 + dlon**2)*111\n",
    "        stn_dist_list1.append(stn_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='amp_vs_dist'></a>\n",
    "## 4.3. Amplitude vs. Distance of Double-differences (not quite a structure function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist_th = np.linspace(0.1,50,100)  # distances for evaluation\n",
    "acpt_error = 4*(1+np.sqrt(dist_th))  # coseismic threshold in mm\n",
    "acpt_error_cm = acpt_error/10.\n",
    "abs_ddiff_disp = [abs(i) for i in diff_res_list1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11,7))\n",
    "plt.scatter(stn_dist_list1,abs_ddiff_disp,label='D_gnss - D_InSAR for station pair')\n",
    "plt.plot(dist_th, acpt_error_cm, 'r',label='Coseismic requirement')\n",
    "plt.ylim(0,5)\n",
    "plt.xlim(0,50)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(f\"Double-Difference Residuals \\n Date range {start_date}-{end_date} \\n GNSS simple subtraction displacement\")\n",
    "plt.xlabel(\"Distance (km)\")\n",
    "plt.ylabel(\"Amplitude of Double-Differenced Displacement Residual (cm)\")\n",
    "plt.show()\n",
    "\n",
    "out_fig = os.path.abspath('coseismic_insar-gnss_vs_distance.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='nisar_anal'></a>\n",
    "## 4.4. GNSS-InSAR Residuals Analysis with Step Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# version of GNSS coseismic displacements from full time-series fit\n",
    "#Create dictionary with the stations as the key and all their info as an array \n",
    "stn_fit_dict = {}  \n",
    "\n",
    "#Loop over GNSS station locations\n",
    "for i in range(0,len(site_names)): \n",
    "    \n",
    "    # convert GNSS station lat/lon information to InSAR x/y grid\n",
    "    # convert GNSS station lat/lon information to InSAR x/y grid\n",
    "    stn_lat = site_lats[i]\n",
    "    stn_lon = site_lons[i]\n",
    "    x_value = round((stn_lon - W)/lon_step)\n",
    "    y_value = round((stn_lat - N)/lat_step)\n",
    "    \n",
    "    # get velocities and residuals\n",
    "    disp_GNSS_fit = gnss_los_coseis_cm[i]\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    disp_px_rad = EQstep[y_value-pixel_radius:y_value+1+pixel_radius, \n",
    "                     x_value-pixel_radius:x_value+1+pixel_radius]\n",
    "    disp_InSAR = np.median(disp_px_rad)\n",
    "\n",
    "    residual_fit = disp_GNSS_fit - disp_InSAR\n",
    "\n",
    "    # populate data structure\n",
    "    values = [x_value, y_value, disp_InSAR, disp_GNSS_fit, residual, stn_lat, stn_lon]\n",
    "    stn = site_names[i]\n",
    "    stn_fit_dict[stn] = values\n",
    "    \n",
    "# extract data from structure\n",
    "res_list2 = []\n",
    "insar_disp2 = []\n",
    "gnss_disp2 = []\n",
    "lat_list2 = []\n",
    "lon_list2 = []\n",
    "for stn in site_names: \n",
    "    insar_disp2.append(stn_fit_dict[stn][2])\n",
    "    gnss_disp2.append(stn_fit_dict[stn][3])\n",
    "    res_list2.append(stn_fit_dict[stn][4])\n",
    "    lat_list2.append(stn_fit_dict[stn][5])\n",
    "    lon_list2.append(stn_fit_dict[stn][6])\n",
    "num_stn = len(site_names) \n",
    "print('Finish creating InSAR residuals at GNSS sites')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='make_velres2'></a>\n",
    "## 4.5. Make Double-differenced Displacement Residuals Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stn_dict = stn_fit_dict # use the fit displacements for the rest of the calculations\n",
    "\n",
    "diff_res_fit_list2 = []\n",
    "stn_dist_list2 = []\n",
    "dict_keys = list(stn_dict.keys())\n",
    "\n",
    "# remove reference stn\n",
    "site_names_analysis = list(site_names)\n",
    "#site_names_analysis.remove(gps_ref_site_name)\n",
    "\n",
    "# loop over stations\n",
    "for i in range(len(site_names_analysis)-1):\n",
    "    stn1 = dict_keys[i]\n",
    "    for l in range(i + 1, len(dict_keys)):\n",
    "        stn2 = dict_keys[l]\n",
    "        # calculate between-station velocity residual\n",
    "        # stn_dict values = [x_value, y_value, vel_InSAR, vel_GPS, residual, stn_lat, stn_lon]\n",
    "        # index 3 = gps vel\n",
    "        gps_vel_diff = stn_dict[stn1][3] - stn_dict[stn2][3]\n",
    "        # index 2 = insar vel \n",
    "        insar_vel_diff = stn_dict[stn1][2]-stn_dict[stn2][2]\n",
    "        # calculate double-difference\n",
    "        diff_res = gps_vel_diff - insar_vel_diff\n",
    "        diff_res_fit_list2.append(diff_res)\n",
    "        # get distance between selected station\n",
    "        # index 5 is lat, 6 is lon\n",
    "        dlat = (stn_dict[stn1][5]-stn_dict[stn2][5])\n",
    "        dlon = (stn_dict[stn1][6]-stn_dict[stn2][6])*np.sin(stn_dict[stn1][5])\n",
    "        #convert degrees to km\n",
    "        stn_dist = math.sqrt(dlat**2 + dlon**2)*111\n",
    "        stn_dist_list2.append(stn_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='ampvsdist2'></a>\n",
    "## 4.6. Amplitude vs. Distance of Double-differences Step (not quite a structure function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abs_ddiff_disp_fit = [abs(i) for i in diff_res_fit_list2]\n",
    "\n",
    "# Write data for statistical tests\n",
    "dist = [np.array(stn_dist_list2)]  # in km\n",
    "rel_measure = [np.array(np.abs(diff_res_fit_list2))*10.0]  # in mm\n",
    "ifgs_date = np.array([[dt.strptime(start_date,\"%Y%m%d\"),dt.strptime(end_date,\"%Y%m%d\")]])\n",
    "#n_ifgs = len(dist)\n",
    "n_ifgs = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11,7))\n",
    "plt.scatter(stn_dist_list2,abs_ddiff_disp_fit,label='D_gnss_fit - D_InSAR for station pair')\n",
    "plt.plot(dist_th, acpt_error_cm, 'r',label='Coseismic requirement')\n",
    "plt.ylim(0,5)\n",
    "plt.xlim(0,50)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(f\"Double-Difference Residuals \\n Date range {start_date}-{end_date} \\n GNSS time-series fit displacement\")\n",
    "plt.xlabel(\"Distance (km)\")\n",
    "plt.ylabel(\"Amplitude of Double-Differenced Displacement Residual (cm)\")\n",
    "plt.show()\n",
    "\n",
    "out_fig = os.path.abspath('coseismic_insar-gnss_fit_vs_distance.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1—Succesful when 68% of points below requirements line\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='coseismic_valid_method1'></a>\n",
    "## 4.7. Coseismic Requirement Validation: Method 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of double-difference residuals from the step-function time-series fit above and below the coseismic requirements curve and calculate percentage of total for a series of distance bins. Compare to 68.3% threshold in each bin and for all points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate Statistics\n",
    "n_bins = 10\n",
    "threshold = 0.683  \n",
    "#  we assume that the distribution of residuals is Gaussian and \n",
    "#  that the threshold represents a 1-sigma limit within which \n",
    "#  we expect 68.3% of residuals to lie.\n",
    "\n",
    "bins = np.linspace(0.1, 50.0, num=n_bins+1)\n",
    "n_all = np.empty([n_ifgs, n_bins+1], dtype=int) # number of points for each ifgs and bins\n",
    "n_pass = np.empty([n_ifgs, n_bins+1], dtype=int) # number of points pass\n",
    "\n",
    "# the final column is the ratio as a whole\n",
    "for i in range(n_ifgs):\n",
    "    inds = np.digitize(dist[i], bins)\n",
    "    for j in range(1, n_bins + 1):\n",
    "        rqmt = 4*(1+np.sqrt(dist[i][inds==j]))# mission requirement for i-th ifgs and j-th bins in mm\n",
    "        rem = rel_measure[i][inds == j] # relative measurement\n",
    "        #assert len(rqmt) == len(rem)\n",
    "        n_all[i,j-1] = len(rem)\n",
    "        n_pass[i,j-1] = np.count_nonzero(rem < rqmt)\n",
    "    n_all[i,-1] = np.sum(n_all[i, 0:-2])\n",
    "    n_pass[i,-1] = np.sum(n_pass[i, 0:-2])\n",
    "ratio = n_pass / n_all\n",
    "success_or_fail = ratio > threshold\n",
    "\n",
    "def to_str(x:bool):\n",
    "    if x==True:\n",
    "        return 'true '\n",
    "    elif x==False:\n",
    "        return 'false '\n",
    "success_or_fail_str = [list(map(to_str, x)) for x in success_or_fail]\n",
    "\n",
    "columns = []\n",
    "for i in range(n_bins):\n",
    "    columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "columns.append('total')\n",
    "\n",
    "index = []\n",
    "for i in range(len(ifgs_date)):\n",
    "    index.append(ifgs_date[i,0].strftime('%Y%m%d')+'-'+ifgs_date[i,1].strftime('%Y%m%d'))\n",
    "    \n",
    "# Display Results\n",
    "\n",
    "n_all_pd = pd.DataFrame(n_all,columns=columns,index=index)\n",
    "n_pass_pd = pd.DataFrame(n_pass,columns=columns,index=index)\n",
    "ratio_pd = pd.DataFrame(ratio,columns=columns,index=index)\n",
    "success_or_fail_pd = pd.DataFrame(success_or_fail_str,columns=columns,index=index)\n",
    "\n",
    "display(n_all_pd)  # Number of data points in each bin\n",
    "display(n_pass_pd) # Number of data points that lie below the curve\n",
    "\n",
    "#Set new style for table\n",
    "s = ratio_pd.style\n",
    "s.set_table_styles([  # create internal CSS classes\n",
    "    {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "    {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "], overwrite=False)\n",
    "display(s.set_td_classes(success_or_fail_pd))  # Percentage of passing points:\n",
    "display(success_or_fail_pd)  # Explicit pass/fail table\n",
    "\n",
    "# Overall pass/fail criterion\n",
    "if n_ifgs == 1:\n",
    "    if success_or_fail_pd.iloc[0]['total']:\n",
    "        print(\"This coseismic displacement dataset passes the requirement.\")\n",
    "    else:\n",
    "        print(\"This coseismic displacement dataset does not pass the requirement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1 table by distance bin—successful when greater than 0.683\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='nisar_validation2'></a>\n",
    "# 5. NISAR Validation Approach 2: InSAR-only Structure Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Validation approach 2, we use a date when there was no earthquake and do the same step function fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(mintpy_dir)  # reset directory in case running out of sequence\n",
    "print(mintpy_dir)\n",
    "# Step function fit when there was no earthquake\n",
    "command = 'timeseries2velocity.py ' + timeseries_filename + ' --step ' + sitedata['sites'][site]['noEarthquakeDate']\n",
    "process = subprocess.run(command, shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can view the step function amplitude for the non-earthquake date and the new linear velocity estimate. Because we have the step function at a time far away from the Ridgecrest earthquake, the step function has a small amplitude and the linear velocity has absorbed the earthquake displacements. The non-earthquake step plot has same color scale as earthquake coseismic plot above to show the small atmospheric noise of a date without an earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scp_args = 'velocity.h5 step' + sitedata['sites'][site]['noEarthquakeDate'] + ' -v -40 40 --ref-yx 200 950 --colormap RdBu --figtitle LOS_Coseismic' # --plot-setting ' + plot_config_file\n",
    "view.main(scp_args.split())\n",
    "scp_args = 'velocity.h5 velocity -v -25 25 --ref-yx 200 950 --colormap RdBu --figtitle LOS_Velocity' # --plot-setting ' + plot_config_file\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='array_mask'></a>\n",
    "## 5.1. Read Array and Mask Pixels with no Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the non-earthquake displacement as the insar_displacment for statistics and convert to mm\n",
    "noEQdataset = 'step' + sitedata['sites'][site]['noEarthquakeDate']\n",
    "noEQstep,atrib = readfile.read(vel_file, datasetName = noEQdataset)  #read and coseismic step \n",
    "noEQdate = sitedata['sites'][site]['noEarthquakeDate']\n",
    "\n",
    "insar_displacement = noEQstep[np.newaxis, :] * 1000. # convert from m to mm and add dimension to array to allow multiple datasets\n",
    "\n",
    "ifgs_date = np.array([noEQdate])  # only one non-earthquake date for now\n",
    "#print(insar_displacement.shape)\n",
    "n_ifgs = insar_displacement.shape[0]\n",
    "\n",
    "# mask out no-data areas\n",
    "insar_displacement[insar_displacement == 0] = np.nan\n",
    "\n",
    "# display map of data after masking\n",
    "cmap = plt.get_cmap('RdBu')\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.imshow(insar_displacement[i], cmap=cmap, interpolation='nearest', extent=(W, E, S, N))\n",
    "    ax.set_title(\"Coseismic \\n Date \"+noEQdate)\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('LOS displacement [mm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='remove_trend'></a>\n",
    "## 5.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate the coordinate for every pixel.\n",
    "\n",
    "Then for each non-earthquake step fit, randomly selected pixels need to be paired up. In order to keep measurements independent, different pixel pairs can not share same pixel. This is achieved by pairing up in sequence, i.e., pairing up pixel number 1 and number 2, 3 and 4..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X0,Y0 = load_geo(atrib)\n",
    "X0_2d,Y0_2d = np.meshgrid(X0,Y0)\n",
    "\n",
    "M2dist = []; rel_measure = []\n",
    "for i in range(n_ifgs):\n",
    "    dist_i, rel_measure_i = samp_pair(X0_2d,Y0_2d,insar_displacement[i],num_samples=1000000)\n",
    "    M2dist.append(dist_i)\n",
    "    rel_measure.append(rel_measure_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the statistical property of selected pixel pairs and overall histogram of relative measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(M2dist[i], bins=100)\n",
    "    ax.set_title(f\"Histogram of distance \\n Coseismic Date \"+noEQdate)\n",
    "    ax.set_xlabel(r'Distance ($km$)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlim(0,50)\n",
    "    \n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(rel_measure[i], bins=100)\n",
    "    ax.set_title(f\"Histogram of Relative Measurement \\n Coseismic Date \"+noEQdate)\n",
    "    ax.set_xlabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='M2ampvsdist2'></a>\n",
    "## 5.3. Amplitude vs. Distance of Relative Measurements (pair differences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist_th = np.linspace(0,50,100)  # distance in km\n",
    "rqmt = 4*(1+np.sqrt(dist_th))   # coseismic requirement in mm\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    ax.plot(dist_th, rqmt, 'r',label='Coseismic requirement')\n",
    "    ax.scatter(M2dist[i], rel_measure[i], s=1, alpha=0.25, label='Relative displacement for pixel pair')\n",
    "    ax.set_title(f\"Method 2: Relative Measurements between Pixel Pairs and Requirement Curve vs. Distance \\n\"+site+\" Non-earthquake Date \"+noEQdate)\n",
    "    ax.set_ylabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_xlabel('Distance (km)')\n",
    "    ax.set_xlim(0,50)\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    out_fig = os.path.abspath('coseismic_insar-only_vs_distance_'+site+'_date'+noEQdate+'.png')\n",
    "    fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2—\n",
    "    68% of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data used of approach 2:\n",
    "- `dist`: distance of pixel pairs,\n",
    "- `rel_measure`: relative measurement of pixel pairs,\n",
    "- `ifgs_date`: list of dates for coseismic step fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(work_dir,'approach2.pkl'),'wb') as f:\n",
    "    pickle.dump((M2dist,rel_measure,ifgs_date),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='M2RelMeasTable'></a>\n",
    "## 5.4. Bin Sample Pairs by Distance Bin and Calculate Statistics\n",
    "\n",
    "In approach 2, the number of pixel pairs which meet the mission requirement as a percentage of the total number of pixel pairs selected are counted.\n",
    "\n",
    "The method we apply to evaluate the noise structure is similar to that in the InSAR-GNSS comparison. We count the percentage of measurements that fall below the threshold curve  for each of the 5-km-wide bins. If the average of the percentages from all bins is larger than 0.683, we judge that the noise level falls below the requirement.\n",
    "\n",
    "Then we prepare table of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "bins = np.linspace(0.1,50.0,num=n_bins+1)\n",
    "\n",
    "n_all = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points for each ifgs and bins\n",
    "n_pass = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points pass\n",
    "#ratio = np.empty([n_ifgs,n_bins+1]) # ratio\n",
    "# the final column is the ratio as a whole\n",
    "for i in range(n_ifgs):\n",
    "    inds = np.digitize(M2dist[i],bins)\n",
    "    for j in range(1,n_bins+1):\n",
    "        rqmt = 4*(1+np.sqrt(M2dist[i][inds==j]))# coseismic mission requirement for i-th ifgs and j-th bins\n",
    "        rem = rel_measure[i][inds==j] # relative measurement\n",
    "        assert len(rqmt) == len(rem)\n",
    "        n_all[i,j-1] = len(rem)\n",
    "        n_pass[i,j-1] = np.count_nonzero(rem<rqmt)\n",
    "    n_all[i,-1] = np.sum(n_all[i,0:-2])\n",
    "    n_pass[i,-1] = np.sum(n_pass[i,0:-2])\n",
    "    \n",
    "ratio = n_pass/n_all\n",
    "mean_ratio = np.array([np.mean(ratio[:,:-1],axis=1)])\n",
    "ratio = np.hstack((ratio,mean_ratio.T))\n",
    "thresthod = 0.683\n",
    "#The assumed nature of Gaussian distribution gives a probability of 0.683 of being within one standard deviation.\n",
    "success_or_fail = ratio>thresthod\n",
    "success_or_fail_str = [list(map(to_str, x)) for x in success_or_fail]\n",
    "\n",
    "# make table\n",
    "\n",
    "columns = []\n",
    "for i in range(n_bins):\n",
    "    columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "columns.append('total')\n",
    "\n",
    "index = []\n",
    "for i in range(len(ifgs_date)):\n",
    "    index.append(ifgs_date[i])\n",
    "    \n",
    "n_all_pd = pd.DataFrame(n_all,columns=columns,index=index)\n",
    "n_pass_pd = pd.DataFrame(n_pass,columns=columns,index=index)\n",
    "ratio_pd = pd.DataFrame(ratio,columns=columns+['mean'],index=index)\n",
    "success_or_fail_pd = pd.DataFrame(success_or_fail_str,columns=columns+['mean'],index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points (pixel pairs) in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_all_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points that below the requirements curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pass_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of pass to total samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ratio_pd.style\n",
    "s.set_table_styles([  # create internal CSS classes\n",
    "    {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "    {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "], overwrite=False)\n",
    "s.set_td_classes(success_or_fail_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2 table of distance bins—\n",
    "    68% of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of dates and bins that pass the requirement (0.683):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(ratio_pd['total']>thresthod)/n_ifgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='appendix'></a>\n",
    "# Appendix: Supplementary Comparisons and Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='compare_raw'></a>\n",
    "## A.1. Compare Raw Velocities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = -40, 40\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(insar_disp1, range = [vmin, vmax],bins = 40, color = \"green\",edgecolor='grey',label='D_InSAR')\n",
    "plt.hist(gnss_disp1, range = [vmin, vmax],bins = 40, color = \"orange\",edgecolor='grey',label='D_gnss', alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Displacements \\n Date range {EQpre_date}-{EQpost_date} \\n Reference stn: {sitedata['sites'][site]['gps_ref_site_name']} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('LOS displacement (cm)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.ylim(0,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='plot_vel'></a>\n",
    "## A.2. Plot Velocity Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(res_list1, bins = 40, range = [vmin, vmax],edgecolor='grey',color=\"darkblue\",linewidth=1,label='D_gnss - D_InSAR (area average)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Residuals \\n Date range {start_date}-{end_date} \\n Reference stn: {sitedata['sites'][site]['gps_ref_site_name']} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('Displacement Residual (cm)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.ylim(0,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='plot_velres'></a>\n",
    "## A.3. Plot Double-differenced Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(diff_res_list1, range = [vmin, vmax],bins = 40, color = \"darkblue\",edgecolor='grey',label='D_gnss_(s1-s2) - D_InSAR_(s1-s2)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Difference Residuals \\n Date range {start_date}-{end_date} \\n Reference stn: {sitedata['sites'][site]['gps_ref_site_name']} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('Double Differenced Displacement Residual (cm)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.ylim(0,250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='appendix_gps'></a>\n",
    "## A.4. GPS Position Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Read in timeseries file\n",
    "time_file = os.path.join(work_dir, 'MintPy/timeseries.h5')\n",
    "insar_displacements = readfile.read(time_file, datasetName='timeseries')[0] * 100.\n",
    "\n",
    "#Get aquisition dates, trim the str, and convert to datetime\n",
    "raw_aqu_list = readfile.get_slice_list(time_file)\n",
    "acquisitions_dates = []\n",
    "for i in range(len(raw_aqu_list)):\n",
    "    date = raw_aqu_list[i].split(\"-\")\n",
    "    #aqu_dates.append = date[1]\n",
    "    acquisitions_dates.append(dt.strptime(date[1], \"%Y%m%d\"))\n",
    "ndates = len(acquisitions_dates)\n",
    "\n",
    "#Plot displacements and velocity timeseries at GNSS station locations\n",
    "test_list = site_names\n",
    "#test_list = ['P595'] #', 'CAHA', 'CAKC', 'CAND', 'CARH'] #for testing, remove after\n",
    "for stn in test_list:\n",
    "    \n",
    "    #InSAR Info\n",
    "    insar_timeseries = []\n",
    "    stn_x = (stn_dict[stn][0])\n",
    "    stn_y = (stn_dict[stn][1])\n",
    "    for i in range(ndates):\n",
    "        insar_displacement = insar_displacements[i,stn_y,stn_x]\n",
    "        insar_timeseries.append(insar_displacement)     \n",
    "    InSAR_stn_disp = stn_dict[stn][2]\n",
    "    GNSS_stn_disp = stn_dict[stn][3]\n",
    "    #print(InSAR_stn_vel)\n",
    "    #print(GNSS_stn_vel)\n",
    "    \n",
    "    #get the length of time between first and last interferogram for velocity plotting\n",
    "    insar_timespan = acquisitions_dates[ndates - 1] - acquisitions_dates[0]\n",
    "    conversion = insar_timespan.days/365.25\n",
    "    aqu_dates_convert=acquisitions_dates #/conversion\n",
    "    \n",
    "    #Plot InSAR\n",
    "    plt.figure(figsize=(15,5))  \n",
    "    plt.plot([0,100000],[0,0], color='grey',linestyle='dashed',linewidth=1)\n",
    "    plt.scatter(acquisitions_dates, insar_timeseries, label=\"InSAR Position Time Series\")\n",
    "    #velocity trendline\n",
    "    plt.plot([acquisitions_dates[0], acquisitions_dates[ndates - 1]],[0,InSAR_stn_disp],label=f\"InSAR Displacement {InSAR_stn_disp:.3f} (Ref: {sitedata['sites'][site]['gps_ref_site_name']})\")\n",
    "\n",
    "    #GNSS Info\n",
    "    gnss_obj = gnss.get_gnss_class('UNR')(site = stn, data_dir = os.path.join(mintpy_dir,'GNSS'))\n",
    "    gnss_obj.open()\n",
    "    dates = gnss_obj.dates\n",
    "    # date range for this station\n",
    "    statStart = gnss_obj.date_list[0]\n",
    "    statEnd = gnss_obj.date_list[-1]\n",
    "\n",
    "    dates, disp_los, std, site_lalo, ref_site_lalo = gnss_obj.get_los_displacement(geom_file, start_date=statStart, end_date=statEnd, ref_site=None,\n",
    "                                  gnss_comp=gnss_comp, print_msg=False)\n",
    "    \n",
    "    #Plot GNSS\n",
    "    index_begin = np.min(np.where(dates >= start_date_gnss))\n",
    "    index_end = np.max(np.where(dates <= end_date_gnss))\n",
    "    dates_cut = dates[index_begin:index_end]\n",
    "    disp_los_cut = disp_los[index_begin:index_end]\n",
    "    disp_los_cut = (disp_los_cut - np.median(disp_los_cut))*100\n",
    "    plt.scatter(dates_cut, disp_los_cut - disp_los_cut[0], label=\"GNSS Daily Positions\")   \n",
    "    #velocity trendline\n",
    "    plt.plot([acquisitions_dates[0], acquisitions_dates[ndates - 1]],[0,GNSS_stn_disp],color='orange',label=f\"GNSS Displacement {GNSS_stn_disp:.3f} (Ref: {sitedata['sites'][site]['gps_ref_site_name']})\")\n",
    "\n",
    "    plt.title(f\"Station Name: {stn}\") \n",
    "    plt.ylabel('Displacement (cm)')\n",
    "    plt.ylim(-37,37) # plot range for Ridgecrest\n",
    "    plt.xlim(aqu_dates_convert[0],aqu_dates_convert[ndates - 1])\n",
    "    plt.legend(loc=\"best\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solid_earth_atbd",
   "language": "python",
   "name": "solid_earth_atbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
