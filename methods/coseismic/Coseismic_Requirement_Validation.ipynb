{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Workflow to Validate NISAR L2 Coseismic Displacement Requirement\n",
    "\n",
    "**Original code authored by:** David Bekaert, Heresh Fattahi, Eric Fielding, and Zhang Yunjun \n",
    "\n",
    "Extensive modifications by Adrian Borsa and Amy Whetter 2022\n",
    "\n",
    "Subsequent updates and reformatting by Rob Zinke, Katia Tymofyeyeva and Adrian Borsa 2025\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Step 1 of the validation workflow is executed in the **ARIA_prep** notebook. All processing steps in this notebook MUST be run sequentially, although several are optional and can be skipped.\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr/>\n",
    "\n",
    "## Table of Contents:\n",
    "<a id='example_TOC'></a>\n",
    "\n",
    "[**Environment Setup**](#setup)\n",
    "\n",
    "[**1. Download and Prepare Interferograms**](#prep_ifg)\n",
    "\n",
    "[**2. Generation of Time Series from Interferograms**](#gen_ts)\n",
    "- [2.1. Validate/Modify Interferogram Network](#validate_network)\n",
    "- [2.2. Generate Quality Control Mask](#generate_mask)\n",
    "- [2.3. Reference Interferograms To Common Lat/Lon](#common_latlon)\n",
    "- [2.4. Invert for SBAS Line-of-Sight Timeseries](#invert_SBAS)\n",
    "\n",
    "[**3. Optional Corrections**](#opt_correction)\n",
    "- [3.1. Solid Earth Tides Correction](#solid_earth)\n",
    "- [3.2. Tropospheric Delay Correction](#tropo_corr)\n",
    "- [3.3. Phase Deramping ](#phase_deramp)\n",
    "- [3.4. Topographic Residual Correction ](#topo_corr) \n",
    "\n",
    "[**4. Decomposition of InSAR and GNSS Time Series into Basis Functions**](#decomp_ts)\n",
    "- [4.1. Estimate InSAR LOS Velocities](#insar_vel)\n",
    "- [4.2. Estimate InSAR Coseismic Displacement](#co_step)\n",
    "- [4.3. Find Collocated GNSS Stations](#find_gps)  \n",
    "- [4.4. Get GNSS Position Time Series](#gps_ts) \n",
    "- [4.5. Make GNSS LOS Velocities/Displacements](#gps_los)\n",
    "- [4.6. Re-reference GNSS and InSAR LOS Coseismic Step](#reference)\n",
    "\n",
    "[**5. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison**](#validation1)\n",
    "- [5.1. Make GNSS-InSAR Displacement Residuals at GNSS Station Locations](#make_resids)\n",
    "- [5.2. Make Double-differenced Displacement Residuals](#make_ddiff)\n",
    "- [5.3. Amplitude vs. Distance of Double-differences (not quite a structure function)](#amp_vs_dist)\n",
    "\n",
    "[**6. NISAR Validation Approach 2: InSAR-only Structure Function**](#nisar_validation2)\n",
    "- [6.1. Read Array and Mask Pixels with no Data](#array_mask)\n",
    "- [6.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend](#remove_trend)\n",
    "- [6.3. Coseismic Requirement Validation: Method 2](#validation2)\n",
    "\n",
    "[**Appendix: Supplementary Comparisons and Plots**](#appendix)\n",
    "- [A.1. Compare Raw Velocities](#compare_raw)\n",
    "- [A.2. Plot Velocity Residuals](#plot_residuals)\n",
    "- [A.3. Plot Double-differenced Residuals](#plot_ddiff)\n",
    "- [A.4. GNSS Time-series Plots](#plot_timeseries)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup<a id='#setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Python Packages<a id='#load_packages'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import warnings\n",
    "from datetime import datetime as dt, timedelta\n",
    "\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from mintpy.cli import plot_network, view\n",
    "from mintpy.utils import readfile, ptime, time_func, utils as ut\n",
    "from mintpy.objects import gnss, timeseries\n",
    "from mintpy.smallbaselineApp import TimeSeriesAnalysis\n",
    "\n",
    "from solid_utils.gnss_utils import scale_gnss_m_to_mm\n",
    "from solid_utils.fitting import IterativeOutlierFit\n",
    "from solid_utils.sampling import SiteDisplacement, load_geo, samp_pair, profile_samples, haversine_distance\n",
    "from solid_utils.plotting import display_coseismic_validation, display_validation_table\n",
    "from solid_utils.saving import save_results\n",
    "from solid_utils.configs import update_reference_point\n",
    "from solid_utils.corrections import run_cmd, apply_cor\n",
    "\n",
    "#Set Global Plot Parameters\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Calval Site and Parameters<a id='set_calval_params'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Basic Configuration ===\n",
    "site='CalVal_S1_LongValleyD144' # Cal/Val location ID from my_sites.txt\n",
    "requirement = 'Coseismic' # Options: 'Secular' 'Coseismic' 'Transient'\n",
    "dataset = \"ARIA_S1_new\"  # Dataset type: 'ARIA_S1', 'ARIA_S1_new'\n",
    "aria_gunw_version = \"3_0_1\"\n",
    "\n",
    "rundate = \"20250512\"  # Date of this Cal/Val run\n",
    "version = \"1\"         # Version of this Cal/Val run\n",
    "custom_sites = \"/home/jovyan/my_sites.txt\"  # Path to custom site metadata\n",
    "\n",
    "# === Username Detection / Creation ===\n",
    "user_file = \"/home/jovyan/me.txt\"\n",
    "if os.path.exists(user_file):\n",
    "    with open(user_file, \"r\") as f:\n",
    "        you = f.readline().strip()\n",
    "else:\n",
    "    you = input(\"Please type a username for your Cal/Val outputs: \").strip()\n",
    "    with open(user_file, \"w\") as f:\n",
    "        f.write(you)\n",
    "\n",
    "# === Load Cal/Val Site Metadata ===\n",
    "try:\n",
    "    with open(custom_sites, \"r\") as f:\n",
    "        sitedata = json.load(f)\n",
    "    site_info = sitedata[\"sites\"][site]\n",
    "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "    raise RuntimeError(f\"Failed to load site metadata from {custom_sites}: {e}\")\n",
    "except KeyError:\n",
    "    raise ValueError(f\"Site ID '{site}' not found in {custom_sites}\")\n",
    "\n",
    "print(f\"Loaded site: {site}\")\n",
    "\n",
    "# === Plot Parameters ===\n",
    "vmin, vmax = -100, 100  # mm/yr\n",
    "cmap = plt.get_cmap('RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Directories and Files<a id='set_directories'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Cal/Val Directory Structure ===\n",
    "BASE_DIR = \"/scratch/nisar-st-calval-solidearth\"\n",
    "site_dir = os.path.join(BASE_DIR, dataset, site)\n",
    "work_dir = os.path.join(site_dir, requirement, you, rundate, f\"v{version}\")\n",
    "gunw_dir = os.path.join(site_dir, \"products\")\n",
    "mintpy_dir = os.path.join(work_dir, \"MintPy\")\n",
    "\n",
    "# === Log Directory Paths ===\n",
    "print(f\"  Work directory: {work_dir}\")\n",
    "print(f\"  GUNW directory: {gunw_dir}\")\n",
    "print(f\"MintPy directory: {mintpy_dir}\")\n",
    "\n",
    "# === Check MintPy Directory Existence ===\n",
    "if not os.path.exists(mintpy_dir):\n",
    "    print(\"\\nERROR: Stop! MintPy processing directory is missing.\")\n",
    "    print(\"This may indicate the prep notebook has not been run.\")\n",
    "    print(\"Missing path:\", mintpy_dir, \"\\n\")\n",
    "else:\n",
    "    os.chdir(mintpy_dir)\n",
    "\n",
    "    # === Set Expected MintPy Filenames ===\n",
    "    geom_file = os.path.join(mintpy_dir, \"inputs\", \"geometryGeo.h5\")\n",
    "    msk_file  = os.path.join(mintpy_dir, \"maskTempCoh.h5\")  # alt: maskConnComp.h5\n",
    "    ts_file   = os.path.join(mintpy_dir, \"timeseries.h5\")\n",
    "    vel_file  = os.path.join(mintpy_dir, \"velocity.h5\")\n",
    "    config_file = os.path.join(mintpy_dir, site_info.get('calval_location') + '.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = readfile.read_template(config_file)\n",
    "print('#' * 10, \"MintPy Configs\", '#' * 10)\n",
    "for key, value in configs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='#prep_ifg'></a>\n",
    "## 1. Download and Prepare Interferograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executed in *ARIA_prep* notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='#gen_ts'></a>\n",
    "## 2. Generation of Time Series from Interferograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='validate_network'></a>\n",
    "### 2.1. Validate/Modify Interferogram Network\n",
    "\n",
    "Add additional parameters to config_file in order to remove selected interferograms, change minimum coherence, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep modify_network'\n",
    "process = subprocess.run(command, shell=True)\n",
    "plot_network.main(['inputs/ifgramStack.h5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='generate_mask'></a>\n",
    "### 2.2. Generate Quality Control Mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask files can be can be used to mask pixels in the time-series processing. Below we generate an initial mask file `maskConnComp.h5` based on the connected components for all the interferograms, which is a metric for unwrapping quality. After time-series analysis is complete, we will calculate a mask from the temporal coherence or variation of phase or displacement with time to make `maskTempCoh.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command='generate_mask.py inputs/ifgramStack.h5  --nonzero  -o maskConnComp.h5  --update'\n",
    "process = subprocess.run(command, shell=True)\n",
    "view.main(['maskConnComp.h5', 'mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='common_latlon'></a>\n",
    "### 2.3. Reference Interferograms To Common Lat/Lon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep reference_point'\n",
    "process = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "os.system('info.py inputs/ifgramStack.h5 | egrep \"REF_\"');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Change Reference Point (Optional) <a id='common_latlon_change'></a>\n",
    "\n",
    "Use the cells below to update the reference point. Uncomment and enter the desired *new_lat* and *new_lon*, then verify the update in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sitedata['sites'][site]['reference_lalo'].strip() != 'auto':\n",
    "    new_lat = sitedata['sites'][site]['reference_lalo'].strip().split()[0]\n",
    "    new_lon = sitedata['sites'][site]['reference_lalo'].strip().split()[1]\n",
    "    update_reference_point(config_file, new_lat, new_lon)  # New latitude and longitude\n",
    "\n",
    "    command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep reference_point'\n",
    "    process = subprocess.run(command, shell=True)\n",
    "    os.system('info.py inputs/ifgramStack.h5 | egrep \"REF_\"');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='invert_SBAS'></a>\n",
    "### 2.4. Invert for SBAS Line-of-Sight Timeseries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep invert_network'\n",
    "process = subprocess.run(command, shell=True)\n",
    "timeseries_filename = f\"{mintpy_dir}/timeseries.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "velocity_filename = f\"{mintpy_dir}/velocity.h5\"\n",
    "command = 'timeseries2velocity.py ' + timeseries_filename + ' -o ' + velocity_filename\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp_args = f\"velocity.h5 velocity -v {vmin} {vmax} --colormap RdBu_r --figtitle LOS_Velocity --unit mm/yr -m {msk_file}\"\n",
    "view.main(scp_args.split());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='opt_correction'></a>\n",
    "## 3. Optional Corrections\n",
    "\n",
    "Phase distortions related to solid earth and ocean tidal effects as well as those due to temporal variations in the vertical stratification of the atmosphere can be mitigated using the approaches described below. At this point, it is expected that these corrections will not be needed to validate the mission requirements, but they may be used to produce the highest quality data products. Typically, these are applied to the estimated time series product rather than to the individual interferograms, since they are a function of the time of each radar acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='solid_earth'></a>\n",
    "### 3.1. Solid Earth Tides Correction\n",
    "\n",
    "Optional solid Earth tide correction is included in the ARIA GUNW products and can be extracted using the <i>ARIA_prep</i> notebook.  \n",
    "Once extracted, you can calculate the correction using the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'do_SET' in site_info.keys() and site_info.get('do_SET') != \"False\":\n",
    "    set_cor_file = f\"{mintpy_dir}/inputs/solidEarthTide_ARIA.h5\"     # Input SET correction file\n",
    "\n",
    "    dirpath, filename = os.path.split(timeseries_filename)\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    output_timeseries = os.path.join(dirpath, f\"{name}_SET{ext}\")\n",
    "    output_velocity = os.path.join(dirpath, f\"velocity.h5\")\n",
    "else:\n",
    "    site_info['do_SET'] = \"False\"\n",
    "    print('#'*10, 'Solid Earth Tide Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply correction if flag is set to True\n",
    "if site_info.get('do_SET') != \"False\":\n",
    "    timeseries_filename, velocity_filename = apply_cor('SET', timeseries_filename, set_cor_file, config_file, mintpy_dir, output_timeseries, output_velocity)\n",
    "else:\n",
    "    print('#'*10, 'Solid Earth Tide Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the corrections\n",
    "if 'do_SET' in site_info.keys() and site_info.get('do_SET') != \"False\":\n",
    "    view.main([set_cor_file, '-m', msk_file])\n",
    "else:\n",
    "    site_info['do_SET'] = \"False\"\n",
    "    print('#'*10, 'Solid Earth Tide Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Ionosphere Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'do_iono' in site_info.keys() and site_info.get('do_iono') != \"False\":\n",
    "    iono_stack_file = f\"{mintpy_dir}/inputs/ionStack.h5\"             # Input ionosphere stack file  \n",
    "\n",
    "    dirpath, filename = os.path.split(timeseries_filename)\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    output_timeseries = os.path.join(dirpath, f\"{name}_iono{ext}\")\n",
    "    output_velocity = os.path.join(dirpath, f\"velocity.h5\")\n",
    "else:\n",
    "    site_info['do_iono'] = \"False\"\n",
    "    print('#'*10, 'Ionosphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply correction if flag is set to True\n",
    "if site_info.get('do_iono')!= \"False\":\n",
    "    timeseries_filename, velocity_filename = apply_cor('iono', timeseries_filename, iono_stack_file, config_file, mintpy_dir, output_timeseries, output_velocity)\n",
    "    iono_cor_file = os.path.join(mintpy_dir, \"ion.h5\")\n",
    "else:\n",
    "    print('#'*10, 'Ionosphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the corrections\n",
    "if site_info.get('do_iono') != \"False\":\n",
    "    view.main([iono_cor_file, '-m', msk_file])\n",
    "else: \n",
    "    print('#'*10, 'Ionosphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='tropo_corr'></a>\n",
    "### 3.3. Tropospheric Delay Correction\n",
    "\n",
    "Optional atmospheric correction utilizes the PyAPS (Jolivet et al., 2011, Jolivet and Agram, 2012) module within GIAnT (or eventually a merged replacement for GIAnT and MintPy). PyAPS is well documented, maintained and can be freely downloaded. PyAPS is included in GIAnT distribution). PyAPS currently includes support for ECMWF’s ERA-Interim, NOAA’s NARR and NASA’s MERRA weather models. A final selection of atmospheric models to be used for operational NISAR processing will be done during Phase C.\n",
    "\n",
    "[T]ropospheric delay maps are produced from atmospheric data provided by Global Atmospheric Models. This method aims to correct differential atmospheric delay correlated with the topography in interferometric phase measurements. Global Atmospheric Models (hereafter GAMs)... provide estimates of the air temperature, the atmospheric pressure and the humidity as a function of elevation on a coarse resolution latitude/longitude grid. In PyAPS, we use this 3D distribution of atmospheric variables to determine the atmospheric phase delay on each pixel of each interferogram.\n",
    "\n",
    "The absolute atmospheric delay is computed at each SAR acquisition date. For a pixel a_i at an elevation z at acquisition date i, the four surrounding grid points are selected and the delays for their respective elevations are computed. The resulting delay at the pixel a_i is then the bilinear interpolation between the delays at the four grid points. Finally, we combine the absolute delay maps of the InSAR partner images to produce the differential delay maps used to correct the interferograms.\n",
    "\n",
    "REFERENCE: [https://github.com/insarlab/pyaps#2-account-setup-for-era5]\n",
    "Read Section 2 for ERA5 (link above) to create an account for the CDS website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'do_tropo' in site_info.keys() and site_info.get(\"do_tropo\") != \"False\":\n",
    "    \n",
    "    if site_info.get(\"tropo_model\") != \"HRRR\":\n",
    "        # Run ERA5-based correction\n",
    "        dirpath, filename = os.path.split(timeseries_filename)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        output_timeseries = os.path.join(dirpath, f\"{name}_ERA5{ext}\")\n",
    "\n",
    "        dirpath, filename = os.path.split(velocity_filename)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        output_velocity = os.path.join(dirpath, f\"{name}_ERA5{ext}\")\n",
    "\n",
    "        tropo_cmd = f\"tropo_pyaps3.py -f {timeseries_filename} -g {mintpy_dir}/inputs/geometryGeo.h5 -o {output_timeseries}\"\n",
    "        run_cmd(tropo_cmd, desc=\"Applying ERA5 tropospheric correction\")\n",
    "\n",
    "        tropo_vel_cmd = f\"timeseries2velocity.py {output_timeseries} -o {output_velocity}\"\n",
    "        run_cmd(tropo_vel_cmd, desc=\"Estimating velocity from ERA5-corrected time series\")\n",
    "\n",
    "        tropo_cor_file = os.path.join(mintpy_dir, \"inputs\", \"ERA5.h5\")\n",
    "        timeseries_filename = output_timeseries\n",
    "\n",
    "    else:\n",
    "        # HRRR-based correction\n",
    "        tropo_cor_file = os.path.join(mintpy_dir, \"inputs\", \"HRRR_ARIA.h5\")\n",
    "\n",
    "        dirpath, filename = os.path.split(timeseries_filename)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        output_timeseries = os.path.join(dirpath, f\"{name}_HRRR{ext}\")\n",
    "\n",
    "        timeseries_filename, velocity_filename = apply_cor(\n",
    "            cor_name=\"tropo\",\n",
    "            ts_file=timeseries_filename,\n",
    "            cor_file=tropo_cor_file,\n",
    "            config_file=config_file,\n",
    "            mintpy_dir=mintpy_dir,\n",
    "            output_ts=output_timeseries,\n",
    "            output_vel=output_velocity\n",
    "        )\n",
    "else:\n",
    "    site_info['do_tropo'] = \"False\"\n",
    "    print(\"#\" * 10, \"Troposphere Correction set to False\", \"#\" * 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the corrections\n",
    "if site_info.get('do_tropo') != \"False\":\n",
    "    view.main([tropo_cor_file, '-m', msk_file])\n",
    "else: \n",
    "    print('#'*10, 'Troposhere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='phase_deramp'></a>\n",
    "### 3.3. Phase Deramping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    deramp_method = configs['mintpy.deramp']\n",
    "    ramp_cmd = f\"remove_ramp.py {timeseries_filename} -m {msk_file} -s {deramp_method} -o {timeseries_filename}_ramp.h5\"\n",
    "    subprocess.run(ramp_cmd, shell=True)\n",
    "    timeseries_filename = f\"{timeseries_filename}_ramp.h5\"\n",
    "except KeyError:\n",
    "    print(\"#\" * 10, \"No deramping\", \"#\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='topo_corr'></a>\n",
    "### 3.4. Topographic Residual Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    do_demErr = configs['mintpy.topographicResidual']\n",
    "    if configs['mintpy.topographicResidual.polyOrder'] == 'auto':\n",
    "        demErr_polyorder = 2\n",
    "    else:\n",
    "        demErr_polyorder = configs['mintpy.topographicResidual.polyOrder']\n",
    "    demErr_cmd = f\"dem_error.py {timeseries_filename} -g {mintpy_dir}/inputs/geometryGeo.h5 -p {demErr_polyorder} -o {timeseries_filename}_demErr.h5\"\n",
    "    subprocess.run(demErr_cmd, shell=True)\n",
    "    timeseries_filename = f\"{timeseries_filename}_demErr.h5\"\n",
    "except KeyError:\n",
    "    print(\"#\" * 10, \"No DEM error correction\", \"#\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='decomp_ts'></a>\n",
    "## 4. Decomposition of InSAR and GNSS Time Series into Basis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='insar_vel'></a>\n",
    "### 4.1. Estimate InSAR LOS Velocities\n",
    "\n",
    "Given a time series of InSAR LOS displacements, the observations for a given pixel, $U(t)$, can be parameterized as:\n",
    "\n",
    "$$ U(t) = a \\;+\\; vt \\;+\\; c_1 cos (\\omega_1t - \\phi_{1,}) \\;+\\; c_2 cos (\\omega_2t - \\phi_2) \\;+\\; \\sum_{j=1}^{N_{eq}} \\left( h_j+f_j F_j (t-t_j) \\right)H(t - t_j) \\;+\\; \\frac{B_\\perp (t)}{R sin \\theta}\\delta z \\;+\\; residual$$ \n",
    "\n",
    "which includes a constant offset $(a)$, velocity $(v)$, and amplitudes $(c_j)$ and phases $(\\phi_j)$ of annual $(\\omega_1)$ and semiannual $(\\omega_2)$ sinusoidal terms.  Where needed we can include additional complexity, such as coseismic and postseismic processes parameterized by Heaviside (step) functions $H$ and postseismic functions $F$ (the latter typically exponential and/or logarithmic).   $B_\\perp(t)$, $R$, $\\theta$, and $\\delta z$ are, respectively, the perpendicular component of the interferometric baseline relative to the first date, slant range distance, incidence angle and topography error correction for the given pixel. \n",
    "\n",
    "Thus, given either an ensemble of interferograms or the output of SBAS (displacement vs. time), we can write the LSQ problem as \n",
    "\n",
    "$$ \\textbf{G}\\textbf{m} = \\textbf{d}$$\n",
    "\n",
    "where $\\textbf{G}$ is the design matrix (constructed out of the different functional terms in Equation 2 evaluated either at the SAR image dates for SBAS output, or between the dates spanned by each pair for interferograms), $\\textbf{m}$ is the vector of model parameters (the coefficients in Equation 2) and $\\textbf{d}$ is the vector of observations.  For GPS time series, $\\textbf{G}, \\textbf{d}, \\textbf{m}$ are constructed using values evaluated at single epochs corresponding to the GPS solution times, as for SBAS InSAR input. \n",
    "\n",
    "With this formulation, we can obtain InSAR velocity estimates and their formal uncertainties (including in areas where the expected answer is zero). \n",
    "\n",
    "The default InSAR velocity fit in MintPy is to estimate a mean linear velocity $(v)$ in in the equation, which we do below. This is the same linear velocity used in the Secular notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "If the time-series span less than one year, then annual and semi-annual fuctions might not be reliably fit for noisy data. In that case, the periodic functions will not be included in these analyses.\n",
    "\n",
    "Note that this workflow **requires** an earthquake date to be specified.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check that an earthquake date is specified and retrieve that date\n",
    "if 'earthquakeDate' in sitedata['sites'][site].keys():\n",
    "    EQdate = [sitedata['sites'][site]['earthquakeDate']]\n",
    "    print('Earthquake date: {:s}'.format(sitedata['sites'][site]['earthquakeDate']))\n",
    "else:\n",
    "    EQdate = []\n",
    "    warnings.warn('No earthquake date specified. This data set might not be suitable for the coseismic workflow.')\n",
    "\n",
    "# Specify basis functions\n",
    "# Changing these might change the meaning of this exercise! Use caution if altering.\n",
    "ts_functions = {\n",
    "                'polynomial': 1,\n",
    "                'periodic': [],  # Periodic terms are in units of years\n",
    "                'stepDate': EQdate,\n",
    "                'polyline': [],\n",
    "                'exp': {},\n",
    "                'log': {}\n",
    "}\n",
    "\n",
    "# Determine time span to check whether fitting of periodic functions is appropriate\n",
    "ts_metadata = readfile.read_attribute(timeseries_filename)\n",
    "start_date = dt.strptime(sitedata['sites'][site]['download_start_date'], '%Y%m%d')\n",
    "end_date = dt.strptime(sitedata['sites'][site]['download_end_date'], '%Y%m%d')\n",
    "ts_timespan = (end_date - start_date).days\n",
    "\n",
    "if (ts_timespan < 365*2) and len(ts_functions['periodic']) > 0:\n",
    "    print('Time span is {:d} days'.format(ts_timespan))\n",
    "    if ts_timespan < 365:\n",
    "        ts_functions['periodic'] = []\n",
    "        print('No periodic functions will be fit')\n",
    "    else:\n",
    "        warnings.warn('Warning: Periodic fits might be unreliable for series less than 2 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the TS model into argument string for command line execution\n",
    "function_str = ''\n",
    "\n",
    "# Loop through basis functions\n",
    "for fcn in ts_functions.keys():\n",
    "    arg = ts_functions[fcn]\n",
    "    \n",
    "    if fcn == 'polynomial':\n",
    "        function_str += f' --polynomial {arg:d}'\n",
    "    \n",
    "    else:\n",
    "        # Check whether values are passed\n",
    "        if len(arg) > 0:\n",
    "            fcn = 'step' if fcn == 'stepDate' else fcn\n",
    "            # Append basis function name\n",
    "            function_str += f' --{fcn:s}'\n",
    "\n",
    "            # Append function arguments (e.g, period, decay, etc.)\n",
    "            if type(arg) == list:\n",
    "                function_str += ' {:s}'.format(' '.join([str(a) for a in arg]))\n",
    "            elif type(arg) == dict:\n",
    "                for event_date in arg.keys():\n",
    "                    function_str += f' {event_date:s}'\n",
    "                    for val in arg[event_date]:\n",
    "                        function_str += f' {val:f}'\n",
    "\n",
    "# Run command\n",
    "command = 'timeseries2velocity.py ' + timeseries_filename + function_str\n",
    "process = subprocess.run(command, shell=True)\n",
    "\n",
    "# Load velocity file\n",
    "insar_velocities,_ = readfile.read(vel_file, datasetName = 'velocity')  # read velocity file\n",
    "insar_velocities = insar_velocities * 1000.  # convert InSAR velocities from m/yr to mm/yr\n",
    "\n",
    "# Set masked pixels to NaN\n",
    "msk,_ = readfile.read(msk_file)\n",
    "insar_velocities[msk == 0] = np.nan\n",
    "insar_velocities[insar_velocities == 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the mean linear velocity fit. The MintPy `view` module automatically reads the temporal coherence mask `maskTempCoh.h5` and applies that to mask out pixels with unreliable velocities (white). Because there is a large coseismic displacement at the time of the earthquake but we are only estimating a single linear velocity, it absorbs the coseismic signal in this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scp_args = f\"velocity.h5 velocity -v {vmin/2} {vmax/2} --colormap RdBu --figtitle LOS_Velocity --unit mm/yr\"\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Note :</b> \n",
    "Negative values indicates that target is moving away from the radar (i.e., Subsidence in case of vertical deformation).\n",
    "Positive values indicates that target is moving towards the radar (i.e., uplift in case of vertical deformation). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='co_step'></a>\n",
    "### 4.2. Estimate InSAR Coseismic Displacement\n",
    "\n",
    "We can use the same time series to estimate the coseismic displacement as a Heaviside $H$ or step function at the time of an earthquake or a number $N_{eq}$ of earthquakes. In the above equation this is the $$\\sum_{j=1}^{N_{eq}} \\left( h_j+f_j F_j (t-t_j) \\right)H(t - t_j) \\;$$ set of terms. For simplicity, we consider only one earthquake and we assume the postseismic displacement functions $F$ are small compared to the coseismic displacements, so we only need to solve for the coefficient $h$ of each interferogram pixel.\n",
    "\n",
    "We call the MintPy `timeseries2velocity.py` program again and specify the time of the earthquake $t_j$. The fit will also include the linear velocity rate separated from the step function amplitude. Both estimated coefficients have their associated uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'timeseries2velocity.py ' + timeseries_filename + function_str\n",
    "process = subprocess.run(command, shell=True)\n",
    "\n",
    "EQdataset = 'step' + sitedata['sites'][site]['earthquakeDate']\n",
    "EQstep, _ = readfile.read(vel_file, datasetName = EQdataset)\n",
    "\n",
    "# Set masked pixels to NaN\n",
    "msk,_ = readfile.read(msk_file)\n",
    "EQstep[msk == 0] = np.nan\n",
    "EQstep[EQstep == 0] = np.nan\n",
    "\n",
    "# Convert coseismic step from m to mm\n",
    "EQstep *= 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can view the step function amplitude and the new linear velocity estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scp_args = f\"velocity.h5 step{sitedata['sites'][site]['earthquakeDate']} -v {vmin} {vmax} --colormap RdBu --unit mm --figtitle LOS_Coseismic\"\n",
    "view.main(scp_args.split())\n",
    "scp_args = f\"velocity.h5 velocity -v {vmin/2} {vmax/2} --colormap RdBu --unit mm/yr --figtitle LOS_Velocity\"\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='find_gps'></a>\n",
    "### 4.3. Find Collocated GNSS Stations\n",
    "\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales.\n",
    "\n",
    "MintPy supports several options for downloading and handling GNSS data. Currently supported data sets are:\n",
    "* UNR National Geodetic Laboratory\n",
    "* SIO/JPL MEaSUREs ESESES\n",
    "* JPL SIDESHOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GNSS processing source\n",
    "if 'gnss_source' in sitedata['sites'][site]:\n",
    "    gnss_source = sitedata['sites'][site]['gnss_source']\n",
    "else:\n",
    "    gnss_source = 'UNR'\n",
    "print(f\"GNSS processing source: {gnss_source:s}\")\n",
    "\n",
    "# Get analysis metadata from InSAR velocity file\n",
    "insar_metadata = readfile.read_attribute(vel_file)\n",
    "lat_step = float(insar_metadata['Y_STEP'])\n",
    "lon_step = float(insar_metadata['X_STEP'])\n",
    "(S,N,W,E) = ut.four_corners(insar_metadata)\n",
    "start_date = insar_metadata.get('START_DATE', None).split('T')[0]\n",
    "end_date = insar_metadata.get('END_DATE', None).split('T')[0]\n",
    "start_date_gnss = dt.strptime(start_date, \"%Y%m%d\")\n",
    "end_date_gnss = dt.strptime(end_date, \"%Y%m%d\")\n",
    "\n",
    "# Get center lat/lon of analysis region\n",
    "bbox = sitedata['sites'][site]['analysis_region'].replace(\"'\",\"\").split()\n",
    "lat0 = (float(bbox[0]) + float(bbox[1]))/2\n",
    "lon0 = (float(bbox[2]) + float(bbox[3]))/2\n",
    "\n",
    "# Identify geometry file x/y location associated with center lat/lon\n",
    "geom_obj = mintpy_dir + '/inputs/geometryGeo.h5'\n",
    "atr = readfile.read_attribute(geom_obj)\n",
    "coord = ut.coordinate(atr, lookup_file=geom_obj)\n",
    "y, x = coord.geo2radar(lat0, lon0)[0:2]\n",
    "y = max(0, y);  y = min(int(atr['LENGTH'])-1, y)\n",
    "x = max(0, x);  x = min(int(atr['WIDTH'])-1, x)\n",
    "\n",
    "# Write out inclination/azimuth at specified x/y location\n",
    "kwargs = dict(box=(x,y,x+1,y+1))\n",
    "inc_angle = readfile.read(geom_obj, datasetName='incidenceAngle', **kwargs)[0][0,0]\n",
    "az_angle  = readfile.read(geom_obj, datasetName='azimuthAngle',   **kwargs)[0][0,0]\n",
    "\n",
    "# Set GNSS Parameters\n",
    "gnss_completeness_threshold = 0.9    #0.9  #percent of data timespan with valid GNSS epochs\n",
    "# max threshold standard deviation of residuals to linear GNSS fit not specified\n",
    "\n",
    "# Search for collocated GNSS stations\n",
    "site_names, _, _ = gnss.search_gnss(SNWE=(S,N,W,E),\n",
    "                                    start_date=start_date,\n",
    "                                    end_date=end_date,\n",
    "                                    source=gnss_source)\n",
    "site_names = [str(stn) for stn in site_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='gps_ts'></a>\n",
    "### 4.4. Get GNSS Position Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the GNSS station data, and evaluate the station data quality based on data completeness and scatter of residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Empty dicts and lists to store GNSS data\n",
    "gnss_stns = {}\n",
    "bad_stns = {}\n",
    "\n",
    "print(f\"GNSS completion threshold: {gnss_completeness_threshold}\")\n",
    "\n",
    "for site_name in site_names:\n",
    "    gnss_stn = gnss.get_gnss_class(gnss_source)(site = site_name)\n",
    "    gnss_stn.open(print_msg=False)\n",
    "\n",
    "    # check if station is in masked out area\n",
    "    site_lat, site_lon = gnss_stn.get_site_lat_lon()\n",
    "    site_y, site_x = ut.coordinate(insar_metadata).geo2radar(site_lat, site_lon)[:2]\n",
    "    masked_loc = np.isnan(insar_velocities[site_y, site_x])\n",
    "\n",
    "    # count number of dates in time range\n",
    "    dates = gnss_stn.dates\n",
    "    range_days = (end_date_gnss - start_date_gnss).days\n",
    "    gnss_count = np.histogram(dates, bins=[start_date_gnss, end_date_gnss])\n",
    "    gnss_count = int(gnss_count[0][0])\n",
    "\n",
    "    # get standard deviation of residuals to linear fit\n",
    "    G = time_func.get_design_matrix4time_func(gnss_stn.date_list, ts_functions)\n",
    "    disp_los = ut.enu2los(gnss_stn.dis_e, gnss_stn.dis_n, gnss_stn.dis_u, inc_angle, az_angle)\n",
    "    m_hat = np.linalg.pinv(G).dot(disp_los)\n",
    "    dis_hat = np.dot(G, m_hat)\n",
    "    stn_stdv = np.std(disp_los - dis_hat)\n",
    "    \n",
    "    # select GNSS stations based on data completeness, scatter of residuals\n",
    "    # and whether they are in masked areas\n",
    "    if (range_days*gnss_completeness_threshold <= gnss_count) \\\n",
    "        and (masked_loc == False):\n",
    "        gnss_stns[site_name] = gnss_stn\n",
    "    else:\n",
    "        bad_stns[site_name] = gnss_stn\n",
    "\n",
    "# [optional] manually remove additional stations\n",
    "gnss_to_remove=[]\n",
    "\n",
    "for site_name in gnss_to_remove:\n",
    "    bad_stns[site_name] = gnss_stns[site_name]\n",
    "    del gnss_stns[site_name]\n",
    "\n",
    "# Final list of site names\n",
    "site_names = [*gnss_stns]\n",
    "bad_site_names = [*bad_stns]\n",
    "\n",
    "print(\"\\nFinal list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)\n",
    "print(\"List of {} stations removed from analysis\".format(len(bad_site_names)))\n",
    "print(bad_site_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stations in map view\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "cax = ax.imshow(EQstep, cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest', extent=(W, E, S, N))\n",
    "cbar = fig.colorbar(cax, ax=ax)\n",
    "cbar.set_label('LOS velocity [mm/year]')\n",
    "\n",
    "# Plot valid stations\n",
    "for i, gnss_stn in enumerate(gnss_stns.values()):\n",
    "    # Plot station\n",
    "    ax.scatter(gnss_stn.site_lon, gnss_stn.site_lat, s=8**2, c='k',\n",
    "               label=('valid' if i == 0 else None))\n",
    "    ax.annotate(gnss_stn.site, (gnss_stn.site_lon, gnss_stn.site_lat))\n",
    "\n",
    "# Plot rejected stations\n",
    "for i, gnss_stn in enumerate(bad_stns.values()):\n",
    "    # Plot station\n",
    "    ax.scatter(gnss_stn.site_lon, gnss_stn.site_lat, s=8**2,\n",
    "               marker='X', facecolor='none', edgecolor='k',\n",
    "              label=('rejected' if i == 0 else None))\n",
    "    ax.annotate(gnss_stn.site, (gnss_stn.site_lon, gnss_stn.site_lat))\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='gps_los'></a>\n",
    "### 4.5. Make GNSS LOS Velocities/Displacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our goals is to compare the coseismic displacements computed from InSAR, to those observed using continuous GNSS measurements. To make a direct comparison, we first projec the 3D GNSS displacement series into InSAR LOS (using the local incidence and azimuth angles). We then estimate the coseismic steps recorded by the GNSS using two methods:\n",
    "\n",
    "1. We calculate a simple difference in GNSS station position at two dates bracketing the earthquake. This will likely be a noisy estimate.\n",
    "2. We fit the LOS-projected GNSS displacement time-series using the same basis functions (i.e., polynomial and step term) that were used to fit the InSAR time-series. For this latter process, outliers are removed from the GNSS position data iteratively and the functional fits are recomputed. This should ensure the most robust estimate of coseismic displacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step date index\n",
    "step_ndx = ts_functions['polynomial'] + len(ts_functions['periodic']) + 1\n",
    "print(f\"Step function parameter at index {step_ndx:d}\")\n",
    "\n",
    "# Outlier ID and removal parameters\n",
    "outlier_thresh = 3  # standard deviations\n",
    "outlier_iters = 1  # iterations\n",
    "\n",
    "# Loop through valid GNSS stations\n",
    "gnss_fits = {}  # empty dict to store fits\n",
    "gnss_sites = {}  # empty dict to store site measurements\n",
    "\n",
    "for i, site_name in enumerate(site_names):\n",
    "    # Retrieve station information\n",
    "    gnss_stn = gnss_stns[site_name]\n",
    "    gnss_stn.get_los_displacement(insar_metadata,\n",
    "                                  start_date=start_date,\n",
    "                                  end_date=end_date)\n",
    "\n",
    "    # Scale displacement values of m to mm\n",
    "    scale_gnss_m_to_mm(gnss_stn)\n",
    "\n",
    "    # Outlier detection and removal\n",
    "    gnss_fit = IterativeOutlierFit(gnss_stn.dates, gnss_stn.dis_los,\n",
    "                                   model=ts_functions, threshold=outlier_thresh,\n",
    "                                   max_iter=outlier_iters)\n",
    "    gnss_fits[site_name] = gnss_fit  # record for posterity\n",
    "\n",
    "    # Record GNSS site velocity\n",
    "    gnss_site = SiteDisplacement(site=site_name,\n",
    "                                 site_lon=gnss_stn.site_lon,\n",
    "                                 site_lat=gnss_stn.site_lat,\n",
    "                                 dis=gnss_fit.m_hat[step_ndx],\n",
    "                                 dis_err=gnss_fit.mhat_se[step_ndx],\n",
    "                                 unit='mm')\n",
    "    gnss_sites[site_name] = gnss_site\n",
    "\n",
    "    # Report\n",
    "    if i == 0 :\n",
    "        print('site displacement(mm)')\n",
    "    print(str(gnss_site))\n",
    "\n",
    "    # Plotting parameters\n",
    "    n_dates = len(gnss_stn.dates)\n",
    "    label_skips = n_dates//6\n",
    "    ax_nb = i % 2\n",
    "\n",
    "    # Spawn figure for even numbers\n",
    "    if ax_nb == 0:\n",
    "        fig, axes = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "\n",
    "    # Plot outliers\n",
    "    if gnss_fit.n_outliers > 0:\n",
    "        axes[ax_nb].scatter(gnss_fit.outlier_dates,\n",
    "                            gnss_fit.outlier_dis,\n",
    "                            2**2, 'firebrick', label='outliers')\n",
    "\n",
    "    # Plot filtered data and model fit\n",
    "    axes[ax_nb].scatter(gnss_fit.dates, gnss_fit.dis, 3**2,\n",
    "                        'dimgrey', zorder=1)\n",
    "    axes[ax_nb].plot(gnss_fit.dates, gnss_fit.dis_hat,\n",
    "                     'c', linewidth=3, label='model fit', zorder=2)\n",
    "    axes[ax_nb].plot(gnss_fit.dates, gnss_fit.err_envelope[0],\n",
    "                     'b--', zorder=3)\n",
    "    axes[ax_nb].plot(gnss_fit.dates, gnss_fit.err_envelope[1],\n",
    "                     'b--', zorder=3, label='95% conf')\n",
    "\n",
    "    # Format plot\n",
    "    axes[ax_nb].legend()\n",
    "    axes[ax_nb].set_xticks(gnss_stn.dates[::label_skips])\n",
    "    axes[ax_nb].set_xticklabels([date.strftime('%Y-%m-%d') \\\n",
    "                                 for date in gnss_stn.dates[::label_skips]],\n",
    "                                rotation=80)\n",
    "    axes[ax_nb].set_ylabel('LOS displacement (mm)')\n",
    "    axes[ax_nb].set_title(str(gnss_site))\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='reference'></a>\n",
    "### 4.6. Re-reference GNSS and InSAR LOS Coseismic Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reference GNSS stations to reference site\n",
    "ref_site = sitedata['sites'][site]['gps_ref_site_name']\n",
    "if sitedata['sites'][site]['gps_ref_site_name'] == 'auto':\n",
    "    ref_site = [*gnss_sites.keys()][0]\n",
    "\n",
    "gnss_ref_dis = gnss_sites[ref_site].dis\n",
    "print(f\"Using GNSS reference station: {ref_site}\")\n",
    "\n",
    "for site_name in site_names:\n",
    "    gnss_sites[site_name].dis -= gnss_ref_dis\n",
    "    print(str(gnss_sites[site_name]))\n",
    "\n",
    "# Reference InSAR to GNSS reference site\n",
    "(ref_y,\n",
    " ref_x,\n",
    " _, _) = ut.coordinate(insar_metadata).geo2radar(gnss_sites[ref_site].site_lat,\n",
    "                                                 gnss_sites[ref_site].site_lon)\n",
    "\n",
    "insar_ref_step = EQstep[ref_y, ref_x]\n",
    "EQstep = EQstep - insar_ref_step\n",
    "print(f\"Insar reference step {insar_ref_step:.1f} mm\")\n",
    "\n",
    "# Plot GNSS stations on InSAR velocity field\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "cax = ax.imshow(EQstep, cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                interpolation='nearest', extent=(W, E, S, N))\n",
    "cbar = fig.colorbar(cax, ax=ax)\n",
    "cbar.set_label('LOS EQ displacement [mm]')\n",
    "\n",
    "for site_name in site_names:\n",
    "    gnss_site = gnss_sites[site_name]\n",
    "    color = cmap((gnss_site.dis - vmin)/(vmax-vmin))\n",
    "    ax.scatter(gnss_site.site_lon, gnss_site.site_lat, color=color, s=8**2, edgecolors='k')\n",
    "    ax.text(gnss_site.site_lon, gnss_site.site_lat, gnss_site.site)\n",
    "    ax.set_title(f\"{gnss_source} GNSS step on InSAR step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='validation1'></a>\n",
    "## 5. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='make_resid'></a>\n",
    "### 5.1. Make GNSS-InSAR Displacement Residuals at GNSS Station Locations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test how well the InSAR measurements agree with the GNSS, we extract the InSAR displacement value at each GNSS site location and compute the differences (residuals) between the two data sets ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "pixel_radius = 5   #number of InSAR pixels to average for comparison with GNSS\n",
    "\n",
    "# Loop over InSAR measurements at GNSS station locations\n",
    "insar_sites = {}  # empty dict to store site measurements\n",
    "\n",
    "for i, site_name in enumerate(site_names):\n",
    "    gnss_site = gnss_sites[site_name]\n",
    "    \n",
    "    # Convert GNSS station lat/lon information to InSAR x/y grid\n",
    "    stn_y, stn_x, _, _ = coord.geo2radar(gnss_site.site_lat,\n",
    "                                         gnss_site.site_lon)\n",
    "\n",
    "    # Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    # To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    dis_px_rad = EQstep[stn_y-pixel_radius:stn_y+1+pixel_radius,\n",
    "                        stn_x-pixel_radius:stn_x+1+pixel_radius]\n",
    "    insar_site_dis = np.nanmedian(dis_px_rad)\n",
    "\n",
    "    # Assign to object\n",
    "    insar_site = SiteDisplacement(site=site_name,\n",
    "                              site_lon=gnss_site.site_lon,\n",
    "                              site_lat=gnss_site.site_lat,\n",
    "                              dis=insar_site_dis,\n",
    "                              unit='mm')\n",
    "    insar_sites[site_name] = insar_site\n",
    "\n",
    "    # Report\n",
    "    if i == 0 :\n",
    "        print('site displacement(mm)')\n",
    "    print(str(insar_site))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='make_ddiff'></a>\n",
    "### 5.2. Make Double-differenced Displacement Residuals (from Step Function Approach)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the \"double difference\" residuals between pairs of collocated InSAR and GNSS measurements. The double difference effectively cancels out any remaining referencing errors between the InSAR and GNSS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute double differences between site pairs\n",
    "n_sites = len(site_names)\n",
    "\n",
    "# Loop over stations\n",
    "double_diffs = {}  # empty dict to populate with InSAR - GNSS diffs\n",
    "for i in range(n_sites - 1):\n",
    "    stn1 = site_names[i]\n",
    "    for j in range(i + 1, n_sites):\n",
    "        stn2 = site_names[j]\n",
    "\n",
    "        # Compute distance (km) between site locations\n",
    "        site_dist = haversine_distance(gnss_sites[stn1].site_lon,\n",
    "                                       gnss_sites[stn1].site_lat,\n",
    "                                       gnss_sites[stn2].site_lon,\n",
    "                                       gnss_sites[stn2].site_lat)\n",
    "        \n",
    "        # Calculate site-to-site differences\n",
    "        diff_name = f\"{stn1}-{stn2}\"\n",
    "        gnss_diff = gnss_sites[stn1] - gnss_sites[stn2]\n",
    "        insar_diff = insar_sites[stn1] - insar_sites[stn2]\n",
    "        diff_res = gnss_diff - insar_diff\n",
    "        diff_res.dist = site_dist\n",
    "        diff_res.site = diff_name\n",
    "\n",
    "        # Record to dictionary\n",
    "        double_diffs[diff_name] = diff_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='amp_vs_dist'></a>\n",
    "### 5.3. Amplitude vs. Distance of Double-differences (not quite a structure function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coseismic requirement is that, for earthquakes that generate surface displacements &ge; 100 mm, at least 68.3% of surface displacements should be measured with an accuracy of $4(1 + \\sqrt{L})$ or better, over length scales of $0.1 \\mathrm{km} < L < 50 \\mathrm{km}$, where $L$ is the distance from one sample point to another. We assume that the distribution of residuals is Gaussian and that the requirement success threshold represents a 1-sigma limit within which we expect 68.3% of residuals to lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set requirement thresholds\n",
    "coseismic_distance_rqmt = (0.1, 50)  # distances for evaluation\n",
    "coseismic_threshold_rqmt = lambda L: 4 * (1 + np.sqrt(L))  # coseismic threshold in mm\n",
    "\n",
    "n_bins = 10  # number of distance bins for analysis\n",
    "threshold = 0.683  # fraction of Gaussian normal distribution for pass/fail\n",
    "\n",
    "# Define validation parameters\n",
    "sensor = 'Sentinel-1'\n",
    "validation_data = 'GNSS'\n",
    "\n",
    "# Write data for statistical tests\n",
    "site_dist = np.array([diff_res.dist for diff_res in double_diffs.values()])\n",
    "double_diff_rel_measure = np.abs(np.array([diff_res.dis for diff_res in double_diffs.values()]))\n",
    "double_diff_rel_measure_err = np.array([diff_res.dis_err for diff_res in double_diffs.values()])\n",
    "\n",
    "# Validation figure and assessment\n",
    "(validation_table_method1,\n",
    " validation_fig_method1) = display_coseismic_validation(site_dist,                           # binned distance for point\n",
    "                                     double_diff_rel_measure,             # binned double-difference velocities mm/yr\n",
    "                                     site,                                # cal/val site name\n",
    "                                     start_date,                          # start date of InSAR dataset\n",
    "                                     end_date,                            # end date of InSAR dataset \n",
    "                                     requirement=coseismic_threshold_rqmt,  # measurement requirement to meet, e.g 2 mm/yr for 3 years of data over 0.1-50km\n",
    "                                     distance_rqmt=coseismic_distance_rqmt, # distance over requirement is to meet, e.g. over length scales of 0.1-50 km [0.1, 50] \n",
    "                                     n_bins=n_bins,                       # number of bins, to collect statistics \n",
    "                                     threshold=threshold,                 # quantile threshold for point-pairs that pass requirement, e.g. 0.683 - we expect 68.3% of residuals to lie. \n",
    "                                     sensor='Sentinel-1',                 # sensor that is validated, Sentinel-1 or NISAR\n",
    "                                     validation_type=requirement.lower(), # validation for: secular, transient, coseismic requirement\n",
    "                                     validation_data='GNSS (step)') # validation method: GNSS - Method 1, InSAR - Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_validation_table(validation_table_method1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1—Succesful when 68.3% of points below requirements line\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Method 1 results to file\n",
    "save_fldr = f\"{dt.now().strftime('%Y%m%dT%H%M%S')}-Coseismic-Method1\"\n",
    "save_dir = os.path.join(mintpy_dir, save_fldr)\n",
    "\n",
    "save_params = {\n",
    "    'method': '1',\n",
    "    'save_dir': save_dir,\n",
    "    'site_info': sitedata['sites'][site],\n",
    "    'validation_table': validation_table_method1,\n",
    "    'validation_fig': validation_fig_method1,\n",
    "}\n",
    "save_results(**save_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='validation2'></a>\n",
    "## 6. NISAR Validation Approach 2: InSAR-only Structure Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Validation Approach 2, we use a date when there was no earthquake and do the same step function fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that an earthquake date is specified and retrieve that date\n",
    "if 'noEarthquakeDate' in sitedata['sites'][site].keys():\n",
    "    noEQdate = [sitedata['sites'][site]['noEarthquakeDate']]\n",
    "    print('No-earthquake date: {:s}'.format(sitedata['sites'][site]['noEarthquakeDate']))\n",
    "else:\n",
    "    noEQdate = []\n",
    "    warnings.warn('No non-earthquake date specified. This data set might not be suitable for the coseismic workflow.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(mintpy_dir)  # reset directory in case running out of sequence\n",
    "print(f\"Present directory: {mintpy_dir:s}\")\n",
    "\n",
    "# Step function fit when there was no earthquake\n",
    "command = 'timeseries2velocity.py ' + timeseries_filename + ' --step ' + sitedata['sites'][site]['noEarthquakeDate']\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can view the step function amplitude for the non-earthquake date and the new linear velocity estimate. Because we have the step function at a time far away from the earthquake, the step function has a small amplitude and the linear velocity has absorbed the time-series displacements. The non-earthquake step plot has the same color scale as the earthquake coseismic plot above to show the small atmospheric noise of a date without an earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp_args = f\"velocity.h5 step{sitedata['sites'][site]['noEarthquakeDate']} -v {vmin} {vmax} --colormap RdBu --unit mm --figtitle LOS_Coseismic\"\n",
    "view.main(scp_args.split())\n",
    "scp_args = f\"velocity.h5 velocity -v {vmin/2} {vmax/2} --colormap RdBu --unit mm/yr --figtitle LOS_Velocity\"\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='array_mask'></a>\n",
    "### 6.1. Read Array and Mask Pixels with no Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the non-earthquake displacement as the insar_displacment for statistics and convert to mm\n",
    "noEQdataset = 'step' + sitedata['sites'][site]['noEarthquakeDate']\n",
    "noEQstep, insar_metadata = readfile.read(vel_file, datasetName = noEQdataset)  #read coseismic step \n",
    "(S,N,W,E) = ut.four_corners(insar_metadata)\n",
    "noEQdate = sitedata['sites'][site]['noEarthquakeDate']\n",
    "\n",
    "insar_displacement = noEQstep * 1000. # convert from m to mm and add dimension to array to allow multiple datasets\n",
    "\n",
    "ifgs_date = np.array([noEQdate])  # only one non-earthquake date for now\n",
    "n_ifgs = insar_displacement.shape[0]\n",
    "\n",
    "# Mask out no-data areas\n",
    "msk, _ = readfile.read(msk_file)\n",
    "insar_displacement[msk == 0] = np.nan\n",
    "insar_displacement[insar_displacement == 0] = np.nan\n",
    "\n",
    "# Display map of data after masking\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.imshow(insar_displacement, vmin=vmin, vmax=vmax, cmap=cmap, interpolation='nearest', extent=(W, E, S, N))\n",
    "ax.set_title(\"Coseismic \\n Date \"+noEQdate)\n",
    "cbar1 = fig.colorbar(img1, ax=ax)\n",
    "cbar1.set_label('LOS displacement [mm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='remove_trend'></a>\n",
    "### 6.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate the coordinate for every pixel.\n",
    "\n",
    "Then for each non-earthquake step fit, randomly selected pixels need to be paired up. In order to keep measurements independent, different pixel pairs can not share same pixel. This is achieved by pairing up in sequence, i.e., pairing up pixel number 1 and number 2, 3 and 4...\n",
    "\n",
    "To pass the requirement, the pixel-to-pixels differences must satisfy the same $4(1 + \\sqrt{L})$ or better, over length scales of $0.1 \\mathrm{km} < L < 50 \\mathrm{km}$ requirement specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set requirement thresholds\n",
    "coseismic_distance_rqmt = (0.1, 50)  # distances for evaluation\n",
    "coseismic_threshold_rqmt = lambda L: 4 * (1 + np.sqrt(L))  # coseismic threshold in mm\n",
    "\n",
    "n_bins = 10  # number of distance bins for analysis\n",
    "threshold = 0.683  # fraction of Gaussian normal distribution for pass/fail\n",
    "\n",
    "sensor = 'Sentinel-1'\n",
    "validation_data = 'InSAR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mode = 'points'  # 'points' or 'profile'\n",
    "# note that the 'profile' method may take significantly longer\n",
    "\n",
    "# Collect samples using the specified method\n",
    "if sample_mode in ['points']:\n",
    "    X0,Y0 = load_geo(insar_metadata)\n",
    "    X0_2d,Y0_2d = np.meshgrid(X0,Y0)\n",
    "\n",
    "    insar_sample_dist, insar_rel_measure = samp_pair(X0_2d, Y0_2d, insar_displacement, num_samples=1000000)\n",
    "\n",
    "elif sample_mode in ['profile']:\n",
    "    # Sample grid setup\n",
    "    length, width = int(insar_metadata['LENGTH']), int(insar_metadata['WIDTH'])\n",
    "    X = np.linspace(W+lon_step, E-lon_step, width)  # longitudes\n",
    "    Y = np.linspace(N+lat_step, S-lat_step, length)  # latitudes\n",
    "    X_coords, Y_coords = np.meshgrid(X, Y)\n",
    "\n",
    "    # Draw random samples from map (without replacement)\n",
    "    num_samples = 20000\n",
    "    \n",
    "    # Retrieve profile samples\n",
    "    insar_sample_dist, insar_rel_measure = profile_samples(\\\n",
    "                    x=X_coords.reshape(-1,1),\n",
    "                    y=Y_coords.reshape(-1,1),\n",
    "                    data=insar_displacement,\n",
    "                    metadata=insar_metadata,\n",
    "                    len_rqmt=secular_distance_rqmt,\n",
    "                    num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the statistical property of selected pixel pairs and overall histogram of relative measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.hist(insar_sample_dist, bins=100)\n",
    "ax.set_title(f\"Histogram of distance \\n Coseismic Date \"+noEQdate)\n",
    "ax.set_xlabel(r'Distance ($km$)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlim(0,50)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.hist(insar_rel_measure, bins=100)\n",
    "ax.set_title(f\"Histogram of Relative Measurement \\n Coseismic Date \"+noEQdate)\n",
    "ax.set_xlabel(r'Relative Measurement ($mm$)')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='validation2'></a>\n",
    "### 6.3. Coseismic Requirement Validation: Method 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In approach 2, the number of pixel pairs which meet the mission requirement as a percentage of the total number of pixel pairs selected are counted.\n",
    "\n",
    "The method we apply to evaluate the noise structure is similar to that in the InSAR-GNSS comparison. We count the percentage of measurements that fall below the threshold curve  for each of the 5-km-wide bins. If the average of the percentages from all bins is larger than 0.683, we judge that the noise level falls below the requirement.\n",
    "\n",
    "Then we prepare table of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation figure and assessment\n",
    "(validation_table_method2,\n",
    " validation_fig_method2) = display_coseismic_validation(insar_sample_dist,                   # binned distance for point\n",
    "                                     insar_rel_measure,                   # binned double-difference velocities mm/yr\n",
    "                                     site,                                # cal/val site name\n",
    "                                     start_date,                          # start date of InSAR dataset\n",
    "                                     end_date,                            # end date of InSAR dataset \n",
    "                                     requirement=coseismic_threshold_rqmt,  # measurement requirement to meet, e.g 2 mm/yr for 3 years of data over 0.1-50km\n",
    "                                     distance_rqmt=coseismic_distance_rqmt, # distance over requirement is to meet, e.g. over length scales of 0.1-50 km [0.1, 50] \n",
    "                                     n_bins=n_bins,                       # number of bins, to collect statistics \n",
    "                                     threshold=threshold,                 # quantile threshold for point-pairs that pass requirement, e.g. 0.683 - we expect 68.3% of residuals to lie. \n",
    "                                     sensor='Sentinel-1',                 # sensor that is validated, Sentinel-1 or NISAR\n",
    "                                     validation_type=requirement.lower(), # validation for: secular, transient, coseismic requirement\n",
    "                                     validation_data=validation_data)     # validation method: GNSS - Method 1, InSAR - Method 2\n",
    "\n",
    "out_fig = os.path.abspath('coseismic_insar-only_vs_distance_'+site+'_date'+noEQdate+'.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_validation_table(validation_table_method2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2 table of distance bins—\n",
    "    68% of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Method 2 results to file\n",
    "save_fldr = f\"{dt.now().strftime('%Y%m%dT%H%M%S')}-Coseismic-Method2\"\n",
    "save_dir = os.path.join(mintpy_dir, save_fldr)\n",
    "\n",
    "save_params = {\n",
    "    'method': '2',\n",
    "    'save_dir': save_dir,\n",
    "    'site_info': sitedata['sites'][site],\n",
    "    'validation_table': validation_table_method2,\n",
    "    'validation_fig': validation_fig_method2\n",
    "}\n",
    "save_results(**save_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='appendix'></a>\n",
    "## Appendix: Supplementary Comparisons and Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='compare_raw'></a>\n",
    "### A.1. Compare Raw Displacements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.hist([insar_site.dis for insar_site in insar_sites.values()],\n",
    "         range=[vmin/2, vmax/2], bins=50, color=\"green\", edgecolor='grey', label='V_InSAR')\n",
    "plt.hist([gnss_site.dis for gnss_site in gnss_sites.values()],\n",
    "         range=[vmin/2, vmax/2], bins=50, color=\"orange\", edgecolor='grey', label='V_gnss', alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f'''Displacements \\n Date range {start_date}-{end_date}\n",
    "Reference stn: {sitedata['sites'][site]['gps_ref_site_name']}\n",
    "Number of stations used: {len(site_names)}''')\n",
    "plt.xlabel('LOS Displacement (mm)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='plot_residuals'></a>\n",
    "### A.2. Plot Displacement Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.hist([(insar_sites[site_name] - gnss_sites[site_name]).dis for site_name in site_names],\n",
    "         bins = 40, range=[vmin/2, vmax/2], edgecolor='grey', color=\"darkblue\", linewidth=1,\n",
    "         label='V_gnss - V_InSAR (area average)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f'''Residuals \\n Date range {start_date}-{end_date}\n",
    "Reference stn: {sitedata['sites'][site]['gps_ref_site_name']}\n",
    "Number of stations used: {len(site_names)}''')\n",
    "plt.xlabel('Displacement Residual (mm)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='plot_ddiff'></a>\n",
    "### A.3. Plot Double-differenced Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.hist([diff_res.dis for diff_res in double_diffs.values()],\n",
    "         range = [vmin/2, vmax/2],bins = 40, color = \"darkblue\",edgecolor='grey',\n",
    "         label='V_gnss_(s1-s2) - V_InSAR_(s1-s2)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f'''Difference Residualts \\n Date range {start_date}-{end_date}\n",
    "Reference stn: {sitedata['sites'][site]['gps_ref_site_name']}\n",
    "Number of stations used: {len(site_names)}''')\n",
    "plt.xlabel('Double Differenced Velocity Residual (mm)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='plot_timeseries'></a>\n",
    "### A.4. GNSS Time-series Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab the time-series file used for time function estimation given the template setup\n",
    "template = readfile.read_template(os.path.join(mintpy_dir, 'smallbaselineApp.cfg'))\n",
    "template = ut.check_template_auto_value(template)\n",
    "ts_file = TimeSeriesAnalysis.get_timeseries_filename(template, mintpy_dir)['velocity']['input']\n",
    "\n",
    "# Read the time-series file\n",
    "insar_ts, ts_metadata = readfile.read(ts_file, datasetName='timeseries')\n",
    "insar_ts *= 1000  # meter to mm\n",
    "mask = readfile.read(os.path.join(mintpy_dir, 'maskTempCoh.h5'))[0]\n",
    "print(f\"reading timeseries from file: {ts_file}\")\n",
    "\n",
    "# Get date list\n",
    "date_list = timeseries(ts_file).get_date_list()\n",
    "num_date = len(date_list)\n",
    "date0, date1 = date_list[0], date_list[-1]\n",
    "insar_dates = ptime.date_list2vector(date_list)[0]\n",
    "\n",
    "# Spatial reference\n",
    "coord = ut.coordinate(ts_metadata)\n",
    "ref_site = sitedata['sites'][site]['gps_ref_site_name']\n",
    "if sitedata['sites'][site]['gps_ref_site_name'] == 'auto':\n",
    "    ref_site = [*gnss_sites.keys()][0]\n",
    "ref_gnss_obj = gnss_stns[ref_site]\n",
    "ref_lat, ref_lon = ref_gnss_obj.get_site_lat_lon()\n",
    "ref_y, ref_x = coord.geo2radar(ref_lat, ref_lon)[:2]\n",
    "if not mask[ref_y, ref_x]:\n",
    "    raise ValueError(f'Given reference GNSS site ({ref_site}) is in mask-out unrelible region in InSAR! Change to a different site.')\n",
    "ref_insar_dis = insar_ts[:, ref_y, ref_x]\n",
    "\n",
    "# Plot displacements and velocity timeseries at GNSS station locations\n",
    "num_site = len(site_names)\n",
    "prog_bar = ptime.progressBar(maxValue=num_site)\n",
    "for i, site_name in enumerate(site_names):\n",
    "    prog_bar.update(i+1, suffix=f'{site_name} {i+1}/{num_site}')\n",
    "\n",
    "    ## Read data\n",
    "    # Recall gnss station displacements with outliers removed\n",
    "    gnss_obj = gnss_stns[site_name]\n",
    "    gnss_lalo = (gnss_obj.site_lat, gnss_obj.site_lon)\n",
    "\n",
    "    # Get relative LOS displacement on common dates\n",
    "    gnss_dates = np.array(sorted(list(set(gnss_obj.dates) & set(ref_gnss_obj.dates))))\n",
    "    gnss_dis = np.zeros(gnss_dates.shape, dtype=np.float32)\n",
    "    for i, date_i in enumerate(gnss_dates):\n",
    "        idx1 = np.where(gnss_obj.dates == date_i)[0][0]\n",
    "        idx2 = np.where(ref_gnss_obj.dates == date_i)[0][0]\n",
    "        gnss_dis[i] = gnss_obj.dis_los[idx1] - ref_gnss_obj.dis_los[idx2]\n",
    "    \n",
    "    # Shift GNSS to zero-mean in time [for plotting purpose]\n",
    "    gnss_dis -= np.nanmedian(gnss_dis)\n",
    "\n",
    "    # Read InSAR\n",
    "    y, x = coord.geo2radar(gnss_lalo[0], gnss_lalo[1])[:2]\n",
    "    insar_dis = insar_ts[:, y, x] - ref_insar_dis\n",
    "\n",
    "    # Apply a constant shift in time to fit InSAR to GNSS\n",
    "    comm_dates = sorted(list(set(gnss_dates) & set(insar_dates)))\n",
    "    if comm_dates:\n",
    "        insar_flag = [x in comm_dates for x in insar_dates]\n",
    "        gnss_flag = [x in comm_dates for x in gnss_dates]\n",
    "        insar_dis -= np.nanmedian(insar_dis[insar_flag] - gnss_dis[gnss_flag])\n",
    "\n",
    "    ## Plot figure\n",
    "    if gnss_dis.size > 0 and np.any(~np.isnan(insar_dis)):\n",
    "        fig, ax = plt.subplots(figsize=(10, 3))\n",
    "        ax.axhline(color='grey',linestyle='dashed', linewidth=2)\n",
    "        ax.scatter(gnss_dates, gnss_dis, s=2**2, label=\"GNSS Daily Positions\")\n",
    "        ax.scatter(insar_dates, insar_dis, label=\"InSAR Positions\")\n",
    "        # axis format\n",
    "        ax.set_title(f\"Station Name: {site_name}\") \n",
    "        ax.set_ylabel('LOS displacement [mm]')\n",
    "        ax.legend()\n",
    "prog_bar.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.local-solid_earth_atbd_dev]",
   "language": "python",
   "name": "conda-env-.local-solid_earth_atbd_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
