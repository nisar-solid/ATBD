{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Workflow to Validate NISAR L2 Transient Displacement Requirements\n",
    "\n",
    "**Original code authored by:** NISAR Science Team Members and Affiliates  \n",
    "\n",
    "*May 13, 2022*\n",
    "\n",
    "*NISAR Solid Earth Team*\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Both the initial setup (<b>Prep A</b> section) and download of the data (<b>Prep B</b> section) should be run at the start of the notebook. And all subsequent sections NEED to be run in order.\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CalVal Site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a site and track direction\n",
    "# Available transient displacement validation sites: \n",
    "#        CentralValleyA144 : Central Valley in California Sentinel-1 track 144  \n",
    "\n",
    "site='CentralValleyA144'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='transient_prep_a'></a>\n",
    "## Prep A. Environment Setup\n",
    "Setup your environment for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import copy\n",
    "from datetime import datetime as dt\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mintpy.utils import readfile, utils as ut\n",
    "from mintpy.objects import gps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "\n",
    "from solid_utils.sampling import load_geo, samp_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Set Directories ##########################################\n",
    "print('\\nCurrent directory:',os.getcwd())\n",
    "\n",
    "if 'work_dir' not in locals():\n",
    "    work_dir = Path.cwd()/'work'/'transient_ouputs'/site\n",
    "\n",
    "print(\"Work directory:\", work_dir)\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Change to Workdir   \n",
    "os.chdir(work_dir)\n",
    "       \n",
    "gunw_dir = work_dir/'products'\n",
    "gunw_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"   GUNW    dir:\", gunw_dir) \n",
    "    \n",
    "mintpy_dir = work_dir/'MintPy' \n",
    "mintpy_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"   MintPy  dir:\", mintpy_dir)\n",
    "############################################################################\n",
    "### List of CalVal Sites:\n",
    "'''\n",
    "Set NISAR calval sites:\n",
    "    CentralValleyA144  : Central Valley\n",
    "    OklahomaA107       : Oklahoma\n",
    "    PuertoRicoD98      : Puerto Rico (Earthquake M6.4 on 20200107) - Descending track \n",
    "    PuertoRicoA135     : Puerto Rico (Earthquake M6.4 on 20200107 & large aftershock on 20200703) - Ascending track\n",
    "    RidgecrestD71      : Ridgecrest  (Earthquake M7.2 on 20190705) - Descending track\n",
    "    RidgecrestA64      : Ridgecrest  (Earthquake M7.2 on 20190705) - Ascending track\n",
    "\n",
    "ARIA & MintPy parameters:\n",
    "    calval_location : name\n",
    "    download_region : download box in S,N,W,E format\n",
    "    analysis_region : analysis box in S,N,W,E format (must be within download_region)\n",
    "    download_start_date : download start date as YYYMMDD  \n",
    "    download_end_date   : download end date as YYYMMDD\n",
    "    earthquakeDate :  arbitrary date for testing with the central_valley dataset\n",
    "    sentinel_track : sentinel track to download\n",
    "    gps_ref_site_name : Name of the GPS site for InSAR re-referencing\n",
    "    tempBaseMax' : maximum number of days, 'don't use interferograms longer than this value \n",
    "    ifgExcludeList : default is not to exclude any interferograms\n",
    "    maskWater' :  interior locations don't need to mask water\n",
    "'''\n",
    "sites = {\n",
    "    ##########  CENTRAL VALLEY ##############\n",
    "    'CentralValleyA144' : {'calval_location' : 'Central_Valley',\n",
    "            'download_region' : '\"36.18 36.26 -119.91 -119.77\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"35.77 36.75 -120.61 -118.06\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'download_start_date' : '20180101',\n",
    "            'download_end_date' : '20190101',\n",
    "            'earthquakeDate' : '20180412',                       # arbitrary date for testing with the central_valley dataset\n",
    "            'sentinel_track' : '144',\n",
    "            'gps_ref_site_name' : 'CAWO',\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'maskWater' : False},                       # reference site for this area\n",
    "    ##########  OKLAHOMA ##############\n",
    "    'OklahomaA107' : {'calval_location' : 'Oklahoma',\n",
    "            'download_region' : '\"31.7 37.4 -103.3 -93.5\"',      # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"35.25 36.5 -100.5 -98.5\"',     # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'download_start_date' : '20210101',\n",
    "            'download_end_date' : '20210801',\n",
    "            'earthquakeDate' : '20210328',                       # arbitrary date for testing with the Oklahoma dataset\n",
    "            'sentinel_track' : '107',\n",
    "            'gps_ref_site_name' : 'OKCL',\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'maskWater' : False},\n",
    "    ##########  PUERTO RICO ##############\n",
    "    'PuertoRicoD98' : {'calval_location' : 'PuertoRicoDesc',\n",
    "            'download_region' : '\"17.5 18.9 -67.5 -66.0\"',       # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"17.9 18.5 -67.3 -66.2\"',       # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'download_start_date' : '20190701',\n",
    "            'download_end_date' : '20200930',\n",
    "            'earthquakeDate' : '20200107',                       # date of M6.4 quake\n",
    "            'sentinel_track' : '98',                             # descending track\n",
    "            'gps_ref_site_name' : 'PRLT',\n",
    "            'tempBaseMax' : 24,                                  # don't use interferograms longer than 24 days\n",
    "            'ifgExcludeList' : 'auto', \n",
    "            'maskWater' : True},                                 # need to mask ocean around Puerto Rico island\n",
    "    'PuertoRicoA135' : {'calval_location' : 'PuertoRicoAsc',\n",
    "             'download_region' : '\"17.5 18.9 -67.5 -66.0\"',      # download box in S,N,W,E format\n",
    "             'analysis_region' : '\"17.9 18.5 -67.3 -66.2\"',      # analysis box in S,N,W,E format (must be within download_region)\n",
    "             'download_start_date' : '20190701',\n",
    "             'download_end_date' : '20200930',\n",
    "             'earthquakeDate' : '20200107',                      # date of M6.4 quake\n",
    "             'earthquakeDate2' : '20200703',                     # date of large aftershock\n",
    "             'sentinel_track' : '135',                           # ascending track\n",
    "             'gps_ref_site_name' : 'PRLT',\n",
    "             'tempBaseMax' : 24,                                 # don't use interferograms longer than 24 days\n",
    "             'ifgExcludeList' : 'auto',\n",
    "             'maskWater' : True},                                # need to mask ocean around Puerto Rico island\n",
    "    ##########  RIDGECREST ##############\n",
    "    'RidgecrestD71': {'calval_location' : 'RidgecrestD71',\n",
    "                      'download_region' : '\"34.5 37.5 -119.0 -116.0\"', # download box in S,N,W,E format\n",
    "                      'analysis_region' : '\"34.7 37.2 -118.9 -116.1\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "                      'download_start_date' : '20190601',\n",
    "                      'download_end_date' : '20190831',\n",
    "                      'earthquakeDate' : '20190705',                   # M7.2 quake date at Ridgecrest\n",
    "                      'sentinel_track' : '71',\n",
    "                      'gps_ref_site_name' : 'ISLK',\n",
    "                      'tempBaseMax' : 'auto',\n",
    "                      'ifgExcludeList' : 'auto',\n",
    "                      'maskWater' : False},\n",
    "    'RidgecrestA64': {'calval_location' : 'Ridgecrest',\n",
    "                      'download_region' : '\"34.5 37.5 -119.0 -116.0\"', # download box in S,N,W,E format\n",
    "                      'analysis_region' : '\"34.7 37.2 -118.9 -116.1\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "                      'download_start_date' : '20190101',\n",
    "                      'download_end_date' : '20191231',\n",
    "                      'earthquakeDate' : '20190705',                   # M7.2 quake date at Ridgecrest\n",
    "                      'sentinel_track' : '64',\n",
    "                      'gps_ref_site_name' : 'ISLK',\n",
    "                      'tempBaseMax' : 'auto',\n",
    "                      'ifgExcludeList' : '[50,121,123,124,125,126]',   # list of bad ifgs to exclude from time-series analysis\n",
    "                      'maskWater' : False}\n",
    "}\n",
    "transient_available_sites = ['CentralValleyA144']\n",
    "\n",
    "if site not in transient_available_sites:\n",
    "    msg = '\\nSelected site not available! Please select one of the following sites:: \\n{}'.format(transient_available_sites)\n",
    "    raise Exception(msg)\n",
    "else:\n",
    "    print('\\nSelected site: {}'.format(site))\n",
    "    for key, value in sites[site].items():\n",
    "        print('   '+ key, ' : ', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_prep_b'></a>\n",
    "## Prep B. Data Staging\n",
    "\n",
    "In this initial processing step, all the necessary Level-2 unwrapped interferogram products over specified time range and 12 days time intervels are gathered by the ARIA-tool and organized by Mintpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option to control the use of pre-staged data; [False/True]\n",
    "Use_Staged_Data = True\n",
    "     \n",
    "######### DO NOT CHANGE LINES BELOW ########\n",
    "\n",
    "if Use_Staged_Data:\n",
    "     # Check if a stage file from S3 already exist, if not try and download it\n",
    "    if len(glob.glob('MintPy/inputs/*.h5')) == 0:\n",
    "        os.chdir(work_dir.parents[0])\n",
    "        zip_name = site + '.zip'\n",
    "        print('Downloading:',  Path.cwd()/zip_name)\n",
    "        if not os.path.isfile(zip_name):\n",
    "            try:\n",
    "                command = \"aws s3 cp --no-sign-request s3://asf-jupyter-data-west/NISAR_SE/\" + site + '.zip ' + zip_name\n",
    "                subprocess.run(command, shell=True, check = True)\n",
    "            except:\n",
    "                command = 'wget --no-check-certificate --no-proxy \"http://asf-jupyter-data-west.s3.amazonaws.com/NISAR_SE/' + site + '.zip\" -q --show-progress'\n",
    "                print('\\nDownloading staged data ... ')\n",
    "                subprocess.run(command, stdout=None, stderr=subprocess.PIPE, shell=True)\n",
    "            finally:\n",
    "                if (work_dir.parents[0]/zip_name).is_file():\n",
    "                    print('Finished downloading!')\n",
    "                else:\n",
    "                    raise RuntimeError('Failed downloading Staged data!!! Install aws or wget to proceed')\n",
    "                \n",
    "        command = 'unzip ' + str(work_dir.parents[0]/zip_name) +'; rm ' + str(work_dir.parents[0]/zip_name)\n",
    "        process = subprocess.run(command, shell=True)\n",
    "        os.chdir(work_dir)\n",
    "    if not os.path.exists(work_dir):\n",
    "        raise Exception(\"Staged data for site {} not sucessfully generated. Please delete site-name folder {} and rerun this cell\".format(site, work_dir))\n",
    "    print('Finish preparing staged data for MintPy!!')\n",
    "    \n",
    "else:\n",
    "    ##################### 1. Download (Aria) Interferograms from ASF ################\n",
    "    print('NEEDED To Download ARIA GUNWs: \\n Link to create account : https://urs.earthdata.nasa.gov/')\n",
    "    earthdata_user = input('Please type your Earthdata username:')\n",
    "    earthdata_password = input('Please type your Earthdata password:')\n",
    "    print('NEEDED To Download DEMs: \\n Link to create account : https://portal.opentopography.org/login')\n",
    "    print('API key location: My Account > myOpenTopo Authorizations and API Key > Request API key')\n",
    "    opentopography_api_key = input('Please type your OpenTopo API key:')\n",
    "\n",
    "    ######################## USE ARIA-TOOLS TO DOWNLOAD GUNW ########################\n",
    "    '''\n",
    "    REFERENCE: https://github.com/aria-tools/ARIA-tools\n",
    "    '''\n",
    "    aria_download = '''ariaDownload.py -b {bbox} -u {user} -p {password} -s {start}  -e {end} -t {track} -o Count'''\n",
    "\n",
    "    ###############################################################################\n",
    "    print('CalVal site {}'.format(site))\n",
    "    print('  Searching for available GUNW products:\\n')\n",
    "\n",
    "    command = aria_download.format(bbox = sites[site]['download_region'],\n",
    "                                   start = sites[site]['download_start_date'],\n",
    "                                   end = sites[site]['download_end_date'],\n",
    "                                   track = sites[site]['sentinel_track'],\n",
    "                                   user = earthdata_user,\n",
    "                                   password = earthdata_password)\n",
    "      \n",
    "    process = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text = True, shell = True)\n",
    "    print(process.stdout)\n",
    "\n",
    "    ############## Download GUNW ##################\n",
    "    print(\"Start downloading GUNW files ...\")\n",
    "    process = subprocess.run(command.split(' -o')[0], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "    # Missing progressbar\n",
    "    print(\"Downloaded {} GUNW files in: {}\\n\".format(len([(x) for x in os.listdir(gunw_dir) if x.endswith('.nc')]), gunw_dir))\n",
    "\n",
    "    ############## DO little CLEANING ###########\n",
    "    data_to_clean = [\"avg_rates.csv\", \"ASFDataDload0.py\", \"AvgDlSpeed.png\", \"error.log\"]\n",
    "\n",
    "    for i, file in enumerate(data_to_clean):\n",
    "        print('Cleaning unnecessary data {} in {}'.format(file, gunw_dir))\n",
    "        (gunw_dir/file).unlink(missing_ok=True)\n",
    "\n",
    "    #Delete error log file from workdir\n",
    "    print('Cleaning unnecessary data error.log in {}'.format(work_dir))\n",
    "    (work_dir/\"error.log\").unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gen_ifg'></a>\n",
    "# Generate Interferogram Stack\n",
    "\n",
    "We use the open-source ARIA-tools package to download processed L2 interferograms over selected cal/val regions from the Alaska Satellite Facility archive and to stitch/crop the frame-based NISAR GUNW products. ARIA-tools uses a phase-minimization approach in the product overlap region to stitch the unwrapped and ionospheric phase, a mosaicing approach for coherence and amplitude, and extracts the geometric information from the 3D data cubes through a mosaicking of the 3D datacubes and subsequent intersection with a DEM. ARIA has been used to pre-process NISAR beta products derived from Sentinel-1 which have revealed interseismic deformation and creep along the San Andreas Fault system, along with subsidence, landsliding, and other signals. \n",
    "\n",
    "We use MintPy to load interferograms and auxiliary metadata (i.e. look angle, longitude and latitude step size) into HDF5 format which could be easily loaded into Python for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Crop Interferograms\n",
    "<a id='secular_crop_ifg'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Crop Interferograms to Analysis Region\n",
    "if not Use_Staged_Data:\n",
    "    ###########################################################################################################\n",
    "    # Set up ARIA product and mask data with GSHHS water mask:\n",
    "    '''\n",
    "    REQUIRED: Acquire API key to access/download DEMs\n",
    "\n",
    "    Follow instructions listed here to generate and access API key through OpenTopography:\n",
    "    https://opentopography.org/blog/introducing-api-keys-access-opentopography-global-datasets.\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(work_dir/'stack'):\n",
    "        os.system('echo \"{api_key}\" > ~/.topoapi; chmod 600 ~/.topoapi'.format(api_key = str(opentopography_api_key)))\n",
    "        print('Preparing GUNWs for MintPY....')\n",
    "        if sites[site]['maskWater']:\n",
    "            command = 'ariaTSsetup.py -f \"products/*.nc\" -b ' + sites[site]['analysis_region'] + ' --mask Download  --croptounion' # slow\n",
    "        else: # skip slow mask download when we don't need to mask water\n",
    "            command = 'ariaTSsetup.py -f \"products/*.nc\" -b ' + sites[site]['analysis_region'] + ' --croptounion'\n",
    "\n",
    "        ################################## CROP & PREPARE STACK ###################################################\n",
    "        result = subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "    print('Finish preparing GUNWs for MintPy!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up MintPy Configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_content = '''\n",
    "mintpy.load.processor = aria\n",
    "mintpy.load.unwFile = ../stack/unwrapStack.vrt\n",
    "mintpy.load.corFile = ../stack/cohStack.vrt\n",
    "mintpy.load.connCompFile = ../stack/connCompStack.vrt\n",
    "mintpy.load.demFile = ../DEM/SRTM_3arcsec.dem\n",
    "mintpy.load.incAngleFile = ../incidenceAngle/*.vrt\n",
    "mintpy.load.azAngleFile = ../azimuthAngle/*.vrt\n",
    "mintpy.load.waterMaskFile = ../mask/watermask.msk\n",
    "mintpy.reference.lalo = auto\n",
    "mintpy.topographicResidual.pixelwiseGeometry = no\n",
    "mintpy.troposphericDelay.method = no\n",
    "mintpy.topographicResidual = no\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = mintpy_dir/(site+'.cfg')\n",
    "config_file.write_text(config_file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data reorganization by Mintpy. The output of this step is an \"inputs\" directory containing two HDF5 files:\n",
    "- ifgramStack.h5: This file contains 6 dataset cubes (e.g. unwrapped phase, coherence, connected components etc.) and multiple metadata\n",
    "- geometryGeo.h5: This file contains geometrical datasets (e.g., incidence/azimuth angle, masks, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(mintpy_dir)\n",
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep load_data'\n",
    "os.system(command)\n",
    "os.chdir(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_file = mintpy_dir/'inputs/ifgramStack.h5'\n",
    "geom_file = mintpy_dir/'inputs/geometryGeo.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the interferogram has a resolution lower than 100 m, we need multi-look the interferogram phase values before calculating the empirical semivarigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the date of interferograms into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ifgs_date = readfile.read(ifgs_file,datasetName='date')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ifgs_date = np.empty_like(ifgs_date,dtype=dt)\n",
    "for i in range(ifgs_date.shape[0]):\n",
    "    start_date = ifgs_date[i,0].decode()\n",
    "    end_date = ifgs_date[i,1].decode()\n",
    "    start_date = dt.strptime(start_date, \"%Y%m%d\")\n",
    "    end_date = dt.strptime(end_date, \"%Y%m%d\")\n",
    "    _ifgs_date[i] = [start_date,end_date]\n",
    "    \n",
    "ifgs_date = _ifgs_date\n",
    "del _ifgs_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove interferograms with time interval other than 12 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_row_index = []\n",
    "for i in range(ifgs_date.shape[0]):\n",
    "    time_interval = (ifgs_date[i,1]-ifgs_date[i,0]).days\n",
    "    if time_interval != 12:\n",
    "        del_row_index.append(i)\n",
    "while i<ifgs_date.shape[0]-1:\n",
    "    if ifgs_date[i,1]==ifgs_date[i+1,0]:\n",
    "        del_row_index.append(i+1)\n",
    "        i = i+2\n",
    "    else:\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_date = np.delete(ifgs_date,del_row_index,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify independent interferograms (i.e., selected inteferograms do NOT share common dates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_row_index = []\n",
    "i = 0\n",
    "while i<ifgs_date.shape[0]-1:\n",
    "    if ifgs_date[i,1]==ifgs_date[i+1,0]:\n",
    "        del_row_index.append(i+1)\n",
    "        i = i+2\n",
    "    else:\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_date = np.delete(ifgs_date,del_row_index,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the phase and coherence of selected interferograms, geometrical datasets, and attribution of them are loaded into numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrapPhaseName = ['unwrapPhase-'+i[0].strftime('%Y%m%d')+'_'+i[1].strftime('%Y%m%d') for i in ifgs_date]\n",
    "coherenceName = ['coherence-'+i[0].strftime('%Y%m%d')+'_'+i[1].strftime('%Y%m%d') for i in ifgs_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_unw,atr = readfile.read(ifgs_file,datasetName=unwrapPhaseName)\n",
    "insar_displacement = -ifgs_unw*float(atr['WAVELENGTH'])/(4*np.pi)*1000 # unit in mm\n",
    "\n",
    "insar_coherence = readfile.read(ifgs_file,datasetName=coherenceName)[0]\n",
    "del ifgs_unw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change default missing phase values in interferograms from 0.0 to `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_displacement[insar_displacement==0.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional interferograms correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Phase distortions related to solid earth and ocean tidal effects as well as those due to temporal variations in the vertical stratification of the atmosphere can be mitigated using the approaches described below. At this point, it is expected that these corrections will not be needed to validate the mission requirements, but they may be used to produce the highest quality data products. Typically, these are applied to the estimated time series product rather than to the individual interferograms, since they are a function of the time of each radar acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solid Earth Tide Correction\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tropospheric Delay Correction\n",
    "Optional atmospheric correction utilizes the PyAPS (Jolivet et al., 2011, Jolivet and Agram, 2012) module within GIAnT (or eventually a merged replacement for GIAnT and MintPy). PyAPS is well documented, maintained and can be freely downloaded. PyAPS is included in GIAnT distribution). PyAPS currently includes support for ECMWF’s ERA-Interim, NOAA’s NARR and NASA’s MERRA weather models. A final selection of atmospheric models to be used for operational NISAR processing will be done during Phase C.\n",
    "\n",
    "[T]ropospheric delay maps are produced from atmospheric data provided by Global Atmospheric Models. This method aims to correct differential atmospheric delay correlated with the topography in interferometric phase measurements. Global Atmospheric Models (hereafter GAMs)... provide estimates of the air temperature, the atmospheric pressure and the humidity as a function of elevation on a coarse resolution latitude/longitude grid. In PyAPS, we use this 3D distribution of atmospheric variables to determine the atmospheric phase delay on each pixel of each interferogram.\n",
    "\n",
    "The absolute atmospheric delay is computed at each SAR acquisition date. For a pixel a_i at an elevation z at acquisition date i, the four surrounding grid points are selected and the delays for their respective elevations are computed. The resulting delay at the pixel a_i is then the bilinear interpolation between the delays at the four grid points. Finally, we combine the absolute delay maps of the InSAR partner images to produce the differential delay maps used to correct the interferograms.\n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topographic Residual Correction \n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Phase deramping is not appplied here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the solid earth tide correction for interferogram is applied, it should also be applied for GNSS observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary summary: we have load all data we need for processing:\n",
    "- `atr`: metadata, including incident angle, longitude and latitude step width, etc;\n",
    "- `insar_displacement`: LOS measurement from InSAR;\n",
    "- `insar_coherence`: coherence value of the interferograms:\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prevent reruning the above preparing again, we save the data into disk. They can be loaded easily next time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(work_dir/'base.pkl','wb') as f:\n",
    "    pickle.dump((atr,insar_displacement,insar_coherence,ifgs_date),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(work_dir/'base.pkl','rb') as f:\n",
    "    atr,insar_displacement,insar_coherence,ifgs_date = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make GNSS LOS Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Collocated GNSS Stations\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get space and time range for searching GNSS station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length, width = int(atr['LENGTH']), int(atr['WIDTH'])\n",
    "lat_step = float(atr['Y_STEP'])\n",
    "lon_step = float(atr['X_STEP'])\n",
    "N = float(atr['Y_FIRST'])\n",
    "W = float(atr['X_FIRST'])\n",
    "S = N+lat_step*(length-1)\n",
    "E = W+lon_step*(width-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_gnss = ifgs_date[0,0]\n",
    "end_date_gnss = ifgs_date[-1,-1]\n",
    "\n",
    "inc_angle = int(float(atr.get('incidenceAngle', None)))\n",
    "az_angle = int(float(atr.get('azimuthAngle', None))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for collocated GNSS stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(mintpy_dir)\n",
    "site_names, site_lats, site_lons = gps.search_gps(SNWE=(S,N,W,E),\n",
    "                                                  start_date=start_date_gnss.strftime('%Y%m%d'),\n",
    "                                                  end_date=end_date_gnss.strftime('%Y%m%d'))\n",
    "os.chdir(work_dir)\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make GNSS LOS Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the 3-D GNSS observations are projected into LOS direction. The InSAR observations are averaged 3 by 3 near the station positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** the number of pixels used in calculating the averaged phase values at the GPS location depends on the resolution of input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get daily position solutions for GNSS stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#os.chdir(mint_dir)\n",
    "displacement = {}\n",
    "gnss_time_series = {}\n",
    "gnss_time_series_std = {}\n",
    "bad_stn = {}  #stations to toss\n",
    "pixel_radius = 1   #number of InSAR pixels to average for comparison with GNSS\n",
    "\n",
    "for counter,stn in enumerate(site_names):\n",
    "    gps_obj = gps.GPS(site = stn, data_dir = str(mintpy_dir/'GPS'))\n",
    "    gps_obj.open()\n",
    "        \n",
    "    # count number of dates in time range\n",
    "    gps_obj.read_displacement()\n",
    "    dates = gps_obj.dates\n",
    "    for i in range(insar_displacement.shape[0]):\n",
    "        start_date = ifgs_date[i,0]\n",
    "        end_date = ifgs_date[i,-1]\n",
    "        \n",
    "        range_days = (end_date - start_date).days\n",
    "        gnss_count = np.histogram(dates, bins=[start_date,end_date])\n",
    "        gnss_count = int(gnss_count[0])\n",
    "        #print(gnss_count)\n",
    "\n",
    "        # select GNSS stations based on data completeness, here we hope to select stations with data frequency of 1 day and no interruption\n",
    "        if range_days == gnss_count-1:\n",
    "        #if start_date in dates and end_date in dates:\n",
    "            disp_gnss_time_series,disp_gnss_time_series_std,site_latlon = gps_obj.read_gps_los_displacement(str(mintpy_dir/'inputs/geometryGeo.h5'),\n",
    "                                                                                                            start_date=start_date.strftime('%Y%m%d'),\n",
    "                                                                                                            end_date=end_date.strftime('%Y%m%d'))[1:4]\n",
    "            x_value = round((site_latlon[1] - W)/lon_step)\n",
    "            y_value = round((site_latlon[0] - N)/lat_step)\n",
    "\n",
    "            #displacement from insar observation in the gnss station, averaged\n",
    "            #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "            disp_insar = insar_displacement[i,\n",
    "                                            y_value-pixel_radius:y_value+pixel_radius, \n",
    "                                            x_value-pixel_radius:x_value+pixel_radius]\n",
    "            if np.isfinite(disp_insar).sum() == 0:\n",
    "                break\n",
    "            disp_insar = np.nanmean(disp_insar)\n",
    "\n",
    "            disp_gnss_time_series = disp_gnss_time_series*1000 # convert unit from meter to mm\n",
    "            disp_gnss_time_series_std = disp_gnss_time_series_std*1000\n",
    "            gnss_time_series[(i,stn)] = disp_gnss_time_series\n",
    "            gnss_time_series_std[(i,stn)] = disp_gnss_time_series_std\n",
    "            displacement[(i,stn)] = list(site_latlon)\n",
    "            disp_gnss = disp_gnss_time_series[-1] - disp_gnss_time_series[0]\n",
    "\n",
    "            displacement[(i,stn)].append(disp_gnss)\n",
    "            displacement[(i,stn)].append(disp_insar)\n",
    "        else:\n",
    "            try:\n",
    "                bad_stn[i].append(stn)\n",
    "            except:\n",
    "                bad_stn[i] = [stn]\n",
    "#os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some data structure transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series = dict(sorted(gnss_time_series.items()))\n",
    "gnss_time_series_std = dict(sorted(gnss_time_series_std.items()))\n",
    "displacement = dict(sorted(displacement.items()))\n",
    "bad_stn = dict(sorted(bad_stn.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series = pd.DataFrame.from_dict(gnss_time_series)\n",
    "gnss_time_series_std = pd.DataFrame.from_dict(gnss_time_series_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement = pd.DataFrame.from_dict(displacement,orient='index',\n",
    "                                      columns=['lat','lon','gnss_disp','insar_disp'])\n",
    "displacement.index = pd.MultiIndex.from_tuples(displacement.index,names=['ifg index','station'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are less than 3 GNSS stations, don't conduct comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_index = []\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    if len(displacement.loc[i]) < 3:\n",
    "        drop_index.append(i)\n",
    "displacement=displacement.drop(drop_index)\n",
    "# ifgs_date after drop for approach 1\n",
    "ifgs_date_ap1=np.delete(ifgs_date,drop_index,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "- A more general critterion is needed for GNSS station selection. Here the stations with uninterrupted data are selected while, in Secular Requirement Validation, stations are selected by data completeness and standard variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make GNSS and InSAR Relative Displacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select reference site randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference GNSS stations to GNSS reference site\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    gps_ref_site_name = random.choice(displacement.loc[i].index.unique())\n",
    "    displacement.loc[i,'gnss_disp'] = displacement.loc[i,'gnss_disp'].values - displacement.loc[(i,gps_ref_site_name),'gnss_disp']\n",
    "    displacement.loc[i,'insar_disp'] = displacement.loc[i,'insar_disp'].values - displacement.loc[(i,gps_ref_site_name),'insar_disp']\n",
    "    ref_x_value = round((displacement.loc[(i,gps_ref_site_name),'lon'] - W)/lon_step)\n",
    "    ref_y_value = round((displacement.loc[(i,gps_ref_site_name),'lat'] - N)/lat_step)\n",
    "\n",
    "    ref_disp_insar = insar_displacement[i,\n",
    "                                        ref_y_value-pixel_radius:ref_y_value+1+pixel_radius, \n",
    "                                        ref_x_value-pixel_radius:ref_x_value+1+pixel_radius]\n",
    "    ref_disp_insar = np.nanmean(ref_disp_insar)\n",
    "    insar_displacement[i] -= ref_disp_insar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the data to be validated next. The measurements are in milimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot GNSS stations on InSAR velocity field\n",
    "cmap = copy.copy(plt.get_cmap('RdBu'))\n",
    "#cmap.set_bad(color='black')\n",
    "vmin, vmax = np.nanmin(insar_displacement), np.nanmax(insar_displacement)\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    fig, ax = plt.subplots()\n",
    "    img1 = ax.imshow(insar_displacement[i], cmap=cmap,vmin=vmin,vmax=vmax, interpolation='nearest', extent=(W, E, S, N))\n",
    "    ax.set_title(ifgs_date[i,0].strftime('%Y%m%d')+'-'+ifgs_date[i,1].strftime('%Y%m%d'))\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('LOS displacement [mm]')\n",
    "\n",
    "    for stn in displacement.loc[i].index:\n",
    "        lon,lat = displacement.loc[(i,stn),'lon'],displacement.loc[(i,stn),'lat']\n",
    "        color = cmap((displacement.loc[(i,stn),'gnss_disp']-vmin)/(vmax-vmin))\n",
    "        ax.scatter(lon,lat,s=8**2,color=color,edgecolors='k')\n",
    "        ax.annotate(stn,(lon,lat),color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NISAR Validation: GNSS-InSAR Direct Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_disp = {}\n",
    "gnss_disp = {}\n",
    "ddiff_dist = {}\n",
    "ddiff_disp = {}\n",
    "abs_ddiff_disp = {}\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    displacement_i = displacement.loc[i]\n",
    "    insar_disp_i = []\n",
    "    gnss_disp_i = []\n",
    "    ddiff_dist_i = []\n",
    "    ddiff_disp_i = []\n",
    "\n",
    "    for sta1 in displacement_i.index:\n",
    "        for sta2 in displacement_i.index:\n",
    "            if sta2 == sta1:\n",
    "                break\n",
    "            insar_disp_i.append(displacement_i.loc[sta1,'insar_disp']-displacement_i.loc[sta2,'insar_disp'])\n",
    "            gnss_disp_i.append(displacement_i.loc[sta1,'gnss_disp']-displacement_i.loc[sta2,'gnss_disp'])\n",
    "            ddiff_disp_i.append(gnss_disp_i[-1]-insar_disp_i[-1])\n",
    "            g = pyproj.Geod(ellps=\"WGS84\")\n",
    "            _,_,distance = g.inv(displacement_i.loc[sta1,'lon'],displacement_i.loc[sta1,'lat'],\n",
    "                                 displacement_i.loc[sta2,'lon'],displacement_i.loc[sta2,'lat'])\n",
    "            distance = distance/1000 # convert unit from m to km\n",
    "            ddiff_dist_i.append(distance)\n",
    "    insar_disp[i]=np.array(insar_disp_i)\n",
    "    gnss_disp[i]=np.array(gnss_disp_i)\n",
    "    ddiff_dist[i]=np.array(ddiff_dist_i)\n",
    "    ddiff_disp[i]=np.array(ddiff_disp_i)\n",
    "    abs_ddiff_disp[i]=abs(np.array(ddiff_disp_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    plt.figure(figsize=(11,7))\n",
    "    disp_range = (min([*insar_disp[i],*gnss_disp[i]]),max([*insar_disp[i],*gnss_disp[i]]))\n",
    "    plt.hist(insar_disp[i],bins=100,range=disp_range,color = \"green\",label='D_InSAR')\n",
    "    plt.hist(gnss_disp[i],bins=100,range=disp_range,color=\"orange\",label='D_GNSS', alpha=0.5)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Displacements \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')} \\n Number of station pairs used: {len(insar_disp[i])}\")\n",
    "    plt.xlabel('LOS Displacement (mm)')\n",
    "    plt.ylabel('Number of Station Pairs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Displacement Residuals Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    plt.figure(figsize=(11,7))\n",
    "    plt.hist(ddiff_disp[i],bins = 100, color=\"darkblue\",linewidth=1,label='D_gnss - D_InSAR')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Residuals \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')} \\n Number of stations pairs used: {len(ddiff_disp[i])}\")\n",
    "    plt.xlabel('Displacement Residual (mm)')\n",
    "    plt.ylabel('N Stations')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Absolute Displacement Residuals As a Function of Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    dist_th = np.linspace(min(ddiff_dist[i]),max(ddiff_dist[i]),100)\n",
    "    acpt_error = 3*(1+np.sqrt(dist_th))\n",
    "    plt.figure(figsize=(11,7))\n",
    "    plt.scatter(ddiff_dist[i],abs_ddiff_disp[i],s=1)\n",
    "    plt.plot(dist_th, acpt_error, 'r')\n",
    "    plt.xlabel(\"Distance (km)\")\n",
    "    plt.ylabel(\"Amplitude of Displacement Residuals (mm)\")\n",
    "    plt.title(f\"Residuals \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')} \\n Number of stations pairs used: {len(ddiff_dist[i])}\")\n",
    "    plt.legend([\"Mission Reqiurement\",\"Measuement\"])\n",
    "    #plt.xlim(0,5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got all needed data for approach 1:\n",
    "- `ddiff_dist_ap1`: distance of GNSS pairs,\n",
    "- `abs_ddiff_disp_ap1`: absolute value of measurement redisuals,\n",
    "- `ifgs_date_ap1`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prevent reruning the above preparing again, we save the data into disk. They can be loaded easily next time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddiff_dist_ap1 = list(ddiff_dist.values())\n",
    "abs_ddiff_disp_ap1 = list(abs_ddiff_disp.values())\n",
    "with open(work_dir/'approach1.pkl','wb') as f:\n",
    "    pickle.dump((ddiff_dist_ap1,abs_ddiff_disp_ap1,ifgs_date_ap1),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(work_dir/'approach1.pkl','rb') as f:\n",
    "    ddiff_dist_ap1, abs_ddiff_disp_ap1, ifgs_date_ap1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ifgs = len(ddiff_dist_ap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "bins = np.linspace(0.1,50.0,num=n_bins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_all = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points for each ifgs and bins\n",
    "n_pass = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points pass\n",
    "#ratio = np.empty([n_ifgs,n_bins+1]) # ratio\n",
    "# the final column is the ratio as a whole\n",
    "for i in range(n_ifgs):\n",
    "    inds = np.digitize(ddiff_dist_ap1[i],bins)\n",
    "    for j in range(1,n_bins+1):\n",
    "        rqmt = 3*(1+np.sqrt(ddiff_dist_ap1[i][inds==j]))# mission requirement for i-th ifgs and j-th bins\n",
    "        rem = abs_ddiff_disp_ap1[i][inds==j] # relative measurement\n",
    "        assert len(rqmt) == len(rem)\n",
    "        n_all[i,j-1] = len(rem)\n",
    "        n_pass[i,j-1] = np.count_nonzero(rem<rqmt)\n",
    "    n_all[i,-1] = np.sum(n_all[i,0:-2])\n",
    "    n_pass[i,-1] = np.sum(n_pass[i,0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(x:bool):\n",
    "    if x==True:\n",
    "        return 'true '\n",
    "    elif x==False:\n",
    "        return 'false '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = n_pass/n_all\n",
    "thresthod = 0.683 \n",
    "#The assumed nature of Gaussian distribution gives a probability of 0.683 of being within one standard deviation.\n",
    "success_or_fail = ratio>thresthod\n",
    "success_or_fail_str = [list(map(to_str, x)) for x in success_or_fail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for i in range(n_bins):\n",
    "    columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "columns.append('total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for i in range(len(ifgs_date_ap1)):\n",
    "    index.append(ifgs_date_ap1[i,0].strftime('%Y%m%d')+'-'+ifgs_date_ap1[i,1].strftime('%Y%m%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_all_pd = pd.DataFrame(n_all,columns=columns,index=index)\n",
    "n_pass_pd = pd.DataFrame(n_pass,columns=columns,index=index)\n",
    "ratio_pd = pd.DataFrame(ratio,columns=columns,index=index)\n",
    "success_or_fail_pd = pd.DataFrame(success_or_fail_str,columns=columns,index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_all_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points that below the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_pass_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = ratio_pd.style\n",
    "s.set_table_styles([  # create internal CSS classes\n",
    "    {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "    {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "], overwrite=False)\n",
    "s.set_td_classes(success_or_fail_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pd.iloc[0]['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = np.count_nonzero(ratio_pd['total']>thresthod)/n_ifgs\n",
    "print(f\"Percentage of interferograms passes the requirement: {percentage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NISAR Validation: Noise Level Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this validation (Approach #2), we evaluate the estimated secular deformation rate (Requirements 658) or co-seismic displacement (Requirement 660) from time series processing or the individual unwrapped interferogram (Requirement 663) over selected cal/val areas with negligible deformation. Any estimated deformation should thus be treated as noise and our goal is to evaluate the significance of this noise. In general, noise in the modeled displacement or the unwrapped interferogram is anisotropic, but here we neglect this anisotropy. Also, we assume the noise is stationary.\n",
    "\n",
    "We first randomly sample measurements and pair up sampled pixel measurements. For each pixel-pair, the difference of their measurement becomes:\n",
    "$$d\\left(r\\right)=|(f\\left(x\\right)-f\\left(x-r\\right))|$$\n",
    "Estimates of $d(r)$ from all pairs are binned according to the distance r. In each bin, $d(r)$ is assumed to be a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Now we simply assume there is no deformation in this study area and time interval. But in fact, it is hard to find a enough large area without any deformation. An more realistic solution is to apply a mask to mask out deformed regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Pixels with Low Coherence (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ifgs = insar_displacement.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insar_displacement[insar_coherence <0.6] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coherence and InSAR measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('gray')\n",
    "\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.imshow(insar_coherence[i],cmap=cmap, interpolation='nearest',extent=(W, E, S, N))\n",
    "    ax.set_title(f\"Coherence \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('coherence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('RdBu')\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.imshow(insar_displacement[i], cmap=cmap, interpolation='nearest', extent=(W, E, S, N))\n",
    "    ax.set_title(f\"Interferogram \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('LOS displacement [mm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly Sample Pixels and Pair Them Up with Option to Remove Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coordinate for every pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X0,Y0 = load_geo(atr)\n",
    "X0_2d,Y0_2d = np.meshgrid(X0,Y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each interferogram, randomly selected pixels need to be paired up. In order to keep measurements independent, different pixel pairs can not share same pixel. This is achieved by pairing up in sequence, i.e., pairing up pixel number 1 and number 2, 3 and 4..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = []; rel_measure = []\n",
    "for i in range(n_ifgs):\n",
    "    dist_i, rel_measure_i = samp_pair(X0_2d,Y0_2d,insar_displacement[i],num_samples=1000000)\n",
    "    dist.append(dist_i)\n",
    "    rel_measure.append(rel_measure_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the statistical property of selected pixel pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(dist[i], bins=100)\n",
    "    ax.set_title(f\"Histogram of distance \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_xlabel(r'Distance ($km$)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlim(0,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(rel_measure[i], bins=100)\n",
    "    ax.set_title(f\"Histogram of Relative Measurement \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_xlabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist_th = np.linspace(0,50,100)\n",
    "rqmt = 3*(1+np.sqrt(dist_th))\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    ax.plot(dist_th, rqmt, 'r')\n",
    "    ax.scatter(dist[i], rel_measure[i], s=1, alpha=0.25)\n",
    "    ax.set_title(f\"Comparation between Relative Measurement and Requirement Curve \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_ylabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_xlabel('Distance (km)')\n",
    "    ax.set_xlim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got data used of approach 2:\n",
    "- `dist`: distance of pixel pairs,\n",
    "- `rel_measure`: relative measurement of pixel pairs,\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram.\n",
    "\n",
    "In order to prevent reruning the above preparing again, we save the data into disk. They can be loaded easily next time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(work_dir/'approach2.pkl','wb') as f:\n",
    "    pickle.dump((dist,rel_measure,ifgs_date),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(work_dir/'approach2.pkl','rb') as f:\n",
    "    dist,rel_measure, ifgs_date = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ifgs = len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "bins = np.linspace(0.1,50.0,num=n_bins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_all = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points for each ifgs and bins\n",
    "n_pass = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points pass\n",
    "#ratio = np.empty([n_ifgs,n_bins+1]) # ratio\n",
    "# the final column is the ratio as a whole\n",
    "for i in range(n_ifgs):\n",
    "    inds = np.digitize(dist[i],bins)\n",
    "    for j in range(1,n_bins+1):\n",
    "        rqmt = 3*(1+np.sqrt(dist[i][inds==j]))# mission requirement for i-th ifgs and j-th bins\n",
    "        rem = rel_measure[i][inds==j] # relative measurement\n",
    "        assert len(rqmt) == len(rem)\n",
    "        n_all[i,j-1] = len(rem)\n",
    "        n_pass[i,j-1] = np.count_nonzero(rem<rqmt)\n",
    "    n_all[i,-1] = np.sum(n_all[i,0:-2])\n",
    "    n_pass[i,-1] = np.sum(n_pass[i,0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(x:bool):\n",
    "    if x==True:\n",
    "        return 'true '\n",
    "    elif x==False:\n",
    "        return 'false '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = n_pass/n_all\n",
    "mean_ratio = np.array([np.mean(ratio[:,:-1],axis=1)])\n",
    "ratio = np.hstack((ratio,mean_ratio.T))\n",
    "thresthod = 0.683\n",
    "#The assumed nature of Gaussian distribution gives a probability of 0.683 of being within one standard deviation.\n",
    "success_or_fail = ratio>thresthod\n",
    "success_or_fail_str = [list(map(to_str, x)) for x in success_or_fail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for i in range(n_bins):\n",
    "    columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "columns.append('total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for i in range(len(ifgs_date)):\n",
    "    index.append(ifgs_date[i,0].strftime('%Y%m%d')+'-'+ifgs_date[i,1].strftime('%Y%m%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_all_pd = pd.DataFrame(n_all,columns=columns,index=index)\n",
    "n_pass_pd = pd.DataFrame(n_pass,columns=columns,index=index)\n",
    "ratio_pd = pd.DataFrame(ratio,columns=columns+['mean'],index=index)\n",
    "success_or_fail_pd = pd.DataFrame(success_or_fail_str,columns=columns+['mean'],index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_all_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points that below the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_pass_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = ratio_pd.style\n",
    "s.set_table_styles([  # create internal CSS classes\n",
    "    {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "    {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "], overwrite=False)\n",
    "s.set_td_classes(success_or_fail_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with percentage of total passed pairs, the mean value of percentage of passed pairs in all bin is a better indicator since it gives all bins same weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = np.count_nonzero(ratio_pd['mean']>thresthod)/n_ifgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of interferograms passes the requirement (70%): {percentage}\")\n",
    "if percentage >= 0.70:\n",
    "    print('The interferogram stack passes the requirement')\n",
    "else:\n",
    "    print('The interferogram stack fails the requirement.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: GPS Position Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(gnss_time_series)):\n",
    "    print(ifgs_date[i,0].strftime('%Y%m%d')+'-'+ifgs_date[i,1].strftime('%Y%m%d'))\n",
    "    for stn in gnss_time_series[i].columns:\n",
    "        series = gnss_time_series[i,str(stn)]\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.title(f\"station name: {stn}\")\n",
    "        plt.scatter(pd.date_range(start=ifgs_date[i,0], end=ifgs_date[i,1]),series)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Displacement (mm)')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NISAR_SE",
   "language": "python",
   "name": "conda-env-NISAR_SE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
