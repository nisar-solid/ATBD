{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Workflow to Validate NISAR L2 Transient Displacement Requirements\n",
    "\n",
    "**Original code authored by:** NISAR Science Team Members and Affiliates  \n",
    "\n",
    "*May 13, 2022*\n",
    "\n",
    "*NISAR Solid Earth Team*\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Both the initial setup (<b>Prep A</b> section) and download of the data (<b>Prep B</b> section) should be run at the start of the notebook. And all subsequent sections NEED to be run in order.\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CalVal Site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose a site and track direction\n",
    "site='MojaveD173' \n",
    "\n",
    "# What dataset are you processing?\n",
    "dataset = 'ARIA_S1' # For Sentinel-1 testing with aria-tools\n",
    "\n",
    "# The date and version of this Cal/Val run\n",
    "today = '20240429'\n",
    "version = '1'\n",
    "\n",
    "# Define your directory structure - you won't need to change this line\n",
    "start_directory = '/scratch/nisar-st-calval-solidearth' \n",
    "\n",
    "# The file where you keep your customized list of sites.\n",
    "custom_sites = '/home/jovyan/my_sites.txt'\n",
    "\n",
    "# Please enter a name or username that will determine where your outputs are stored\n",
    "import os\n",
    "if os.path.exists('/home/jovyan/me.txt'): # if OpenTopo API key already installed\n",
    "    with open('/home/jovyan/me.txt') as m:\n",
    "        you = m.readline().strip()\n",
    "    print('You are', you)\n",
    "    print('Using this as the name of the directory where your outputs will be stored.')\n",
    "    print('Directory structure: start_directory / dataset/ requirement / site / you / today / version ')\n",
    "\n",
    "else:\n",
    "    print('We need a name or username (determines where your outputs will be stored)')\n",
    "    print('Directory structure: start_directory / dataset/ requirement / site / you / today / version ')\n",
    "\n",
    "    you = input('Please type your name:')\n",
    "    with open ('/home/jovyan/me.txt', 'w') as m: \n",
    "        m.write(you)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of Contents:\n",
    "<a id='secular_TOC'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "[**Prep A. Environment Setup**](#transient_prep_a)\n",
    "\n",
    "[**Prep B. Data Staging**](#transient_prep_b)\n",
    "\n",
    "[**1. Generate Interferogram Stack**](#transient_gen_ifg)\n",
    "- [1.1.  Crop Interferograms](#transient_crop_ifg)\n",
    "\n",
    "[**2. Optional Corrections**](#transient_opt_correction)\n",
    "- [2.1. Solid Earth Tides Correction](#transient_solid_earth)\n",
    "- [2.2. Tropospheric Delay Correction](#transient_tropo_corr)\n",
    "- [2.3. Topographic Residual Correction ](#transient_tropo_res_corr)\n",
    "\n",
    "[**3. Make GNSS LOS Measurements**](#transient_gnss_los)\n",
    "- [3.1. Find Collocated GNSS Stations](#transient_co_gnss)  \n",
    "- [3.2. Make GNSS LOS Measurements](#transient_gnss_los2) \n",
    "- [3.3. Make GNSS and InSAR Relative Displacements](#transient_gnss_insar)\n",
    "\n",
    "[**4. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison**](#transient_validation1)\n",
    "- [4.1. Pair up GNSS stations and make measurement residuals](#transient_pair1)\n",
    "- [4.2. Validate the requirement based on binned measurement residuals](#transient_bin1)\n",
    "- [4.3. Result visulazation](#transient_result1)\n",
    "- [4.3. Conclusion](#transient_conclusion1)\n",
    "\n",
    "[**5. NISAR Validation Approach 2: Noise Level Validation**](#transient_validation2)\n",
    "- [5.1. Randomly sample pixels and pair them up](#transient_pair2)\n",
    "- [5.2. Validate the requirement based on binned measurement residuals](#transient_bin2)\n",
    "- [5.3. Result visulazation](#transient_result2)\n",
    "- [5.3. Conclusion](#transient_conclusion2)\n",
    "\n",
    "[**6. Appendix: GNSS Position Plots**](#transient_appendix)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='transient_prep_a'></a>\n",
    "## Prep A. Environment Setup\n",
    "Setup your environment for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import copy\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "from matplotlib import pyplot as plt\n",
    "from mintpy import smallbaselineApp\n",
    "from mintpy.objects import gnss\n",
    "from mintpy.utils import readfile, utils as ut\n",
    "\n",
    "from solid_utils.sampling import load_geo, samp_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################# Set Directories ##########################################\n",
    "requirement = 'Transient'\n",
    "work_dir = os.path.join(start_directory,dataset,requirement,site,you,today,'v'+version)\n",
    "print(\"Work directory:\", work_dir)\n",
    "\n",
    "gunw_dir = os.path.join(work_dir,'products')\n",
    "print(\"   GUNW    dir:\", gunw_dir) \n",
    "\n",
    "mintpy_dir = os.path.join(work_dir,'MintPy')\n",
    "print(\"   MintPy  dir:\", mintpy_dir)\n",
    "\n",
    "### Change to MintPy workdir\n",
    "if not os.path.exists(mintpy_dir):\n",
    "    print()\n",
    "    print('ERROR: Stop! Your MintPy processing directory does not exist for this requirement, site, version, or date of your ATBD run.')\n",
    "    print('You may need to run the prep notebook first!')\n",
    "    print()\n",
    "else:\n",
    "    os.chdir(mintpy_dir)\n",
    "\n",
    "vel_file = os.path.join(mintpy_dir, 'velocity.h5')\n",
    "msk_file = os.path.join(mintpy_dir, 'maskConnComp.h5')  # maskTempCoh.h5 maskConnComp.h5\n",
    "\n",
    "with open(custom_sites,'r') as fid:\n",
    "    sitedata = json.load(fid)\n",
    "\n",
    "sitedata['sites'][site]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ifgs_file = os.path.join(mintpy_dir,'inputs/ifgramStack.h5')\n",
    "geom_file = os.path.join(mintpy_dir,'inputs/geometryGeo.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the interferogram has a resolution lower than 100 m, we need multi-look the interferogram phase values before calculating the empirical semivarigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the date of interferograms into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ifgs_date = readfile.read(ifgs_file,datasetName='date')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ifgs_date = np.empty_like(ifgs_date,dtype=dt)\n",
    "for i in range(ifgs_date.shape[0]):\n",
    "    start_date = ifgs_date[i,0].decode()\n",
    "    end_date = ifgs_date[i,1].decode()\n",
    "    start_date = dt.strptime(start_date, \"%Y%m%d\")\n",
    "    end_date = dt.strptime(end_date, \"%Y%m%d\")\n",
    "    _ifgs_date[i] = [start_date,end_date]\n",
    "    \n",
    "ifgs_date = _ifgs_date\n",
    "del _ifgs_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove interferograms with time interval other than 12 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del_row_index = []\n",
    "for i in range(ifgs_date.shape[0]):\n",
    "    time_interval = (ifgs_date[i,1]-ifgs_date[i,0]).days\n",
    "    if time_interval != 12:\n",
    "        del_row_index.append(i)\n",
    "while i<ifgs_date.shape[0]-1:\n",
    "    if ifgs_date[i,1]==ifgs_date[i+1,0]:\n",
    "        del_row_index.append(i+1)\n",
    "        i = i+2\n",
    "    else:\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ifgs_date = np.delete(ifgs_date,del_row_index,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify independent interferograms (i.e., selected inteferograms do NOT share common dates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del_row_index = []\n",
    "i = 0\n",
    "while i<ifgs_date.shape[0]-1:\n",
    "    if ifgs_date[i,1]==ifgs_date[i+1,0]:\n",
    "        del_row_index.append(i+1)\n",
    "        i = i+2\n",
    "    else:\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ifgs_date = np.delete(ifgs_date,del_row_index,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the phase and coherence of selected interferograms, geometrical datasets, and attribution of them are loaded into numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unwrapPhaseName = ['unwrapPhase-'+i[0].strftime('%Y%m%d')+'_'+i[1].strftime('%Y%m%d') for i in ifgs_date]\n",
    "coherenceName = ['coherence-'+i[0].strftime('%Y%m%d')+'_'+i[1].strftime('%Y%m%d') for i in ifgs_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ifgs_unw,atr = readfile.read(ifgs_file,datasetName=unwrapPhaseName)\n",
    "insar_displacement = -ifgs_unw*float(atr['WAVELENGTH'])/(4*np.pi)*1000 # unit in mm\n",
    "\n",
    "insar_coherence = readfile.read(ifgs_file,datasetName=coherenceName)[0]\n",
    "del ifgs_unw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change default missing phase values in interferograms from 0.0 to `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "insar_displacement[insar_displacement==0.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_opt_correction'></a>\n",
    "# Optional interferograms correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Phase distortions related to solid earth and ocean tidal effects as well as those due to temporal variations in the vertical stratification of the atmosphere can be mitigated using the approaches described below. At this point, it is expected that these corrections will not be needed to validate the mission requirements, but they may be used to produce the highest quality data products. Typically, these are applied to the estimated time series product rather than to the individual interferograms, since they are a function of the time of each radar acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_solid_earth'></a>\n",
    "## Solid Earth Tides Correction\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_tropo_corr'></a>\n",
    "## Tropospheric Delay Correction\n",
    "Optional atmospheric correction utilizes the PyAPS (Jolivet et al., 2011, Jolivet and Agram, 2012) module within GIAnT (or eventually a merged replacement for GIAnT and MintPy). PyAPS is well documented, maintained and can be freely downloaded. PyAPS is included in GIAnT distribution). PyAPS currently includes support for ECMWF’s ERA-Interim, NOAA’s NARR and NASA’s MERRA weather models. A final selection of atmospheric models to be used for operational NISAR processing will be done during Phase C.\n",
    "\n",
    "[T]ropospheric delay maps are produced from atmospheric data provided by Global Atmospheric Models. This method aims to correct differential atmospheric delay correlated with the topography in interferometric phase measurements. Global Atmospheric Models (hereafter GAMs)... provide estimates of the air temperature, the atmospheric pressure and the humidity as a function of elevation on a coarse resolution latitude/longitude grid. In PyAPS, we use this 3D distribution of atmospheric variables to determine the atmospheric phase delay on each pixel of each interferogram.\n",
    "\n",
    "The absolute atmospheric delay is computed at each SAR acquisition date. For a pixel a_i at an elevation z at acquisition date i, the four surrounding grid points are selected and the delays for their respective elevations are computed. The resulting delay at the pixel a_i is then the bilinear interpolation between the delays at the four grid points. Finally, we combine the absolute delay maps of the InSAR partner images to produce the differential delay maps used to correct the interferograms.\n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_tropo_res_corr'></a>\n",
    "## Topographic Residual Correction \n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Phase deramping is not appplied here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the solid earth tides correction for interferogram is applied, it should also be applied for GNSS observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary summary: we have load all data we need for processing:\n",
    "- `atr`: metadata, including incident angle, longitude and latitude step width, etc;\n",
    "- `insar_displacement`: LOS measurement from InSAR;\n",
    "- `insar_coherence`: coherence value of the interferograms:\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_gnss_los'></a>\n",
    "# Make GNSS LOS Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_co_gnss'></a>\n",
    "## Find Collocated GNSS Stations\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get space and time range for searching GNSS station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "length, width = int(atr['LENGTH']), int(atr['WIDTH'])\n",
    "lat_step = float(atr['Y_STEP'])\n",
    "lon_step = float(atr['X_STEP'])\n",
    "N = float(atr['Y_FIRST'])\n",
    "W = float(atr['X_FIRST'])\n",
    "S = N+lat_step*(length-1)\n",
    "E = W+lon_step*(width-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_date_gnss = ifgs_date[0,0]\n",
    "end_date_gnss = ifgs_date[-1,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for collocated GNSS stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(mintpy_dir)\n",
    "site_names, site_lats, site_lons = gnss.search_gnss(SNWE=(S,N,W,E),\n",
    "                                                  start_date=start_date_gnss.strftime('%Y%m%d'),\n",
    "                                                  end_date=end_date_gnss.strftime('%Y%m%d'))\n",
    "os.chdir(work_dir)\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_gnss_los2'></a>\n",
    "## Make GNSS LOS Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the 3-D GNSS observations are projected into LOS direction. The InSAR observations are averaged 3 by 3 near the station positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** the number of pixels used in calculating the averaged phase values at the GPS location depends on the resolution of input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get daily position solutions for GNSS stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#os.chdir(mint_dir)\n",
    "displacement = {}\n",
    "gnss_time_series = {}\n",
    "gnss_time_series_std = {}\n",
    "bad_stn = {}  #stations to toss\n",
    "pixel_radius = 1   #number of InSAR pixels to average for comparison with GNSS\n",
    "\n",
    "for counter,stn in enumerate(site_names):\n",
    "    gnss_obj = gnss.get_gnss_class('UNR')(site = stn, data_dir = os.path.join(mintpy_dir,'GNSS'))\n",
    "    gnss_obj.open()\n",
    "        \n",
    "    # count number of dates in time range\n",
    "    gnss_obj.read_displacement()\n",
    "    dates = gnss_obj.dates\n",
    "    for i in range(insar_displacement.shape[0]):\n",
    "        start_date = ifgs_date[i,0]\n",
    "        end_date = ifgs_date[i,-1]\n",
    "        \n",
    "        range_days = (end_date - start_date).days\n",
    "        gnss_count = np.histogram(dates, bins=[start_date,end_date])\n",
    "        gnss_count = int(gnss_count[0])\n",
    "        #print(gnss_count)\n",
    "\n",
    "        # select GNSS stations based on data completeness, here we hope to select stations with data frequency of 1 day and no interruption\n",
    "        if range_days == gnss_count-1:\n",
    "        #if start_date in dates and end_date in dates:\n",
    "            disp_gnss_time_series,disp_gnss_time_series_std,site_latlon = gnss_obj.get_los_displacement(os.path.join(mintpy_dir,'inputs/geometryGeo.h5'),\n",
    "                                                                                                            start_date=start_date.strftime('%Y%m%d'),\n",
    "                                                                                                            end_date=end_date.strftime('%Y%m%d'))[1:4]\n",
    "            x_value = round((site_latlon[1] - W)/lon_step)\n",
    "            y_value = round((site_latlon[0] - N)/lat_step)\n",
    "\n",
    "            #displacement from insar observation in the gnss station, averaged\n",
    "            #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "            disp_insar = insar_displacement[i,\n",
    "                                            y_value-pixel_radius:y_value+pixel_radius, \n",
    "                                            x_value-pixel_radius:x_value+pixel_radius]\n",
    "            if np.isfinite(disp_insar).sum() == 0:\n",
    "                break\n",
    "            disp_insar = np.nanmean(disp_insar)\n",
    "\n",
    "            disp_gnss_time_series = disp_gnss_time_series*1000 # convert unit from meter to mm\n",
    "            disp_gnss_time_series_std = disp_gnss_time_series_std*1000\n",
    "            gnss_time_series[(i,stn)] = disp_gnss_time_series\n",
    "            gnss_time_series_std[(i,stn)] = disp_gnss_time_series_std\n",
    "            displacement[(i,stn)] = list(site_latlon)\n",
    "            disp_gnss = disp_gnss_time_series[-1] - disp_gnss_time_series[0]\n",
    "\n",
    "            displacement[(i,stn)].append(disp_gnss)\n",
    "            displacement[(i,stn)].append(disp_insar)\n",
    "        else:\n",
    "            try:\n",
    "                bad_stn[i].append(stn)\n",
    "            except:\n",
    "                bad_stn[i] = [stn]\n",
    "#os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some data structure transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series = dict(sorted(gnss_time_series.items()))\n",
    "gnss_time_series_std = dict(sorted(gnss_time_series_std.items()))\n",
    "displacement = dict(sorted(displacement.items()))\n",
    "bad_stn = dict(sorted(bad_stn.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series = pd.DataFrame.from_dict(gnss_time_series)\n",
    "gnss_time_series_std = pd.DataFrame.from_dict(gnss_time_series_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement = pd.DataFrame.from_dict(displacement,orient='index',\n",
    "                                      columns=['lat','lon','gnss_disp','insar_disp'])\n",
    "displacement.index = pd.MultiIndex.from_tuples(displacement.index,names=['ifg index','station'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are less than 3 GNSS stations, don't conduct comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_index = []\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    if len(displacement.loc[i]) < 3:\n",
    "        drop_index.append(i)\n",
    "displacement=displacement.drop(drop_index)\n",
    "# ifgs_date after drop for approach 1\n",
    "ifgs_date_ap1=np.delete(ifgs_date,drop_index,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data needed for approach 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "- A more general critterion is needed for GNSS station selection. Here the stations with uninterrupted data are selected while, in Secular Requirement Validation, stations are selected by data completeness and standard variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_gnss_insar'></a>\n",
    "## Make GNSS and InSAR Relative Displacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we randomly select one reference site and make both the GNSS and InSAR measurements relative to that reference to remove a constant offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference GNSS stations to GNSS reference site\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    gnss_ref_site_name = random.choice(displacement.loc[i].index.unique())\n",
    "    displacement.loc[i,'gnss_disp'] = displacement.loc[i,'gnss_disp'].values - displacement.loc[(i,gnss_ref_site_name),'gnss_disp']\n",
    "    displacement.loc[i,'insar_disp'] = displacement.loc[i,'insar_disp'].values - displacement.loc[(i,gnss_ref_site_name),'insar_disp']\n",
    "    ref_x_value = round((displacement.loc[(i,gnss_ref_site_name),'lon'] - W)/lon_step)\n",
    "    ref_y_value = round((displacement.loc[(i,gnss_ref_site_name),'lat'] - N)/lat_step)\n",
    "\n",
    "    ref_disp_insar = insar_displacement[i,\n",
    "                                        ref_y_value-pixel_radius:ref_y_value+1+pixel_radius, \n",
    "                                        ref_x_value-pixel_radius:ref_x_value+1+pixel_radius]\n",
    "    ref_disp_insar = np.nanmean(ref_disp_insar)\n",
    "    insar_displacement[i] -= ref_disp_insar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot GNSS stations on InSAR displacement fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = copy.copy(plt.get_cmap('RdBu'))\n",
    "#cmap.set_bad(color='black')\n",
    "vmin, vmax = np.nanmin(insar_displacement), np.nanmax(insar_displacement)\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    fig, ax = plt.subplots()\n",
    "    img1 = ax.imshow(insar_displacement[i], cmap=cmap,vmin=vmin,vmax=vmax, interpolation='nearest', extent=(W, E, S, N))\n",
    "    ax.set_title(ifgs_date[i,0].strftime('%Y%m%d')+'-'+ifgs_date[i,1].strftime('%Y%m%d'))\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('LOS displacement [mm]')\n",
    "\n",
    "    for stn in displacement.loc[i].index:\n",
    "        lon,lat = displacement.loc[(i,stn),'lon'],displacement.loc[(i,stn),'lat']\n",
    "        color = cmap((displacement.loc[(i,stn),'gnss_disp']-vmin)/(vmax-vmin))\n",
    "        ax.scatter(lon,lat,s=8**2,color=color,edgecolors='k')\n",
    "        ax.annotate(stn,(lon,lat),color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='transient_validation1'></a>\n",
    "# NISAR Validation Approach 1: GNSS-InSAR Direct Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_pair1'></a>\n",
    "## Pair up GNSS stations and make measurement residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first pair up all GNSS stations and compare the relative measurement from both GNSS and INSAR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_disp = {}\n",
    "gnss_disp = {}\n",
    "ddiff_dist = {}\n",
    "ddiff_disp = {}\n",
    "abs_ddiff_disp = {}\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    displacement_i = displacement.loc[i]\n",
    "    insar_disp_i = []\n",
    "    gnss_disp_i = []\n",
    "    ddiff_dist_i = []\n",
    "    ddiff_disp_i = []\n",
    "\n",
    "    for sta1 in displacement_i.index:\n",
    "        for sta2 in displacement_i.index:\n",
    "            if sta2 == sta1:\n",
    "                break\n",
    "            insar_disp_i.append(displacement_i.loc[sta1,'insar_disp']-displacement_i.loc[sta2,'insar_disp'])\n",
    "            gnss_disp_i.append(displacement_i.loc[sta1,'gnss_disp']-displacement_i.loc[sta2,'gnss_disp'])\n",
    "            ddiff_disp_i.append(gnss_disp_i[-1]-insar_disp_i[-1])\n",
    "            g = pyproj.Geod(ellps=\"WGS84\")\n",
    "            _,_,distance = g.inv(displacement_i.loc[sta1,'lon'],displacement_i.loc[sta1,'lat'],\n",
    "                                 displacement_i.loc[sta2,'lon'],displacement_i.loc[sta2,'lat'])\n",
    "            distance = distance/1000 # convert unit from m to km\n",
    "            ddiff_dist_i.append(distance)\n",
    "    insar_disp[i]=np.array(insar_disp_i)\n",
    "    gnss_disp[i]=np.array(gnss_disp_i)\n",
    "    ddiff_dist[i]=np.array(ddiff_dist_i)\n",
    "    ddiff_disp[i]=np.array(ddiff_disp_i)\n",
    "    abs_ddiff_disp[i]=abs(np.array(ddiff_disp_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot to compare displacement from GNSS and InSAR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    plt.figure(figsize=(11,7))\n",
    "    disp_range = (min([*insar_disp[i],*gnss_disp[i]]),max([*insar_disp[i],*gnss_disp[i]]))\n",
    "    plt.hist(insar_disp[i],bins=100,range=disp_range,color = \"green\",label='D_InSAR')\n",
    "    plt.hist(gnss_disp[i],bins=100,range=disp_range,color=\"orange\",label='D_GNSS', alpha=0.5)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Displacements \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')} \\n Number of station pairs used: {len(insar_disp[i])}\")\n",
    "    plt.xlabel('LOS Displacement (mm)')\n",
    "    plt.ylabel('Number of Station Pairs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Displacement Residuals Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    plt.figure(figsize=(11,7))\n",
    "    plt.hist(ddiff_disp[i],bins = 100, color=\"darkblue\",linewidth=1,label='D_gnss - D_InSAR')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Residuals \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')} \\n Number of stations pairs used: {len(ddiff_disp[i])}\")\n",
    "    plt.xlabel('Displacement Residual (mm)')\n",
    "    plt.ylabel('N Stations')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Absolute Displacement Residuals As a Function of Distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    dist_th = np.linspace(min(ddiff_dist[i]),max(ddiff_dist[i]),100)\n",
    "    acpt_error = 3*(1+np.sqrt(dist_th))\n",
    "    plt.figure(figsize=(11,7))\n",
    "    plt.scatter(ddiff_dist[i],abs_ddiff_disp[i],s=1)\n",
    "    plt.plot(dist_th, acpt_error, 'r')\n",
    "    plt.xlabel(\"Distance (km)\")\n",
    "    plt.ylabel(\"Amplitude of Displacement Residuals (mm)\")\n",
    "    plt.title(f\"Residuals \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')} \\n Number of stations pairs used: {len(ddiff_dist[i])}\")\n",
    "    plt.legend([\"Measurement\",\"Mission Reqiurement\"])\n",
    "    #plt.xlim(0,5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddiff_dist_ap1 = list(ddiff_dist.values())\n",
    "abs_ddiff_disp_ap1 = list(abs_ddiff_disp.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got all needed data for approach 1:\n",
    "- `ddiff_dist_ap1`: distance of GNSS pairs,\n",
    "- `abs_ddiff_disp_ap1`: absolute value of measurement redisuals,\n",
    "- `ifgs_date_ap1`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_bin1'></a>\n",
    "## Validate the requirement based on binned measurement residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ifgs = len(ddiff_dist_ap1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin all measurement residuals to check if they pass the requirements or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "bins = np.linspace(0.1,50.0,num=n_bins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_all = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points for each ifgs and bins\n",
    "n_pass = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points pass\n",
    "for i in range(n_ifgs):\n",
    "    inds = np.digitize(ddiff_dist_ap1[i],bins)\n",
    "    for j in range(1,n_bins+1):\n",
    "        rqmt = 3*(1+np.sqrt(ddiff_dist_ap1[i][inds==j]))# mission requirement for i-th ifgs and j-th bins\n",
    "        rem = abs_ddiff_disp_ap1[i][inds==j] # relative measurement\n",
    "        assert len(rqmt) == len(rem)\n",
    "        n_all[i,j-1] = len(rem)\n",
    "        n_pass[i,j-1] = np.count_nonzero(rem<rqmt)\n",
    "    n_all[i,-1] = np.sum(n_all[i,0:-2])\n",
    "    n_pass[i,-1] = np.sum(n_pass[i,0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = n_pass/n_all\n",
    "thresthod = 0.683 \n",
    "#The assumed nature of Gaussian distribution gives a probability of 0.683 of being within one standard deviation.\n",
    "success_or_fail = ratio>thresthod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_result1'></a>\n",
    "## Result visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the result to pandas DataFrame for better visulization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(x:bool):\n",
    "    if x==True:\n",
    "        return 'true '\n",
    "    elif x==False:\n",
    "        return 'false '\n",
    "\n",
    "success_or_fail_str = [list(map(to_str, x)) for x in success_or_fail]\n",
    "\n",
    "columns = []\n",
    "for i in range(n_bins):\n",
    "    columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "columns.append('total')\n",
    "\n",
    "index = []\n",
    "for i in range(len(ifgs_date_ap1)):\n",
    "    index.append(ifgs_date_ap1[i,0].strftime('%Y%m%d')+'-'+ifgs_date_ap1[i,1].strftime('%Y%m%d'))\n",
    "\n",
    "n_all_pd = pd.DataFrame(n_all,columns=columns,index=index)\n",
    "n_pass_pd = pd.DataFrame(n_pass,columns=columns,index=index)\n",
    "ratio_pd = pd.DataFrame(ratio,columns=columns,index=index)\n",
    "success_or_fail_pd = pd.DataFrame(success_or_fail_str,columns=columns,index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_all_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points that below the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_pass_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = ratio_pd.style\n",
    "s.set_table_styles([  # create internal CSS classes\n",
    "    {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "    {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "], overwrite=False)\n",
    "s.set_td_classes(success_or_fail_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_conclusion1'></a>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = np.count_nonzero(ratio_pd['total']>thresthod)/n_ifgs\n",
    "print(f\"Percentage of interferograms passes the requirement: {percentage}\")\n",
    "if percentage >= 0.70:\n",
    "    print('The interferogram stack passes the requirement.')\n",
    "else:\n",
    "    print('The interferogram stack fails the requirement.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Approach 1 final result for CentralValleyA144: around 79% of interferograms passes the requirement.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_validation2'></a>\n",
    "# NISAR Validation Approach 2: Noise Level Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this validation (Approach #2), we evaluate the estimated secular deformation rate (Requirements 658) or co-seismic displacement (Requirement 660) from time series processing or the individual unwrapped interferogram (Requirement 663) over selected cal/val areas with negligible deformation. Any estimated deformation should thus be treated as noise and our goal is to evaluate the significance of this noise. In general, noise in the modeled displacement or the unwrapped interferogram is anisotropic, but here we neglect this anisotropy. Also, we assume the noise is stationary.\n",
    "\n",
    "We first randomly sample measurements and pair up sampled pixel measurements. For each pixel-pair, the difference of their measurement becomes:\n",
    "$$d\\left(r\\right)=|(f\\left(x\\right)-f\\left(x-r\\right))|$$\n",
    "Estimates of $d(r)$ from all pairs are binned according to the distance r. In each bin, $d(r)$ is assumed to be a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Now we simply assume there is no deformation in this study area and time interval. But in fact, it is hard to find a enough large area without any deformation. An more realistic solution is to apply a mask to mask out deformed regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ifgs = insar_displacement.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask Pixels with Low Coherence (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insar_displacement[insar_coherence <0.6] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coherence and InSAR measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('gray')\n",
    "\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.imshow(insar_coherence[i],cmap=cmap, interpolation='nearest',extent=(W, E, S, N))\n",
    "    ax.set_title(f\"Coherence \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('coherence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('RdBu')\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.imshow(insar_displacement[i], cmap=cmap, interpolation='nearest', extent=(W, E, S, N))\n",
    "    ax.set_title(f\"Interferogram \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('LOS displacement [mm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_pair2'></a>\n",
    "## Randomly sample pixels and pair them up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coordinate for every pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X0,Y0 = load_geo(atr)\n",
    "X0_2d,Y0_2d = np.meshgrid(X0,Y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each interferogram, randomly selected pixels need to be paired up. In order to keep measurements independent, different pixel pairs can not share same pixel. This is achieved by pairing up in sequence, i.e., pairing up pixel number 1 and number 2, 3 and 4..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = []; rel_measure = []\n",
    "for i in range(n_ifgs):\n",
    "    dist_i, rel_measure_i = samp_pair(X0_2d,Y0_2d,insar_displacement[i],num_samples=1000000)\n",
    "    dist.append(dist_i)\n",
    "    rel_measure.append(rel_measure_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the statistical property of selected pixel pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(dist[i], bins=100)\n",
    "    ax.set_title(f\"Histogram of distance \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_xlabel(r'Distance ($km$)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlim(0,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(rel_measure[i], bins=100)\n",
    "    ax.set_title(f\"Histogram of Relative Measurement \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_xlabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist_th = np.linspace(0,50,100)\n",
    "rqmt = 3*(1+np.sqrt(dist_th))\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    ax.plot(dist_th, rqmt, 'r')\n",
    "    ax.scatter(dist[i], rel_measure[i], s=1, alpha=0.25)\n",
    "    ax.set_title(f\"Comparation between Relative Measurement and Requirement Curve \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_ylabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_xlabel('Distance (km)')\n",
    "    ax.set_xlim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got data used of approach 2:\n",
    "- `dist`: distance of pixel pairs,\n",
    "- `rel_measure`: relative measurement of pixel pairs,\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_bin2'></a>\n",
    "## Validate the requirement based on binned measurement residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ifgs = len(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin all measurement residuals to check if they pass the requirements or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "bins = np.linspace(0.1,50.0,num=n_bins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_all = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points for each ifgs and bins\n",
    "n_pass = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points pass\n",
    "#ratio = np.empty([n_ifgs,n_bins+1]) # ratio\n",
    "# the final column is the ratio as a whole\n",
    "for i in range(n_ifgs):\n",
    "    inds = np.digitize(dist[i],bins)\n",
    "    for j in range(1,n_bins+1):\n",
    "        rqmt = 3*(1+np.sqrt(dist[i][inds==j]))# mission requirement for i-th ifgs and j-th bins\n",
    "        rem = rel_measure[i][inds==j] # relative measurement\n",
    "        assert len(rqmt) == len(rem)\n",
    "        n_all[i,j-1] = len(rem)\n",
    "        n_pass[i,j-1] = np.count_nonzero(rem<rqmt)\n",
    "    n_all[i,-1] = np.sum(n_all[i,0:-2])\n",
    "    n_pass[i,-1] = np.sum(n_pass[i,0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = n_pass/n_all\n",
    "mean_ratio = np.array([np.mean(ratio[:,:-1],axis=1)])\n",
    "ratio = np.hstack((ratio,mean_ratio.T))\n",
    "thresthod = 0.683\n",
    "#The assumed nature of Gaussian distribution gives a probability of 0.683 of being within one standard deviation.\n",
    "success_or_fail = ratio>thresthod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_result2'></a>\n",
    "## Result visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the result to pandas DataFrame for better visulization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(x:bool):\n",
    "    if x==True:\n",
    "        return 'true '\n",
    "    elif x==False:\n",
    "        return 'false '\n",
    "\n",
    "success_or_fail_str = [list(map(to_str, x)) for x in success_or_fail]\n",
    "\n",
    "columns = []\n",
    "for i in range(n_bins):\n",
    "    columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "columns.append('total')\n",
    "\n",
    "index = []\n",
    "for i in range(len(ifgs_date)):\n",
    "    index.append(ifgs_date[i,0].strftime('%Y%m%d')+'-'+ifgs_date[i,1].strftime('%Y%m%d'))\n",
    "\n",
    "n_all_pd = pd.DataFrame(n_all,columns=columns,index=index)\n",
    "n_pass_pd = pd.DataFrame(n_pass,columns=columns,index=index)\n",
    "ratio_pd = pd.DataFrame(ratio,columns=columns+['mean'],index=index)\n",
    "success_or_fail_pd = pd.DataFrame(success_or_fail_str,columns=columns+['mean'],index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_all_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points that below the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_pass_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = ratio_pd.style\n",
    "s.set_table_styles([  # create internal CSS classes\n",
    "    {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "    {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "], overwrite=False)\n",
    "s.set_td_classes(success_or_fail_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_conclusion2'></a>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with percentage of total passed pairs, the mean value of percentage of passed pairs in all bin is a better indicator since it gives all bins same weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = np.count_nonzero(ratio_pd['mean']>thresthod)/n_ifgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of interferograms passes the requirement (70%): {percentage}.\")\n",
    "if percentage >= 0.70:\n",
    "    print('The interferogram stack passes the requirement.')\n",
    "else:\n",
    "    print('The interferogram stack fails the requirement.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Approach 2 final result for CentralValleyA144: 100% of interferograms passes the requirement.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_appendix'></a>\n",
    "# Appendix: GNSS Position Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative position in LOS direction for all GNSS stations are plotted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(gnss_time_series)):\n",
    "    start_time_str = ifgs_date[i,0].strftime('%Y%m%d')\n",
    "    end_time_str = ifgs_date[i,1].strftime('%Y%m%d')\n",
    "    for stn in gnss_time_series[i].columns:\n",
    "        print(f'Plotting GPS postion from {start_time_str} to {end_time_str} at station: {stn}')\n",
    "        series = gnss_time_series[i,str(stn)]\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.title(f\"station name: {stn}\")\n",
    "        plt.scatter(pd.date_range(start=ifgs_date[i,0], end=ifgs_date[i,1]),series)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Relative position in LOS direction (mm)')\n",
    "        plt.savefig(os.path.join(work_dir,start_time_str+'_'+end_time_str+'_'+stn+'.jpg'))\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solid_earth_atbd",
   "language": "python",
   "name": "solid_earth_atbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
