{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Workflow to Validate NISAR L2 Transient Displacement Requirements\n",
    "\n",
    "**Original code authored by:** NISAR Science Team Members and Affiliates  \n",
    "\n",
    "*May 13, 2022*\n",
    "\n",
    "*NISAR Solid Earth Team*\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Both the initial setup (<b>Prep A</b> section) and download of the data (<b>Prep B</b> section) should be run at the start of the notebook. And all subsequent sections NEED to be run in order.\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Table of Contents: <a id='TOC'></a>\n",
    "\n",
    "[**Environment Setup**](#setup)\n",
    "- [Load Python Packages](#load_packages)\n",
    "- [Define CalVal Site and Parameters](#set_calval_params)\n",
    "- [Set Directories and Files](#set_directories)\n",
    "\n",
    "[**1. Download and Prepare Interferograms**](#prep_ifg)\n",
    "[Executed in ARIA_prep]\n",
    "\n",
    "[**2. Selection of Interferograms**](#transient_select_ifg)\n",
    "- [2.1.  Validate/Modify Interferogram Network](#transient_crop_ifg)\n",
    "- [2.2.  Modify Reference Point](#transient_ref_pt)\n",
    "\n",
    "[**3. Optional Corrections**](#opt_correction)\n",
    "- [3.1. Solid Earth Tide Correction](#solid_earth)\n",
    "- [3.2 Ionosphere Correction](#iono_corr)\n",
    "- [3.3. Tropospheric Delay Correction](#tropo_corr)\n",
    "- [3.4. Phase Deramping ](#phase_deramp)\n",
    "- [3.5. Topographic Residual Correction ](#topo_corr)\n",
    "\n",
    "[**4. Make GNSS LOS Measurements**](#transient_gnss_los)\n",
    "- [4.3. Find Collocated GNSS Stations](#transient_co_gnss)  \n",
    "- [4.4. Make GNSS LOS Measurements](#transient_gnss_los2) \n",
    "- [4.5. Make GNSS and InSAR Relative Displacements](#transient_gnss_insar)\n",
    "\n",
    "[**5. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison**](#transient_validation1)\n",
    "- [5.1. Pair up GNSS stations and make measurement residuals](#transient_pair1)\n",
    "- [5.2. Validate the requirement based on binned measurement residuals](#transient_bin1)\n",
    "- [5.3. Result visulazation](#transient_result1)\n",
    "- [5.3. Conclusion](#transient_conclusion1)\n",
    "\n",
    "[**6. NISAR Validation Approach 2: Noise Level Validation**](#transient_validation2)\n",
    "- [6.1. Randomly sample pixels and pair them up](#transient_pair2)\n",
    "- [6.2. Validate the requirement based on binned measurement residuals](#transient_bin2)\n",
    "- [6.3. Result visulazation](#transient_result2)\n",
    "- [6.3. Conclusion](#transient_conclusion2)\n",
    "\n",
    "[**Appendix: GNSS Position Plots**](#transient_appendix)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='#setup'></a>\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Python Packages <a id='#load_packages'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import copy\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "from matplotlib import pyplot as plt\n",
    "from mintpy import smallbaselineApp\n",
    "from mintpy.objects import gnss\n",
    "from mintpy.utils import readfile, utils as ut, network\n",
    "from mintpy.cli import view\n",
    "\n",
    "from solid_utils.sampling import load_geo, samp_pair\n",
    "from solid_utils.plotting import display_validation_table, \\\n",
    "    display_coseismic_validation as display_transient_validation\n",
    "from solid_utils.configs import update_reference_point\n",
    "from solid_utils.corrections import run_cmd, pairwise_stack_from_timeseries\n",
    "from solid_utils.saving import save_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Calval Site and Parameters <a id='set_calval_params'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Basic Configuration ===\n",
    "site = \"test\"\n",
    "requirement = \"Transient\"\n",
    "dataset = 'ARIA_S1_new' # For Sentinel-1 testing with aria-tools\n",
    "aria_gunw_version = \"3_0_1\"\n",
    "\n",
    "rundate = \"20250826\"  # Date of this Cal/Val run\n",
    "version = \"1b\"         # Version of this Cal/Val run\n",
    "custom_sites = \"/home/jovyan/my_sites.txt\"  # Path to custom site metadata\n",
    "\n",
    "# === Username Detection / Creation ===\n",
    "user_file = \"/home/jovyan/me.txt\"\n",
    "if os.path.exists(user_file):\n",
    "    with open(user_file, \"r\") as f:\n",
    "        you = f.readline().strip()\n",
    "else:\n",
    "    you = input(\"Please type a username for your Cal/Val outputs: \").strip()\n",
    "    with open(user_file, \"w\") as f:\n",
    "        f.write(you)\n",
    "\n",
    "# === Load Cal/Val Site Metadata ===\n",
    "try:\n",
    "    with open(custom_sites, \"r\") as f:\n",
    "        sitedata = json.load(f)\n",
    "    site_info = sitedata[\"sites\"][site]\n",
    "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "    raise RuntimeError(f\"Failed to load site metadata from {custom_sites}: {e}\")\n",
    "except KeyError:\n",
    "    raise ValueError(f\"Site ID '{site}' not found in {custom_sites}\")\n",
    "\n",
    "print(f\"Loaded site: {site}\")\n",
    "\n",
    "# === Plot Parameters ===\n",
    "vmin, vmax = -50, 50  # mm\n",
    "cmap = plt.get_cmap('RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Directories and Files <a id='set_directories'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Cal/Val Directory Structure ===\n",
    "BASE_DIR = \"/scratch/nisar-st-calval-solidearth\"\n",
    "site_dir = os.path.join(BASE_DIR, dataset, site)\n",
    "work_dir = os.path.join(site_dir, requirement, you, rundate, f\"v{version}\")\n",
    "gunw_dir = os.path.join(site_dir, \"products\")\n",
    "mintpy_dir = os.path.join(work_dir, \"MintPy\")\n",
    "weather_dir = os.path.join(site_dir)\n",
    "\n",
    "# === Home directory for saving reports ===\n",
    "home_dir = os.path.join(\"/home/jovyan/validation/\", site, requirement, rundate, f\"v{version}\")\n",
    "if not os.path.exists(home_dir):\n",
    "    os.makedirs(home_dir)\n",
    "\n",
    "# === Log Directory Paths ===\n",
    "print(f\"  Work directory: {work_dir}\")\n",
    "print(f\"  GUNW directory: {gunw_dir}\")\n",
    "print(f\"MintPy directory: {mintpy_dir}\")\n",
    "\n",
    "# === Check MintPy Directory Existence ===\n",
    "if not os.path.exists(mintpy_dir):\n",
    "    print(\"\\nERROR: Stop! MintPy processing directory is missing.\")\n",
    "    print(\"This may indicate the prep notebook has not been run.\")\n",
    "    print(\"Missing path:\", mintpy_dir, \"\\n\")\n",
    "else:\n",
    "    os.chdir(mintpy_dir)\n",
    "\n",
    "# === Set Expected MintPy Filenames ===\n",
    "ifgs_file = os.path.join(mintpy_dir, \"inputs/ifgramStack.h5\")\n",
    "geom_file = os.path.join(mintpy_dir, \"inputs\", \"geometryGeo.h5\")\n",
    "msk_file  = os.path.join(work_dir, \"mask\", \"esa_world_cover_2021.msk\")\n",
    "config_file = os.path.join(mintpy_dir, site_info.get('calval_location') + '.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "configs = readfile.read_template(config_file)\n",
    "print('#' * 10, \"MintPy Configs\", '#' * 10)\n",
    "for key, value in configs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='#prep_ifg'></a>\n",
    "## 1. Download and Prepare Interferograms \n",
    "Executed in *ARIA_prep* notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='#transient_select_ifg'></a>\n",
    "## 2. Selection of Interferograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Validate/Modify Interferogram Network <a id='transient_crop_ifg'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the interferogram has a resolution lower than 100 m, we need multi-look the interferogram phase values before calculating the empirical semivarigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the date of interferograms into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulate ifgramStack file\n",
    "ifgramStack_file = os.path.join(mintpy_dir, 'inputs/ifgramStack.h5')\n",
    "\n",
    "# Modify network - base command\n",
    "command = f\"modify_network.py {ifgramStack_file} -t {config_file} \"\n",
    "\n",
    "# Check whether exclusions specified in my_sites file\n",
    "if site_info.get('ifgExcludePair') not in [None, 'auto', 'no']:\n",
    "    command += f\" --exclude-ifg {site_info.get('ifgExcludePair')}\"\n",
    "\n",
    "if site_info.get('ifgExcludeDate') not in [None, 'auto', 'no']:\n",
    "    command += f\" --exclude-date {site_info.get('ifgExcludeDate')}\"\n",
    "\n",
    "if site_info.get('ifgExcludeIndex') not in [None, 'auto', 'no']:\n",
    "    command += f\" --exclude-ifg-index {site_info.get('ifgExcludeIndex')} \"\n",
    "\n",
    "# Run command\n",
    "process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the valid date pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve available interferogram date pairs\n",
    "ifgs_date = network.get_date12_list(ifgs_file, dropIfgram=True)\n",
    "\n",
    "# Report all available date pairs\n",
    "print(f\"Total {len(ifgs_date)} interferograms available\")\n",
    "for pair in ifgs_date:\n",
    "    print(f\"{'-'.join(pair.split('_'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Format date strings into Python datetime objects\n",
    "_ifgs_date = np.empty_like(ifgs_date, dtype=dt)\n",
    "for i, pair in enumerate(ifgs_date):\n",
    "    start_date, end_date = pair.split(\"_\")\n",
    "    start_date = dt.strptime(start_date, \"%Y%m%d\")\n",
    "    end_date = dt.strptime(end_date, \"%Y%m%d\")\n",
    "    _ifgs_date[i] = [start_date, end_date]\n",
    "\n",
    "# Update list of interferogram dates\n",
    "ifgs_date = _ifgs_date\n",
    "\n",
    "# Remove temporary list to avoid future confusion\n",
    "del _ifgs_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove interferograms with time interval other than 12 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine which interferograms to exclude based on 12-day criterion\n",
    "del_row_index = []\n",
    "for i in range(ifgs_date.shape[0]):\n",
    "    time_interval = (ifgs_date[i][1]-ifgs_date[i][0]).days\n",
    "    if time_interval != 12:\n",
    "        del_row_index.append(i)\n",
    "while i<ifgs_date.shape[0]-1:\n",
    "    if ifgs_date[i][1]==ifgs_date[i+1][0]:\n",
    "        del_row_index.append(i+1)\n",
    "        i = i+2\n",
    "    else:\n",
    "        i = i+1\n",
    "\n",
    "# Remove non-12-day interferograms\n",
    "ifgs_date = np.delete(ifgs_date, del_row_index, 0)\n",
    "\n",
    "# Report 12-day date pairs\n",
    "print(f\"{len(ifgs_date)} interferograms with 12-day baselines\")\n",
    "for pair in ifgs_date:\n",
    "    print(f\"{pair[0].strftime('%Y%m%d')}-{pair[1].strftime('%Y%m%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify independent interferograms (i.e., selected inteferograms do NOT share common dates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine which interferograms to exclude based on independence criterion\n",
    "del_row_index = []\n",
    "i = 0\n",
    "while i<ifgs_date.shape[0]-1:\n",
    "    if ifgs_date[i][1]==ifgs_date[i+1][0]:\n",
    "        del_row_index.append(i+1)\n",
    "        i = i+2\n",
    "    else:\n",
    "        i = i+1\n",
    "\n",
    "# Remove non-independent interferograms from list\n",
    "ifgs_date = np.delete(ifgs_date, del_row_index, 0)\n",
    "\n",
    "# Report independent date pairs\n",
    "print(f\"{len(ifgs_date)} independent interferograms (no common date shared)\")\n",
    "for pair in ifgs_date:\n",
    "    print(f\"{pair[0].strftime('%Y%m%d')}-{pair[1].strftime('%Y%m%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the coherence data sets of selected interferograms into memory. Hold off loading the phase values until optional correction layers have been applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct dataset-layer names as lists\n",
    "coherenceName = [f\"coherence-{date[0].strftime('%Y%m%d')}_{date[1].strftime('%Y%m%d')}\"\n",
    "                 for date in ifgs_date]\n",
    "\n",
    "# Read average filtered spatial coherence\n",
    "insar_coherence, _ = readfile.read(ifgs_file, datasetName=coherenceName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Generate Quality Control Mask <a id='generate_mask'></a>\n",
    "\n",
    "Not implemented. Interferogram masking currently relies on the water mask embedded in the `geometryGeo.h5` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Reference Interferograms To Common Lat/Lon <a id='common_latlon'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if site_info.get('reference_lalo') != 'auto':\n",
    "    new_lat = site_info.get('reference_lalo').split(',')[0]\n",
    "    new_lon = site_info.get('reference_lalo').split(',')[1]\n",
    "    update_reference_point(config_file, new_lat, new_lon) # updates the reference point in MintPy config file\n",
    "    \n",
    "# Now reference interferograms to common lat/lon\n",
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep reference_point'\n",
    "process = subprocess.run(command, shell=True)\n",
    "os.system('info.py inputs/ifgramStack.h5 | egrep \"REF_\"');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the original interferograms\n",
    "if site_info.get('do_iono') != \"False\":\n",
    "    view.main([ifgs_file, '-c', 'RdBu_r', '-m', msk_file])\n",
    "else: \n",
    "    print('#'*10, 'Ionosphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_opt_correction'></a>\n",
    "## 3. Optional Interferogram Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Phase distortions related to solid earth and ocean tidal effects as well as those due to temporal variations in the vertical stratification of the atmosphere can be mitigated using the approaches described below. At this point, it is expected that these corrections will not be needed to validate the mission requirements, but they may be used to produce the highest quality data products. Typically, these are applied to the estimated time series product rather than to the individual interferograms, since they are a function of the time of each radar acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Solid Earth Tides Correction <a id='solid_earth'></a>\n",
    "\n",
    "Not implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically set do_SET to False\n",
    "site_info['do_SET'] = \"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Ionosphere Correction <a id='iono_corr'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine whether the ionosphere correction will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'do_iono' in site_info.keys() and site_info.get('do_iono') != \"False\":\n",
    "    # Input ionosphere stack file  \n",
    "    iono_stack_file = f\"{mintpy_dir}/inputs/ionStack.h5\"\n",
    "\n",
    "    # Check if iono correction file exists\n",
    "    if os.path.exists(iono_stack_file):\n",
    "        # Specify file paths\n",
    "        dirpath, filename = os.path.split(ifgs_file)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        output_ifgs = os.path.join(dirpath, f\"{name}_iono{ext}\")\n",
    "    else:\n",
    "        site_info['do_iono'] = \"False\"\n",
    "else:\n",
    "    site_info['do_iono'] = \"False\"\n",
    "    print('#'*10, 'Ionosphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ionStack.h5` dataset is written using the convention ref_repeat and is stored in units of radians. Therefore, the ionosphere correction layers can be directly subtracted from the ifgramStack.h5 unwrapped phase values to apply the ionosphere correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set reference point of ionosphere file\n",
    "if site_info.get('do_iono')!= \"False\":\n",
    "    # Run difference\n",
    "    run_cmd(f\"reference_point.py {iono_stack_file} -r {ifgs_file}\",\n",
    "            desc=\"Apply iono correction to IFG stack\")\n",
    "    \n",
    "\n",
    "else:\n",
    "    print('#'*10, 'Ionosphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply correction if flag is set to True\n",
    "if site_info.get('do_iono')!= \"False\":\n",
    "    # Run difference\n",
    "    run_cmd(f\"diff.py {ifgs_file} {iono_stack_file} -o {output_ifgs}\",\n",
    "            desc=\"Apply iono correction to IFG stack\")\n",
    "    \n",
    "    # Update filename\n",
    "    ifgs_file = output_ifgs\n",
    "else:\n",
    "    print('#'*10, 'Ionosphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the corrections\n",
    "if site_info.get('do_iono') != \"False\":\n",
    "    view.main([iono_stack_file, '-c', 'RdBu_r', '-m', msk_file])\n",
    "else: \n",
    "    print('#'*10, 'Ionosphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the corrected interferograms\n",
    "if site_info.get('do_iono') != \"False\":\n",
    "    view.main([ifgs_file, '-c', 'RdBu_r', '-m', msk_file])\n",
    "else: \n",
    "    print('#'*10, 'Ionosphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Tropospheric Delay Correction <a id='tropo_corr'></a>\n",
    "\n",
    "Optional atmospheric correction utilizes the PyAPS (Jolivet et al., 2011, Jolivet and Agram, 2012) module within GIAnT (or eventually a merged replacement for GIAnT and MintPy). PyAPS is well documented, maintained and can be freely downloaded. PyAPS is included in GIAnT distribution). PyAPS currently includes support for ECMWF’s ERA-Interim, NOAA’s NARR and NASA’s MERRA weather models. A final selection of atmospheric models to be used for operational NISAR processing will be done during Phase C.\n",
    "\n",
    "Tropospheric delay maps are produced from atmospheric data provided by Global Atmospheric Models. This method aims to correct differential atmospheric delay correlated with the topography in interferometric phase measurements. Global Atmospheric Models (hereafter GAMs)... provide estimates of the air temperature, the atmospheric pressure and the humidity as a function of elevation on a coarse resolution latitude/longitude grid. In PyAPS, we use this 3D distribution of atmospheric variables to determine the atmospheric phase delay on each pixel of each interferogram.\n",
    "\n",
    "The absolute atmospheric delay is computed at each SAR acquisition date. For a pixel a_i at an elevation z at acquisition date i, the four surrounding grid points are selected and the delays for their respective elevations are computed. The resulting delay at the pixel a_i is then the bilinear interpolation between the delays at the four grid points. Finally, we combine the absolute delay maps of the InSAR partner images to produce the differential delay maps used to correct the interferograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'do_tropo' in site_info.keys() and site_info.get(\"do_tropo\") != \"False\":\n",
    "    if site_info.get(\"tropo_model\") != \"HRRR\":\n",
    "        # ERA5-based correction\n",
    "        tropo_source = \"ERA5\"\n",
    "        tropo_cor_file = os.path.join(mintpy_dir, \"inputs\", f\"{tropo_source}.h5\")\n",
    "\n",
    "    else:\n",
    "        # HRRR-based correction\n",
    "        tropo_source = \"HRRR_ARIA\"\n",
    "        tropo_cor_file = os.path.join(mintpy_dir, \"inputs\", f\"{tropo_source}.h5\")\n",
    "\n",
    "    print(f\"Troposphere Correction dataset: {tropo_cor_file:s}\")\n",
    "\n",
    "    # Check it tropo correction file exists\n",
    "    if os.path.exists(tropo_cor_file):\n",
    "        dirpath, filename = os.path.split(ifgs_file)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        output_ifgs = os.path.join(dirpath, f\"{name}_{tropo_source}{ext}\")\n",
    "        print('#'*10, 'Troposphere Correction set to True', '#'*10)\n",
    "    else:\n",
    "        site_info['do_tropo'] = \"False\"\n",
    "\n",
    "else:\n",
    "    site_info['do_tropo'] = \"False\"\n",
    "\n",
    "if site_info['do_tropo'] == \"False\":\n",
    "    print(\"#\" * 10, \"Troposphere Correction set to False\", \"#\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troposphere correction layer datasets are stored as time-series (one date per acquisition) and in units of meters. To apply the troposphere delay corrections to the `ifgramStack.h5` files, the differential layers must be computed in ref_repeat format, and scaled to units of radians. This is done using the `create_tropo_pairs` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stack of troposphere correction layer pairs following the above sign convention\n",
    "if site_info.get('do_tropo')!= \"False\":\n",
    "    try:\n",
    "        # Attempt to create a pairwise stack of tropo files\n",
    "        tropo_stack_file = pairwise_stack_from_timeseries(ifgs_file, tropo_cor_file)\n",
    "    except:\n",
    "        print(\"Tropo stack generation failed. Setting tropo correction to False\")\n",
    "\n",
    "        # Stack creation failed, set tropo correction to false\n",
    "        site_info['do_tropo'] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set reference point of troposphere file\n",
    "if site_info.get('do_tropo')!= \"False\":\n",
    "    # Run difference\n",
    "    run_cmd(f\"reference_point.py {tropo_stack_file} -r {ifgs_file}\",\n",
    "            desc=\"Apply tropo correction to IFG stack\")\n",
    "\n",
    "else:\n",
    "    print('#'*10, 'Troposphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply correction if flag is set to True\n",
    "if site_info.get('do_tropo')!= \"False\":\n",
    "    # Run difference\n",
    "    run_cmd(f\"diff.py {ifgs_file} {tropo_stack_file} -o {output_ifgs}\",\n",
    "            desc=\"Apply tropo correction to IFG stack\")\n",
    "    \n",
    "    # Update filename\n",
    "    ifgs_file = output_ifgs\n",
    "else:\n",
    "    print('#'*10, 'Troposphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the corrections\n",
    "if site_info.get('do_tropo') != \"False\":\n",
    "    view.main([tropo_stack_file, '-c', 'RdBu_r', '-m', msk_file])\n",
    "else: \n",
    "    print('#'*10, 'Troposphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the corrected interferograms\n",
    "if site_info.get('do_tropo') != \"False\":\n",
    "    view.main([ifgs_file, '-c', 'RdBu_r', '-m', msk_file])\n",
    "else: \n",
    "    print('#'*10, 'Troposphere Correction set to False', '#'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Phase Deramping <a id='phase_deramp'></a>\n",
    "\n",
    "Not implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Topographic Residual Correction <a id='topo_corr'></a>\n",
    "\n",
    "Not implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary summary: we have load all data we need for processing:\n",
    "- `atr`: metadata, including incident angle, longitude and latitude step width, etc;\n",
    "- `insar_displacement`: LOS measurement from InSAR;\n",
    "- `insar_coherence`: coherence value of the interferograms:\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_gnss_los'></a>\n",
    "# 4. Make GNSS and InSAR Relative Displacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the unwrapped phase values into memory and convert from phase in radians, to displacement in mm. Change default missing phase values in interferograms from 0.0 to `np.nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> This notebook uses the sign convention <b>ref_repeat</b> (e.g., 20190124_20190112). That is, range decrease is positive and therefore \"up\" is positive. In contrast, MintPy interferograms follow the opposite convention (range increase is positive).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataset-layer names as lists\n",
    "unwrapPhaseName = [f\"unwrapPhase-{date[0].strftime('%Y%m%d')}_{date[1].strftime('%Y%m%d')}\"\n",
    "                   for date in ifgs_date]\n",
    "\n",
    "# Read unwrapped phase from selected interferograms\n",
    "ifgs_unw, insar_metadata = readfile.read(ifgs_file, datasetName=unwrapPhaseName)\n",
    "\n",
    "# Convert phase to displacement in m and switch convention to positive range decrease\n",
    "insar_displacement = -ifgs_unw*float(insar_metadata['WAVELENGTH']) / (4*np.pi)\n",
    "\n",
    "# Convert displacement units from m to mm\n",
    "insar_displacement = insar_displacement * 1000.\n",
    "\n",
    "# Read 2D mask array\n",
    "msk, _ = readfile.read(msk_file, datasetName=\"waterMask\")\n",
    "\n",
    "# Repeat mask array for each interferogram\n",
    "msk = np.stack([msk] * insar_displacement.shape[0], axis=0)\n",
    "    \n",
    "# Set masked pixels to NaN\n",
    "insar_displacement[msk == 0] = np.nan\n",
    "insar_displacement[insar_displacement==0.0] = np.nan\n",
    "\n",
    "# Clean up phase-only IFGs to avoid future confusion\n",
    "del ifgs_unw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Not Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Not Used <a id='empty'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Find Collocated GNSS Stations <a id='transient_co_gnss'></a>\n",
    "\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get space and time range for searching GNSS station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial metadata\n",
    "length, width = int(insar_metadata['LENGTH']), int(insar_metadata['WIDTH'])\n",
    "lat_step = float(insar_metadata['Y_STEP'])\n",
    "lon_step = float(insar_metadata['X_STEP'])\n",
    "N = float(insar_metadata['Y_FIRST'])\n",
    "W = float(insar_metadata['X_FIRST'])\n",
    "S = N+lat_step*(length-1)\n",
    "E = W+lon_step*(width-1)\n",
    "\n",
    "# Temporal metadata\n",
    "start_date_gnss = ifgs_date[0,0]\n",
    "end_date_gnss = ifgs_date[-1,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for collocated GNSS stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GNSS processing source\n",
    "if 'gnss_source' in sitedata['sites'][site]:\n",
    "    gnss_source = sitedata['sites'][site]['gnss_source']\n",
    "else:\n",
    "    gnss_source = 'UNR'\n",
    "print(f\"GNSS processing source: {gnss_source:s}\")\n",
    "\n",
    "# Start and end dates\n",
    "start_date = start_date_gnss.strftime('%Y%m%d')\n",
    "end_date = end_date_gnss.strftime('%Y%m%d')\n",
    "\n",
    "# Query GNSS sites within geographic and date range\n",
    "site_names, site_lats, site_lons = gnss.search_gnss(SNWE=(S,N,W,E),\n",
    "                                                    start_date=start_date,\n",
    "                                                    end_date=end_date,\n",
    "                                                    source=gnss_source)\n",
    "os.chdir(work_dir)\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(f\"Initial list of {len(site_names)} stations used in analysis:\")\n",
    "print(site_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Get GNSS Position Time Series <a id='gps_ts'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the 3D GNSS observations are projected into the satellite LOS. The InSAR observations are averaged over a 3$\\times$3 pixel window around the station positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> The number of pixels used in calculating the averaged phase values at the GPS location depends on the resolution of input data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get daily position solutions for GNSS stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Empty dictionaries to store InSAR and GNSS data\n",
    "displacement = {}\n",
    "gnss_time_series = {}\n",
    "gnss_time_series_std = {}\n",
    "bad_stn = {}  # stations to toss\n",
    "pixel_radius = 3  # number of InSAR pixels to average for comparison with GNSS\n",
    "\n",
    "# Loop through GNSS sites\n",
    "for counter, site_name in enumerate(site_names):\n",
    "    gnss_stn = gnss.get_gnss_class(gnss_source)(site = site_name)\n",
    "    gnss_stn.open(print_msg=False)\n",
    "\n",
    "    # Download / read the GNSS site displacement and dates\n",
    "    gnss_stn.read_displacement()\n",
    "    dates = gnss_stn.dates\n",
    "\n",
    "    # Count number of dates in time range by looping through interferograms\n",
    "    for ifg_ndx in range(insar_displacement.shape[0]):\n",
    "        # Days in interferogram range (should be 12 based on above filtering)\n",
    "        start_date = ifgs_date[ifg_ndx,0]\n",
    "        end_date = ifgs_date[ifg_ndx,-1]\n",
    "        range_days = (end_date - start_date).days\n",
    "\n",
    "        # Count number of GNSS epochs in IFG date range\n",
    "        gnss_count = np.histogram(dates, bins=[start_date,end_date])\n",
    "        gnss_count = int(gnss_count[0])\n",
    "\n",
    "        # Select GNSS stations based on data completeness\n",
    "        # Here we hope to select stations with data frequency of 1 day and no interruption\n",
    "        if range_days == gnss_count - 1:\n",
    "            # If start_date in dates and end_date in dates, retrieve displacement data\n",
    "            (disp_gnss_time_series,\n",
    "             disp_gnss_time_series_std,\n",
    "             site_latlon) = gnss_stn.get_los_displacement(geom_file,\n",
    "                                                          start_date=start_date.strftime('%Y%m%d'),\n",
    "                                                          end_date=end_date.strftime('%Y%m%d'))[1:4]\n",
    "\n",
    "            # Compute station pixel coordinates\n",
    "            x_value = round((site_latlon[1] - W)/lon_step)\n",
    "            y_value = round((site_latlon[0] - N)/lat_step)\n",
    "\n",
    "            # Displacement from insar observation in the gnss station, averaged\n",
    "            # Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "            disp_insar = insar_displacement[ifg_ndx,\n",
    "                                            y_value-pixel_radius:y_value+pixel_radius+1, \n",
    "                                            x_value-pixel_radius:x_value+pixel_radius+1]\n",
    "\n",
    "            # Check values in InSAR displacement series are valid\n",
    "            if np.isfinite(disp_insar).sum() == 0:\n",
    "                # Ignore station if infinite values\n",
    "                break\n",
    "\n",
    "            # Compute mean of window around InSAR pixel, ignoring NaNs\n",
    "            disp_insar = np.nanmean(disp_insar)\n",
    "\n",
    "            # Scale GNSS displacement values from m to mm\n",
    "            disp_gnss_time_series = disp_gnss_time_series*1000\n",
    "            disp_gnss_time_series_std = disp_gnss_time_series_std*1000\n",
    "\n",
    "            # Store time-series displacements to dicts, labeled by IFG index and site name\n",
    "            gnss_time_series[(ifg_ndx, site_name)] = disp_gnss_time_series\n",
    "            gnss_time_series_std[(ifg_ndx, site_name)] = disp_gnss_time_series_std\n",
    "            displacement[(ifg_ndx, site_name)] = list(site_latlon)\n",
    "            disp_gnss = disp_gnss_time_series[-1] - disp_gnss_time_series[0]\n",
    "\n",
    "            displacement[(ifg_ndx, site_name)].append(disp_gnss)\n",
    "            displacement[(ifg_ndx, site_name)].append(disp_insar)\n",
    "        else:\n",
    "            try:\n",
    "                bad_stn[ifg_ndx].append(site_name)\n",
    "            except:\n",
    "                bad_stn[ifg_ndx] = [site_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some data structure transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rearrange GNSS TS and disp values by IFG number and site name\n",
    "gnss_time_series = dict(sorted(gnss_time_series.items()))\n",
    "gnss_time_series_std = dict(sorted(gnss_time_series_std.items()))\n",
    "displacement = dict(sorted(displacement.items()))\n",
    "bad_stn = dict(sorted(bad_stn.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert GNSS TS dictionaries to pandas dataframes\n",
    "gnss_time_series = pd.DataFrame.from_dict(gnss_time_series)\n",
    "gnss_time_series_std = pd.DataFrame.from_dict(gnss_time_series_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert displacement dictionaries to pandas dataframes\n",
    "displacement = pd.DataFrame.from_dict(displacement, orient='index',\n",
    "                                      columns=['lat','lon','gnss_disp','insar_disp'])\n",
    "\n",
    "# Organize by IFG index and site name\n",
    "displacement.index = pd.MultiIndex.from_tuples(displacement.index, names=['ifg index','station'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are fewer than 3 GNSS stations, don't conduct comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop IFGs with fewer than three stations\n",
    "drop_index = []\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    if len(displacement.loc[i]) < 3:\n",
    "        drop_index.append(i)\n",
    "displacement=displacement.drop(drop_index)\n",
    "\n",
    "# ifgs_date after drop for approach 1\n",
    "ifgs_date_ap1=np.delete(ifgs_date,drop_index,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data needed for approach 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> A more general critterion is needed for GNSS station selection. Here the stations with uninterrupted data are selected while, in Secular Requirement Validation, stations are selected by data completeness and standard variation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Re-reference GNSS and InSAR <a id='reference'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we randomly select one reference site and make both the GNSS and InSAR measurements relative to that reference to remove a constant offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read reference site\n",
    "gnss_ref_site_name = sitedata['sites'][site]['gps_ref_site_name']\n",
    "print(f\"Using reference site: {gnss_ref_site_name:s}\")\n",
    "\n",
    "# Loop through interferograms to re-reference\n",
    "for ifg_ndx in displacement.index.get_level_values(0).unique():\n",
    "    # Determine reference site\n",
    "    if gnss_ref_site_name in ['auto', 'random']:\n",
    "        # Choose random GNSS site\n",
    "        gnss_ref_site_name = random.choice(displacement.loc[ifg_ndx].index.unique())\n",
    "        print(f\"Ifg {ifg_ndx} reference site: {gnss_ref_site_name}\")\n",
    "\n",
    "    # Remove reference site values from GNSS and InSAR displacements\n",
    "    displacement.loc[ifg_ndx, 'gnss_disp'] = displacement.loc[ifg_ndx, 'gnss_disp'].values \\\n",
    "            - displacement.loc[(ifg_ndx, gnss_ref_site_name), 'gnss_disp']\n",
    "    displacement.loc[ifg_ndx, 'insar_disp'] = displacement.loc[ifg_ndx, 'insar_disp'].values \\\n",
    "            - displacement.loc[(ifg_ndx, gnss_ref_site_name), 'insar_disp']\n",
    "\n",
    "    # Reference point pixel coordinates\n",
    "    ref_x_value = round((displacement.loc[(ifg_ndx, gnss_ref_site_name),'lon'] - W)/lon_step)\n",
    "    ref_y_value = round((displacement.loc[(ifg_ndx, gnss_ref_site_name),'lat'] - N)/lat_step)\n",
    "\n",
    "    # InSAR displacement values at site location for re-referencing\n",
    "    ref_disp_insar = insar_displacement[ifg_ndx,\n",
    "                                        ref_y_value-pixel_radius:ref_y_value+1+pixel_radius, \n",
    "                                        ref_x_value-pixel_radius:ref_x_value+1+pixel_radius]\n",
    "\n",
    "    # Re-referenced mean value at site location\n",
    "    ref_disp_insar = np.nanmean(ref_disp_insar)\n",
    "\n",
    "    # Subtract reference value from InSAR displacement\n",
    "    insar_displacement[ifg_ndx] -= ref_disp_insar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot GNSS stations on InSAR displacement fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set color values\n",
    "cmap_obj = copy.copy(plt.get_cmap(cmap))\n",
    "\n",
    "vmin = vmin if vmin is not None else np.nanmin(insar_displacement)\n",
    "vmax = vmax if vmax is not None else np.nanmax(insar_displacement)\n",
    "\n",
    "# Loop through interferograms\n",
    "gnss_insar_figs = []\n",
    "for ifg_ndx in displacement.index.get_level_values(0).unique():\n",
    "    fig, ax = plt.subplots(figsize = (8,8))\n",
    "    img1 = ax.imshow(insar_displacement[ifg_ndx],\n",
    "                     cmap=cmap_obj, vmin=vmin, vmax=vmax, interpolation='nearest',\n",
    "                     extent=(W, E, S, N))\n",
    "    ax.set_title(f\"{ifgs_date[ifg_ndx,0].strftime('%Y%m%d')}-{ifgs_date[ifg_ndx,1].strftime('%Y%m%d')}\")\n",
    "    cbar1 = fig.colorbar(img1, ax=ax, orientation='horizontal')\n",
    "    cbar1.set_label('LOS displacement [mm]')\n",
    "\n",
    "    for site_name in displacement.loc[ifg_ndx].index:\n",
    "        lon, lat = displacement.loc[(ifg_ndx, site_name), 'lon'], displacement.loc[(ifg_ndx, site_name), 'lat']\n",
    "        color = cmap((displacement.loc[(ifg_ndx, site_name), 'gnss_disp']-vmin)/(vmax-vmin))\n",
    "        ax.scatter(lon, lat, s=8**2, color=color, edgecolors='k')\n",
    "        ax.annotate(site_name, (lon,lat), color='black')\n",
    "\n",
    "    # Append figure to list\n",
    "    gnss_insar_figs.append(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='validation1'></a>\n",
    "## 5. Validation Method 1: GNSS-InSAR Direct Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_pair1'></a>\n",
    "### 5.1, 5.2. Make GNSS-InSAR Velocity Residuals at GNSS Station Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first pair up all GNSS stations and compare the relative measurement from both GNSS and InSAR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dictionaries for GNSS and InSAR measurements, etc.\n",
    "insar_disp = {}\n",
    "gnss_disp = {}\n",
    "ddiff_dist = {}\n",
    "ddiff_disp = {}\n",
    "abs_ddiff_disp = {}\n",
    "\n",
    "# Define ellipsoid for distance calculation\n",
    "geod = pyproj.Geod(ellps=\"WGS84\")\n",
    "\n",
    "# Loop through interferograms\n",
    "for ifg_ndx in displacement.index.get_level_values(0).unique():\n",
    "    displacement_i = displacement.loc[ifg_ndx]\n",
    "    insar_disp_i = []\n",
    "    gnss_disp_i = []\n",
    "    ddiff_dist_i = []\n",
    "    ddiff_disp_i = []\n",
    "\n",
    "    # Loop through site pairs\n",
    "    for sta1 in displacement_i.index:\n",
    "        for sta2 in displacement_i.index:\n",
    "            if sta2 == sta1:\n",
    "                break\n",
    "\n",
    "            # Compute InSAR and GNSS displacement residuals\n",
    "            insar_disp_i.append(displacement_i.loc[sta1, 'insar_disp'] \\\n",
    "                                - displacement_i.loc[sta2, 'insar_disp'])\n",
    "            gnss_disp_i.append(displacement_i.loc[sta1, 'gnss_disp'] \\\n",
    "                               - displacement_i.loc[sta2, 'gnss_disp'])\n",
    "\n",
    "            # Compute double-difference residual\n",
    "            ddiff_disp_i.append(gnss_disp_i[-1] - insar_disp_i[-1])\n",
    "\n",
    "            # Compute distance between sites\n",
    "            _, _, distance = geod.inv(displacement_i.loc[sta1,'lon'], displacement_i.loc[sta1,'lat'],\n",
    "                                      displacement_i.loc[sta2,'lon'], displacement_i.loc[sta2,'lat'])\n",
    "            distance = distance / 1000  # convert unit from m to km\n",
    "\n",
    "            # Record double difference\n",
    "            ddiff_dist_i.append(distance)\n",
    "\n",
    "    # Record all site-to-site values within an IFG\n",
    "    insar_disp[ifg_ndx] = np.array(insar_disp_i)\n",
    "    gnss_disp[ifg_ndx] = np.array(gnss_disp_i)\n",
    "    ddiff_dist[ifg_ndx] = np.array(ddiff_dist_i)\n",
    "    ddiff_disp[ifg_ndx] = np.array(ddiff_disp_i)\n",
    "    abs_ddiff_disp[ifg_ndx] = abs(np.array(ddiff_disp_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Absolute Displacement Residuals As a Function of Distance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_bin1'></a>\n",
    "## 5.3 Validate Requirement Based on Binned Measurement Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set requirement thresholds\n",
    "transient_distance_rqmt = (0.1, 50)  # distances for evaluation\n",
    "transient_threshold_rqmt = lambda L: 3 * (1 + np.sqrt(L))  # coseismic threshold in mm\n",
    "\n",
    "n_bins = 10  # number of distance bins for analysis\n",
    "threshold = 0.683  # fraction of Gaussian normal distribution for pass/fail\n",
    "\n",
    "# Loop through interferograms\n",
    "method1_validation_figs = []\n",
    "site_loc = sitedata['sites'][site]['calval_location']\n",
    "for ifg_ndx in displacement.index.get_level_values(0).unique():\n",
    "    # Start and end dates as strings\n",
    "    start_date = ifgs_date[ifg_ndx,0].strftime('%Y%m%d')\n",
    "    end_date = ifgs_date[ifg_ndx,1].strftime('%Y%m%d')\n",
    "\n",
    "    # Validation figure and assessment\n",
    "    _, validation_fig_method1 = display_transient_validation(ddiff_dist[ifg_ndx], abs_ddiff_disp[ifg_ndx],\n",
    "                                                             site_loc, start_date, end_date,\n",
    "                                                             requirement=transient_threshold_rqmt,\n",
    "                                                             distance_rqmt=transient_distance_rqmt,\n",
    "                                                             n_bins=n_bins,\n",
    "                                                             threshold=threshold,\n",
    "                                                             sensor='Sentinel-1',\n",
    "                                                             validation_type=requirement.lower(),\n",
    "                                                             validation_data='GNSS')\n",
    "    method1_validation_figs.append(validation_fig_method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat double differences as list\n",
    "ddiff_dist_ap1 = list(ddiff_dist.values())\n",
    "abs_ddiff_disp_ap1 = list(abs_ddiff_disp.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got all needed data for approach 1:\n",
    "- `ddiff_dist_ap1`: distance of GNSS pairs,\n",
    "- `abs_ddiff_disp_ap1`: absolute value of measurement redisuals,\n",
    "- `ifgs_date_ap1`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of interferograms\n",
    "n_ifgs = len(ddiff_dist_ap1)\n",
    "print(f\"Analyzing {n_ifgs} interferograms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin all measurement residuals to check if they pass the requirements or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins over distance requirement\n",
    "n_bins = 10\n",
    "bins = np.linspace(0.1, 50.0, num=n_bins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-allocate array for number of points for each IFG and bins\n",
    "n_all = np.empty([n_ifgs, n_bins+1], dtype=int)\n",
    "\n",
    "# Pre-allocate array for number of points that pass based on requirement\n",
    "n_pass = np.empty([n_ifgs,n_bins+1], dtype=int)\n",
    "\n",
    "# Loop through interferograms\n",
    "for i in range(n_ifgs):\n",
    "    # Determine bin indices\n",
    "    inds = np.digitize(ddiff_dist_ap1[i], bins)\n",
    "\n",
    "    # Loop through bins\n",
    "    for j in range(1,n_bins+1):\n",
    "        # Evaluate requirement for the i-th IFG and j-th distance bin\n",
    "        rqmt = 3*(1+np.sqrt(ddiff_dist_ap1[i][inds==j]))\n",
    "\n",
    "        # Relative measurement of i-th IFG and j-th distance bin\n",
    "        rem = abs_ddiff_disp_ap1[i][inds==j]\n",
    "        assert len(rqmt) == len(rem)\n",
    "        n_all[i,j-1] = len(rem)\n",
    "        n_pass[i,j-1] = np.count_nonzero(rem<rqmt)\n",
    "\n",
    "    # Total number of residuals\n",
    "    n_all[i,-1] = np.sum(n_all[i,0:-2])\n",
    "\n",
    "    # Number of residuals that pass requirement\n",
    "    n_pass[i,-1] = np.sum(n_pass[i,0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of double-difference residuals that pass requirement\n",
    "ratio = n_pass / n_all\n",
    "\n",
    "# Define threshold of data points in a bin that must pass\n",
    "thresthod = 0.683\n",
    "\n",
    "# The assumed nature of Gaussian distribution gives a probability of 0.683 of being within one standard deviation.\n",
    "success_or_fail = ratio > thresthod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_result1'></a>\n",
    "## Result visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the result to pandas DataFrame for better visulization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(x:bool):\n",
    "    if x==True:\n",
    "        return 'true '\n",
    "    elif x==False:\n",
    "        return 'false '\n",
    "\n",
    "success_or_fail_str = [list(map(to_str, x)) for x in success_or_fail]\n",
    "\n",
    "columns = []\n",
    "for i in range(n_bins):\n",
    "    columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "columns.append('total')\n",
    "\n",
    "index = []\n",
    "for i in range(len(ifgs_date_ap1)):\n",
    "    index.append(ifgs_date_ap1[i,0].strftime('%Y%m%d')+'-'+ifgs_date_ap1[i,1].strftime('%Y%m%d'))\n",
    "\n",
    "n_all_pd = pd.DataFrame(n_all,columns=columns,index=index)\n",
    "n_pass_pd = pd.DataFrame(n_pass,columns=columns,index=index)\n",
    "ratio_pd = pd.DataFrame(ratio,columns=columns,index=index)\n",
    "success_or_fail_pd = pd.DataFrame(success_or_fail_str,columns=columns,index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_all_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points that below the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_pass_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stylized pandas table\n",
    "validation_table_method1 = ratio_pd.style\n",
    "validation_table_method1.set_table_styles([  # create internal CSS classes\n",
    "    {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "    {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "], overwrite=False)\n",
    "validation_table_method1.set_td_classes(success_or_fail_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_conclusion1'></a>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = np.count_nonzero(ratio_pd['total'] > thresthod) / n_ifgs\n",
    "\n",
    "method_summary = f\"Percentage of interferograms passes the requirement: {percentage}\"\n",
    "\n",
    "if percentage >= 0.70:\n",
    "    method_summary += \"\\nThe interferogram stack passes the requirement.\"\n",
    "else:\n",
    "    method_summary += \"\\nThe interferogram stack fails the requirement.\"\n",
    "\n",
    "print(method_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Method 1 results to file\n",
    "run_date = dt.now().strftime('%Y%m%dT%H%M%S')\n",
    "save_fldr = f\"{run_date}-Transient-Method1\"\n",
    "save_dir = os.path.join(mintpy_dir, save_fldr)\n",
    "\n",
    "save_params = {\n",
    "    'save_dir': save_dir,\n",
    "    'run_date': run_date,\n",
    "    'requirement': requirement,\n",
    "    'site': site,\n",
    "    'method': '1',\n",
    "    'sitedata': sitedata['sites'][site],\n",
    "    'gnss_insar_figs': gnss_insar_figs,\n",
    "    'validation_figs': method1_validation_figs,\n",
    "    'validation_table': validation_table_method1,\n",
    "    'summary': method_summary\n",
    "}\n",
    "save_results(**save_params)\n",
    "\n",
    "# Save the report in the home directory as well\n",
    "save_params['save_dir'] = home_dir\n",
    "save_results(**save_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Approach 1 final result for CentralValleyA144: around 79% of interferograms passes the requirement.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "<a id='transient_validation2'></a>\n",
    "## 6. Validation Approach 2: Noise Level Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this validation (Approach #2), we evaluate the estimated secular deformation rate (Requirements 658) or co-seismic displacement (Requirement 660) from time series processing or the individual unwrapped interferogram (Requirement 663) over selected cal/val areas with negligible deformation. Any estimated deformation should thus be treated as noise and our goal is to evaluate the significance of this noise. In general, noise in the modeled displacement or the unwrapped interferogram is anisotropic, but here we neglect this anisotropy. Also, we assume the noise is stationary.\n",
    "\n",
    "We first randomly sample measurements and pair up sampled pixel measurements. For each pixel-pair, the difference of their measurement becomes:\n",
    "$$d\\left(r\\right)=|(f\\left(x\\right)-f\\left(x-r\\right))|$$\n",
    "Estimates of $d(r)$ from all pairs are binned according to the distance r. In each bin, $d(r)$ is assumed to be a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Now we simply assume there is no deformation in this study area and time interval. But in fact, it is hard to find a enough large area without any deformation. An more realistic solution is to apply a mask to mask out deformed regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Read InSAR Array and Mask Pixels with no Data <a id='array_mask'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataset-layer names as lists\n",
    "unwrapPhaseName = [f\"unwrapPhase-{date[0].strftime('%Y%m%d')}_{date[1].strftime('%Y%m%d')}\"\n",
    "                   for date in ifgs_date]\n",
    "\n",
    "# Read unwrapped phase from selected interferograms\n",
    "ifgs_unw, insar_metadata = readfile.read(ifgs_file, datasetName=unwrapPhaseName)\n",
    "\n",
    "# Convert phase to displacement in m and switch convention to positive range decrease\n",
    "insar_displacement = -ifgs_unw*float(insar_metadata['WAVELENGTH']) / (4*np.pi)\n",
    "\n",
    "# Convert displacement units from m to mm\n",
    "insar_displacement = insar_displacement * 1000.\n",
    "\n",
    "# Read 2D mask array\n",
    "msk, _ = readfile.read(msk_file, datasetName=\"waterMask\")\n",
    "\n",
    "# Repeat mask array for each interferogram\n",
    "msk = np.stack([msk] * insar_displacement.shape[0], axis=0)\n",
    "    \n",
    "# Set masked pixels to NaN\n",
    "insar_displacement[msk == 0] = np.nan\n",
    "insar_displacement[insar_displacement==0.0] = np.nan\n",
    "\n",
    "# Clean up phase-only IFGs to avoid future confusion\n",
    "del ifgs_unw\n",
    "\n",
    "# Define number of interferograms\n",
    "n_ifgs = len(ddiff_dist_ap1)\n",
    "print(f\"Analyzing {n_ifgs} interferograms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask Pixels with Low Coherence (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insar_displacement[insar_coherence <0.6] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coherence and InSAR measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap_obj = copy.copy(plt.get_cmap('gray'))\n",
    "\n",
    "for ifg_ndx in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.imshow(insar_coherence[ifg_ndx], cmap=cmap_obj, interpolation='nearest', extent=(W, E, S, N))\n",
    "    ax.set_title(f\"Coherence \"\n",
    "                 f\"\\n Date range {ifgs_date[ifg_ndx,0].strftime('%Y%m%d')}-{ifgs_date[ifg_ndx,1].strftime('%Y%m%d')}\")\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('coherence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap_obj = copy.copy(plt.get_cmap(cmap))\n",
    "\n",
    "for ifg_ndx in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.imshow(insar_displacement[ifg_ndx], cmap=cmap_obj, interpolation='nearest', extent=(W, E, S, N))\n",
    "    ax.set_title(f\"Interferogram \"\n",
    "                 f\"\\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('LOS displacement [mm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_pair2'></a>\n",
    "### 6.2. Randomly Sample Pixels and Pair Them Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coordinate for every pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X0,Y0 = load_geo(insar_metadata)\n",
    "X0_2d, Y0_2d = np.meshgrid(X0, Y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each interferogram, randomly selected pixels need to be paired up. In order to keep measurements independent, different pixel pairs can not share same pixel. This is achieved by pairing up in sequence, i.e., pairing up pixel number 1 and number 2, 3 and 4..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the distance and phase difference between site pairs\n",
    "dist = []; rel_measure = []\n",
    "for ifg_ndx in range(n_ifgs):\n",
    "    dist_i, rel_measure_i = samp_pair(X0_2d, Y0_2d, insar_displacement[ifg_ndx], num_samples=1000000)\n",
    "    dist.append(dist_i)\n",
    "    rel_measure.append(rel_measure_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the statistical property of selected pixel pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot histogram of distances between pixel pairs\n",
    "for ifg_ndx in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(dist[ifg_ndx], bins=100)\n",
    "    ax.set_title(f\"Histogram of distance \"\n",
    "                 f\"\\n Date range {ifgs_date[ifg_ndx,0].strftime('%Y%m%d')}-{ifgs_date[ifg_ndx,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_xlabel(r'Distance ($km$)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlim(0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot histogram of relative measurements\n",
    "for ifg_ndx in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(rel_measure[ifg_ndx], bins=100)\n",
    "    ax.set_title(f\"Histogram of Relative Measurement \"\n",
    "                 f\"\\n Date range {ifgs_date[ifg_ndx,0].strftime('%Y%m%d')}-{ifgs_date[ifg_ndx,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_xlabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got data used of approach 2:\n",
    "- `dist`: distance of pixel pairs,\n",
    "- `rel_measure`: relative measurement of pixel pairs,\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_bin2'></a>\n",
    "### 6.3. Validate the requirement based on binned measurement residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of interferograms\n",
    "n_ifgs = len(ddiff_dist_ap1)\n",
    "print(f\"Analyzing {n_ifgs} interferograms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin all measurement residuals to check if they pass the requirements or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins over distance requirement\n",
    "n_bins = 10\n",
    "bins = np.linspace(0.1, 50.0, num=n_bins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points for each ifgs and bins\n",
    "n_all = np.empty([n_ifgs, n_bins+1], dtype=int)\n",
    "\n",
    "# Number of points pass\n",
    "n_pass = np.empty([n_ifgs,n_bins+1], dtype=int)\n",
    "\n",
    "# Loop through interferograms\n",
    "for i in range(n_ifgs):\n",
    "    # Determine bin indices\n",
    "    inds = np.digitize(dist[i], bins)\n",
    "\n",
    "    # Loop through bins\n",
    "    for j in range(1, n_bins+1):\n",
    "        # Evaluate requirement for the i-th IFG and j-th distance bin\n",
    "        rqmt = 3*(1+np.sqrt(dist[i][inds==j]))  # mission requirement for i-th ifgs and j-th bins\n",
    "\n",
    "        # Relative measurement of i-th IFG and j-th distance bin\n",
    "        rem = rel_measure[i][inds==j] # relative measurement\n",
    "        assert len(rqmt) == len(rem)\n",
    "        n_all[i,j-1] = len(rem)\n",
    "        n_pass[i,j-1] = np.count_nonzero(rem<rqmt)\n",
    "\n",
    "    # Total number of residuals\n",
    "    n_all[i,-1] = np.sum(n_all[i,0:-2])\n",
    "\n",
    "    # Number of residuals that pass requirement\n",
    "    n_pass[i,-1] = np.sum(n_pass[i,0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of sample pairs that pass requirement\n",
    "ratio = n_pass / n_all\n",
    "mean_ratio = np.array([np.mean(ratio[:,:-1],axis=1)])\n",
    "ratio = np.hstack((ratio,mean_ratio.T))\n",
    "\n",
    "# Define threshold of data points in a bin that must pass\n",
    "thresthod = 0.683\n",
    "\n",
    "#The assumed nature of Gaussian distribution gives a probability of 0.683 of being within one standard deviation.\n",
    "success_or_fail = ratio > thresthod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set requirement thresholds\n",
    "transient_distance_rqmt = (0.1, 50)  # distances for evaluation\n",
    "transient_threshold_rqmt = lambda L: 3 * (1 + np.sqrt(L))  # coseismic threshold in mm\n",
    "\n",
    "n_bins = 10  # number of distance bins for analysis\n",
    "threshold = 0.683  # fraction of Gaussian normal distribution for pass/fail\n",
    "\n",
    "\n",
    "# Loop through interferograms\n",
    "method2_validation_figs = []\n",
    "for ifg_ndx in displacement.index.get_level_values(0).unique():\n",
    "    # Start and end dates as strings\n",
    "    start_date = ifgs_date[ifg_ndx,0].strftime('%Y%m%d')\n",
    "    end_date = ifgs_date[ifg_ndx,1].strftime('%Y%m%d')\n",
    "\n",
    "    # Validation figure and assessment\n",
    "    _, validation_fig_method2 = display_transient_validation(dist[ifg_ndx], rel_measure[ifg_ndx],\n",
    "                                 site, start_date, end_date,\n",
    "                                 requirement=transient_threshold_rqmt,\n",
    "                                 distance_rqmt=transient_distance_rqmt,\n",
    "                                 n_bins=n_bins,\n",
    "                                 threshold=threshold,\n",
    "                                 sensor='Sentinel-1',\n",
    "                                 validation_type=requirement.lower(),\n",
    "                                 validation_data='GNSS')\n",
    "\n",
    "    method2_validation_figs.append(validation_fig_method2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_result2'></a>\n",
    "## Result visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the result to pandas DataFrame for better visulization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format evaluation success/failure as string\n",
    "def to_str(x:bool):\n",
    "    if x==True:\n",
    "        return 'true '\n",
    "    elif x==False:\n",
    "        return 'false '\n",
    "\n",
    "success_or_fail_str = [list(map(to_str, x)) for x in success_or_fail]\n",
    "\n",
    "columns = []\n",
    "for i in range(n_bins):\n",
    "    columns.append(f\"{bins[i]:.2f}-{bins[i+1]:.2f}\")\n",
    "columns.append('total')\n",
    "\n",
    "index = []\n",
    "for i in range(len(ifgs_date)):\n",
    "    index.append(f\"{ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "\n",
    "n_all_pd = pd.DataFrame(n_all, columns=columns, index=index)\n",
    "n_pass_pd = pd.DataFrame(n_pass, columns=columns, index=index)\n",
    "ratio_pd = pd.DataFrame(ratio, columns=columns+['mean'], index=index)\n",
    "success_or_fail_pd = pd.DataFrame(success_or_fail_str, columns=columns+['mean'], index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_all_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points that below the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_pass_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stylized pandas table\n",
    "validation_table_method2 = ratio_pd.style\n",
    "validation_table_method2.set_table_styles([  # create internal CSS classes\n",
    "    {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "    {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "], overwrite=False)\n",
    "validation_table_method2.set_td_classes(success_or_fail_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_conclusion2'></a>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with percentage of total passed pairs, the mean value of percentage of passed pairs in all bin is a better indicator since it gives all bins same weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = np.count_nonzero(ratio_pd['mean'] > thresthod) / n_ifgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of interferograms passes the requirement (70%): {percentage}.\")\n",
    "if percentage >= 0.70:\n",
    "    print('The interferogram stack passes the requirement.')\n",
    "else:\n",
    "    print('The interferogram stack fails the requirement.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Method 2 results to file\n",
    "save_fldr = f\"{dt.now().strftime('%Y%m%dT%H%M%S')}-Transient-Method2\"\n",
    "save_dir = os.path.join(mintpy_dir, save_fldr)\n",
    "\n",
    "save_params = {\n",
    "    'save_dir': save_dir,\n",
    "    'run_date': run_date,\n",
    "    'requirement': requirement,\n",
    "    'site': site,\n",
    "    'method': '2',\n",
    "    'sitedata': sitedata['sites'][site],\n",
    "    'gnss_insar_figs': gnss_insar_figs,\n",
    "    'validation_figs': method2_validation_figs,\n",
    "    'validation_table': validation_table_method2,\n",
    "    'summary': method_summary\n",
    "}\n",
    "save_results(**save_params)\n",
    "\n",
    "# Save the report in the home directory as well\n",
    "save_params['save_dir'] = home_dir\n",
    "save_results(**save_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Approach 2 final result for CentralValleyA144: 100% of interferograms passes the requirement.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_appendix'></a>\n",
    "# Appendix: InSAR and GNSS Position Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative position in LOS direction for all GNSS stations are plotted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through interferograms\n",
    "for ifg_ndx in displacement.index.get_level_values(0).unique():\n",
    "    # Initialize figure\n",
    "    plt.figure(figsize=(11,7))\n",
    "\n",
    "    # Define range of displacement values\n",
    "    disp_range = (min([*insar_disp[ifg_ndx],*gnss_disp[ifg_ndx]]), max([*insar_disp[ifg_ndx],*gnss_disp[ifg_ndx]]))\n",
    "\n",
    "    # Plot histograms of InSAR and GNSS displacement values\n",
    "    plt.hist(insar_disp[ifg_ndx], bins=100, range=disp_range, color = \"green\", label='D_InSAR')\n",
    "    plt.hist(gnss_disp[ifg_ndx], bins=100, range=disp_range, color=\"orange\", label='D_GNSS', alpha=0.5)\n",
    "\n",
    "    # Format figure\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Displacements \"\n",
    "              f\"\\n Date range {ifgs_date[ifg_ndx,0].strftime('%Y%m%d')}-{ifgs_date[ifg_ndx,1].strftime('%Y%m%d')} \"\n",
    "              f\"\\n Number of station pairs used: {len(insar_disp[ifg_ndx])}\")\n",
    "    plt.xlabel('LOS Displacement (mm)')\n",
    "    plt.ylabel('Number of Station Pairs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through interferograms\n",
    "for ifg_ndx in displacement.index.get_level_values(0).unique():\n",
    "    # Initialize figure\n",
    "    plt.figure(figsize=(11,7))\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.hist(ddiff_disp[ifg_ndx], bins=100, color='darkblue', linewidth=1, label='D_gnss - D_InSAR')\n",
    "\n",
    "    # Format figure\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Residuals\"\n",
    "              f\"\\n Date range {ifgs_date[ifg_ndx,0].strftime('%Y%m%d')}-{ifgs_date[ifg_ndx,1].strftime('%Y%m%d')}\"\n",
    "              f\"\\n Number of stations pairs used: {len(ddiff_disp[i])}\")\n",
    "    plt.xlabel('Displacement Residual (mm)')\n",
    "    plt.ylabel('N Stations')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through interferograms\n",
    "for ifg_ndx in displacement.index.get_level_values(0).unique():\n",
    "    # Initialize figure\n",
    "    plt.figure(figsize=(11,7))\n",
    "\n",
    "    # Draw distance threshold\n",
    "    dist_th = np.linspace(min(ddiff_dist[ifg_ndx]), max(ddiff_dist[ifg_ndx]),100)\n",
    "    acpt_error = 3*(1+np.sqrt(dist_th))\n",
    "\n",
    "    # Plot residuals\n",
    "    plt.scatter(ddiff_dist[ifg_ndx], abs_ddiff_disp[ifg_ndx], s=1)\n",
    "    plt.plot(dist_th, acpt_error, 'r')\n",
    "\n",
    "    # Format plot\n",
    "    plt.xlabel(\"Distance (km)\")\n",
    "    plt.ylabel(\"Amplitude of Displacement Residuals (mm)\")\n",
    "    plt.title(f\"Residuals \"\n",
    "              f\"\\n Date range {ifgs_date[ifg_ndx,0].strftime('%Y%m%d')}-{ifgs_date[ifg_ndx,1].strftime('%Y%m%d')} \"\n",
    "              f\"\\n Number of stations pairs used: {len(ddiff_dist[ifg_ndx])}\")\n",
    "    plt.legend([\"Measurement\", \"Mission Reqiurement\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through interferograms\n",
    "for ifg_ndx in range(insar_displacement.shape[0]):\n",
    "    # Define start and end dates\n",
    "    start_time_str = ifgs_date[ifg_ndx,0].strftime('%Y%m%d')\n",
    "    end_time_str = ifgs_date[ifg_ndx,1].strftime('%Y%m%d')\n",
    "\n",
    "    # Loop through stations in IFG\n",
    "    for site_name in gnss_time_series[ifg_ndx].columns:\n",
    "        print(f\"Plotting GPS postion from {start_time_str} to {end_time_str} at station: {site_name}\")\n",
    "\n",
    "        # Retrieve GNSS time-series\n",
    "        series = gnss_time_series[ifg_ndx, site_name]\n",
    "\n",
    "        # Initialize figure\n",
    "        plt.figure(figsize=(15,5))\n",
    "\n",
    "        # Plot time-series data\n",
    "        plt.scatter(pd.date_range(start=ifgs_date[ifg_ndx,0], end=ifgs_date[ifg_ndx,1]),series)\n",
    "\n",
    "        # Foramt figure\n",
    "        plt.title(f\"station name: {site_name}\")\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Relative position in LOS direction (mm)')\n",
    "\n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(work_dir, f\"{start_time_str}_{end_time_str}_{site_name}.jpg\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.local-solid_earth_atbd_dev]",
   "language": "python",
   "name": "conda-env-.local-solid_earth_atbd_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
