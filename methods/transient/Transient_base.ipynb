{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Algorithm Theoretical Basis Document: Algorithms to Validate NISAR L2 Transient Displacement Requirements\n",
    "\n",
    "**Original code authored by:** NISAR Science Team Members and Affiliates  \n",
    "\n",
    "*March 08, 2022*\n",
    "\n",
    "*NISAR Solid Earth Team*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr/>\n",
    "\n",
    "## 1.Transient Deformation\n",
    "Detecting and quantifying transient deformation is essential for improving our understanding of fundamental processes associated with tectonics, subsurface movement of magma, volcanic eruptions, landslides, response to changing surface loads and a wide variety of anthropogenic phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Theoretical Basis of Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Requirements\n",
    "**L2 Requirement 663 – Deformation Transients:** *The NISAR project shall measure at least two components of the point-to-point vector displacements over at least 70% of targeted sites with accuracy of $3(1+ L^{1/2})$ mm or better, over length scales 0.1 km < L < 50 km, at 100 m resolution, and over 12-day time scales.*\n",
    "\n",
    "NISAR must be able to constrain displacements in two look directions (left-looking on both ascending and descending tracks) for the active area of 70% of target sites with an accuracy that scales with baseline distance L between any two locations within a scene. This pertains to all directly measured or inferred 12-day interferograms over the duration of the mission. Here, accuracy is calculated using L in kilometers but with the units removed, at or better than 100 m resolution. The 12-day time scale corresponds to a repeat NISAR pass on each of the ascending and descending satellite tracks, from which interferograms can be made and displacements estimated. The NISAR mission has compiled a list of 2000 global targets covering areas of known or potential transient deformation related to the processes specified in the requirement (NISAR Handbook Appendix H, 2018). These targets include all Earth's active volcanoes above sea-level, areas of rapid glacial mass changes, selected deforming reservoirs of water, oil, gas, CO2 and steam and landslide-prone areas near major population centers, as well as sites where selected disaster-related events have occurred. \n",
    "\n",
    "### 2.2 Approach to validating the L2 requirements\n",
    "We use three approaches for validating the NISAR Solid Earth Science L2 requirements. All approaches require L2 data inputs in the form of unwrapped NISAR interferograms over calibration/validation (cal/val) targets that sample a range of vegetation types, topographic relief, and strain rates.  Requirement 663 requires individual interferograms only. Requirements 658 and 660 require a set of temporally contiguous/overlapping SAR interferograms over all time periods of interest, from which deformation timeseries can be generated (see description of inputs and potential preprocessing steps in Sections 3 and 5). \n",
    "\n",
    "For Validation Approach #1, we compare InSAR-derived surface displacements with point observations of surface motion from collocated continuous GNSS stations (hereafter we refer to continuous GNSS simply as “GNSS”). Since all requirements are written in terms of relative displacements between individual points within the deformation field, comparisons are done using the differences of observed surface motion (from both InSAR and GNSS) between GNSS station locations. For a GNSS station network of N stations located within an interferogram, this will yield N(N-1)/2 distinct observations for comparison, distributed across a range of length scales. As we discuss below, the methodology differs slightly depending on whether we perform our comparison using displacements from single interferograms (Requirement 663) or using basis functions fit to deformation time series derived from many interferograms (Requirements 658, 660), but the underlying premise is the same: that GNSS provides a sufficiently high-quality time series to validate InSAR observations. This approach is appropriate where measurable displacements are occurring across the cal/val region and the GNSS network is sufficiently dense to capture most of the expected spatial variability of the signal. \n",
    "\n",
    "For Validation Approach #2, which is appropriate for negligibly deforming regions, we examine the NISAR interferograms without comparison to GNSS, under the assumption that surface deformation is essentially zero at all relevant spatial scales. This method involves computing InSAR displacement observations at a statistically significant number of randomly chosen interferogram pixels and confirming that the estimates are consistent with there being no deformation within the scene.\n",
    "\n",
    "For Validation Approach #3, which can be blended with the first two approaches, we use UAVSAR to validate the InSAR-derived motions at scales smaller than the characteristic spacing between the GNSS stations. UAVSAR has the advantage of filling in the spatial sampling between GNSS stations at a resolution approximately 10X higher than the resolution of NISAR images. Ideally, UAVSAR data will be collected during NISAR passes. Corner reflectors will further validate the accuracy and can be used to assess UAVSAR motion compensation errors. Assessment of UAVSAR motion compensation errors should be carried out prior to NISAR launch from experiments using corner reflectors that are moved between passes. **Note: this ATBD does not cover implementation of Validation Approach #3**\n",
    "\n",
    "All the Solid Earth Science requirements specify a minimum spatial coverage component, whose validation will rely on a combination of assessing the coverage of InSAR-quality data and ensuring that the required measurement accuracy is achieved in a suite of locations that comprehensively sample different types of regions with respect to surface properties and vegetation land cover. Many of these regions will be automatically evaluated as part of the targeted sites for the transient deformation requirement.\n",
    "\n",
    "#### 2.2.3.\tL2 Requirement 663 - Transient Displacements\n",
    "\n",
    "To validate the L2 requirements on transient displacements, we will produce 12-day interferograms from both descending and ascending tracks over diverse target sites where GNSS observations are available. The two components of vector displacement, ascending and descending, will be validated separately.\n",
    "\n",
    "For **Validation Approach 1**, we will use unwrapped interferograms at 100-m resolution to produce point-to-point relative LOS measurements (and their associated uncertainties) between GNSS sites. Position observations from the same set of GNSS sites and at the InSAR acquisition times will be projected into the LOS direction and differenced pairwise. These will be compared to the point-to-point InSAR LOS measurements using a methodology similar to that described in Section 2.2.2., except that the accuracy specification is $3(1+L^{1/2})$ mm over 0.1 km < L < 50 km. \n",
    "\n",
    "To validate the noise in individual interferograms using **Validation Approach 2**, we will utilize interferograms over the set of non-deforming sites discussed in Section 2.2.1. In practice, characterization of transient deformation will usually be improved by examining longer time series of interferograms. The approach described here validates the requirement that short timescale or temporally complex transients can be characterized with a single interferogram.\n",
    "\n",
    "**Validation Approach 3** uses measurements from UAVSAR across an area that is well characterized with GNSS stations.  This could be an area of known deformation, such as a creeping section of the San Andreas Fault.\n",
    "\n",
    "Comprehensive validation requires transient sites possessing different deformation characteristics (e.g., volcanoes, landslides, aquifers, hydrocarbons, etc.), vegetation covers (forest, shrub, bare surface, etc.), seasonality (leaf on/off, snow, etc.), and terrain slopes. The NISAR Science Team will select a set of cal/val regions to be used for this requirement and will list those sites in the NISAR cal/val plan.\n",
    "\n",
    "### 2.3.\tTechnical Framework for Validating Requirements\n",
    "#### 2.3.1.\tComparison of GNSS and InSAR measurements\n",
    "The InSAR and GNSS comparisons for Requirement [663] will be performed based on the basis of interferogram by interferogram.\n",
    "\n",
    "#### 2.3.2.\tSpatial Analysis of InSAR scenes\n",
    "Individual interferogram analysis in non-deforming regions will be conducted based on unwrapped interferograms at the required spatial resolutions. We first estimate the covariances or semi-variogram of phase observations between points of varying distances by constructing the structure function (e.g., Lohman & Simons, 2005) (see Section 4.2). We then compare the spatial spectrum of the covariance function to the requirement(s) at distances between 0.1 and 50 km  to validate that the observed noise is smaller than the threshold in the various requirements. We use ensemble statistics over many interferograms and over different terrains and seasons for this validation approach. SES members have done relevant work in the past to validate the NISAR performance tool (Hensley et al., 2016) using the  70-km swath of ALOS interferograms. Before the NISAR launch, we can use ALOS-2 wide-swath or Sentinel-1 scenes to conduct this validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Interferogram Preparation\n",
    "In this initial processing step, all the necessary Level-2 unwrapped interferogram products are gathered. Ascending and descending interferograms will be prepared for independent analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1  Introduction\n",
    "The project will provide sets of ascending and descending unwrapped L2 interferograms over regions of interest listed in the NISAR Solid Earth calval document. For the purpose of testing calval algorithms prior to NISAR launch, the NISAR SE team will make interferograms using SAR data from complementary missions (e.g. Sentinel-1 or ALOS-2). These will include at a minimum nearest-neighbor interferograms.\n",
    "\n",
    "As part of L2 processing, the project will calculate and apply required and optional corrections to minimize errors due to non-geophysical sources. An example of a required correction is the removal of ionospheric propagation delays using split-band processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  Configure Local Processing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Packages\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from solid_utils.sampling import load_geo, samp_pair\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from mintpy.utils import readfile, utils as ut\n",
    "from mintpy.objects import gps\n",
    "from mintpy.objects.gps import GPS\n",
    "from scipy import special\n",
    "from scipy.stats import binned_statistic\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import pyproj\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "#from scipy.spatial.distance import pdist\n",
    "#from itertools import combinations,chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Custom Parameters in the cell below. Users only need to input the following parameters:\n",
    "- `calval_location`: name of the study area (e.g., 'oklahoma');\n",
    "- `download_region`: lat-lon box that falls into the study region (e.g., '\"35.00 36.00 -100.00 -98.00\"');\n",
    "- `sentinel_track`: track number of sentinel-1 data (e.g., '135');\n",
    "- `download_start_date` and `download_end_date`: data in this time interval will be downloaded and analysed (e.g., '20000101', '20230101')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: the `download_start_date` and `download_end_date` are set to download three years of data from 2019 to 2021, users can modified it to select the data they want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calval_location = 'central_valley'\n",
    "download_region = '\"36.18 36.26 -119.91 -119.77\"' #download box in S,N,W,E format\n",
    "sentinel_track = '144'\n",
    "\n",
    "# calval_location = 'texas'\n",
    "# download_region = '\"31.00 33.00 -106.00 -103.00\"' #download box in S,N,W,E format\n",
    "# sentinel_track = '151'\n",
    "\n",
    "# calval_location = 'oklahoma'\n",
    "# download_region = '\"35.00 36.00 -100.00 -98.00\"' #download box in S,N,W,E format\n",
    "# sentinel_track = '107'\n",
    "\n",
    "# calval_location = 'purtorico'\n",
    "# download_region = '\"18.00 18.10 -67.00 -66.00\"' #download box in S,N,W,E format\n",
    "# sentinel_track = '135'\n",
    "\n",
    "download_start_date = '20190101'\n",
    "download_end_date = '20220101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd()\n",
    "calval_dir = cwd/'calval'\n",
    "\n",
    "#Set Directories\n",
    "print(\"calval directory: \", str(calval_dir))\n",
    "calval_dir.mkdir(exist_ok=True)\n",
    "work_dir = calval_dir/calval_location\n",
    "print(\"Work directory: \", str(work_dir))\n",
    "work_dir.mkdir(exist_ok=True)\n",
    "mint_dir = work_dir/'Mintpy'\n",
    "print(\"Mintpy directory: \", str(mint_dir))\n",
    "mint_dir.mkdir(exist_ok=True)\n",
    "gunw_dir = work_dir/'products'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3  Download Interferogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all interferograms that intersect download_region over specified time range and 12 days time intervels by ARIA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(work_dir)\n",
    "command = 'ariaDownload.py --bbox ' + download_region + ' --start ' + download_start_date + ' --end ' + download_end_date + ' --daysless 13 --daysmore 11'\n",
    "if sentinel_track != '':\n",
    "    command = command + ' --track ' + sentinel_track\n",
    "\n",
    "result = subprocess.run(command,capture_output=True,text=True,shell=True)\n",
    "print(result.stdout)\n",
    "\n",
    "#delete unnecessary files\n",
    "(gunw_dir/\"avg_rates.csv\").unlink(missing_ok=True)\n",
    "(gunw_dir/\"ASFDataDload0.py\").unlink(missing_ok=True)\n",
    "(gunw_dir/\"AvgDlSpeed.png\").unlink(missing_ok=True)\n",
    "(gunw_dir/\"error.log\").unlink(missing_ok=True)\n",
    "(work_dir/\"error.log\").unlink(missing_ok=True)\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up ARIA product and mask data with GSHHS water mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(work_dir)\n",
    "command = 'ariaTSsetup.py -f \"products/*.nc\" --mask Download'\n",
    "\n",
    "result = subprocess.run(command,capture_output=True,text=True,shell=True)\n",
    "print(result.stdout)\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4  Set Up MintPy Configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series analysis is not needed for validating Transient Requirement. We use Mintpy here only for data loading. The configuration file required by Mintpy needs to be created and written into `mint_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_content = 'mintpy.load.processor = aria\\n'\n",
    "config_file_content += 'mintpy.load.unwFile = ../stack/unwrapStack.vrt\\n'\n",
    "config_file_content += 'mintpy.load.corFile = ../stack/cohStack.vrt\\n'\n",
    "config_file_content += 'mintpy.load.connCompFile = ../stack/connCompStack.vrt\\n'\n",
    "config_file_content += 'mintpy.load.demFile = ../DEM/SRTM_3arcsec.dem\\n'\n",
    "config_file_content += 'mintpy.load.incAngleFile = ../incidenceAngle/*.vrt\\n'\n",
    "config_file_content += 'mintpy.load.azAngleFile = ../azimuthAngle/*.vrt\\n'\n",
    "config_file_content += 'mintpy.load.waterMaskFile = ../mask/watermask.msk\\n'\n",
    "config_file_content += 'mintpy.reference.lalo = auto\\n'\n",
    "config_file_content += 'mintpy.topographicResidual.pixelwiseGeometry = no\\n'\n",
    "config_file_content += 'mintpy.troposphericDelay.method = no\\n'\n",
    "config_file_content += 'mintpy.topographicResidual = no\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = mint_dir/(calval_location+'.cfg')\n",
    "config_file.write_text(config_file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5  Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(mint_dir)\n",
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep load_data'\n",
    "result = subprocess.run(command,capture_output=True,text=True,shell=True)\n",
    "print(result.stdout)\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The output of this step is an \"inputs\" directory containing two HDF5 files:\n",
    "- ifgramStack.h5: This file contains 6 dataset cubes (e.g. unwrapped phase, coherence, connected components etc.) and multiple metadata\n",
    "- geometryGeo.h5: This file contains geometrical datasets (e.g., incidence/azimuth angle, masks, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_file = mint_dir/'inputs/ifgramStack.h5'\n",
    "geom_file = mint_dir/'inputs/geometryGeo.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the interferogram has a resolution lower than 100 m, we need multi-look the interferogram phase values before calculating the empirical semivarigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the date of interferograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ifgs_date = readfile.read(ifgs_file,datasetName='date')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ifgs_date = np.empty_like(ifgs_date,dtype=dt)\n",
    "for i in range(ifgs_date.shape[0]):\n",
    "    start_date = ifgs_date[i,0].decode()\n",
    "    end_date = ifgs_date[i,1].decode()\n",
    "    start_date = dt.strptime(start_date, \"%Y%m%d\")\n",
    "    end_date = dt.strptime(end_date, \"%Y%m%d\")\n",
    "    _ifgs_date[i] = [start_date,end_date]\n",
    "    \n",
    "ifgs_date = _ifgs_date\n",
    "del _ifgs_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify independent interferograms (i.e., selected inteferograms do NOT share common dates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_row_index = []\n",
    "i = 0\n",
    "while i<ifgs_date.shape[0]-1:\n",
    "    if ifgs_date[i,1]==ifgs_date[i+1,0]:\n",
    "        del_row_index.append(i+1)\n",
    "        i = i+2\n",
    "    else:\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_date = np.delete(ifgs_date,del_row_index,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the phase and coherence of selected interferograms, geometrical datasets, and attribution of them are loaded into numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrapPhaseName = ['unwrapPhase-'+i[0].strftime('%Y%m%d')+'_'+i[1].strftime('%Y%m%d') for i in ifgs_date]\n",
    "coherenceName = ['coherence-'+i[0].strftime('%Y%m%d')+'_'+i[1].strftime('%Y%m%d') for i in ifgs_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_unw,atr = readfile.read(ifgs_file,datasetName=unwrapPhaseName)\n",
    "insar_displacement = -ifgs_unw*float(atr['WAVELENGTH'])/(4*np.pi)*1000 # unit in mm\n",
    "\n",
    "insar_coherence = readfile.read(ifgs_file,datasetName=coherenceName)[0]\n",
    "del ifgs_unw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change default missing phase values in interferograms from 0.0 to `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_displacement[insar_displacement==0.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Optional interferograms correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Phase distortions related to solid earth and ocean tidal effects as well as those due to temporal variations in the vertical stratification of the atmosphere can be mitigated using the approaches described below. At this point, it is expected that these corrections will not be needed to validate the mission requirements, but they may be used to produce the highest quality data products. Typically, these are applied to the estimated time series product rather than to the individual interferograms, since they are a function of the time of each radar acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.A Solid Earth Tide Correction\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.B Tropospheric Delay Correction\n",
    "Optional atmospheric correction utilizes the PyAPS (Jolivet et al., 2011, Jolivet and Agram, 2012) module within GIAnT (or eventually a merged replacement for GIAnT and MintPy). PyAPS is well documented, maintained and can be freely downloaded. PyAPS is included in GIAnT distribution). PyAPS currently includes support for ECMWF’s ERA-Interim, NOAA’s NARR and NASA’s MERRA weather models. A final selection of atmospheric models to be used for operational NISAR processing will be done during Phase C.\n",
    "\n",
    "[T]ropospheric delay maps are produced from atmospheric data provided by Global Atmospheric Models. This method aims to correct differential atmospheric delay correlated with the topography in interferometric phase measurements. Global Atmospheric Models (hereafter GAMs)... provide estimates of the air temperature, the atmospheric pressure and the humidity as a function of elevation on a coarse resolution latitude/longitude grid. In PyAPS, we use this 3D distribution of atmospheric variables to determine the atmospheric phase delay on each pixel of each interferogram.\n",
    "\n",
    "The absolute atmospheric delay is computed at each SAR acquisition date. For a pixel a_i at an elevation z at acquisition date i, the four surrounding grid points are selected and the delays for their respective elevations are computed. The resulting delay at the pixel a_i is then the bilinear interpolation between the delays at the four grid points. Finally, we combine the absolute delay maps of the InSAR partner images to produce the differential delay maps used to correct the interferograms.\n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.C Topographic Residual Correction \n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Phase deramping is not appplied here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the solid earth tide correction for interferogram is applied, it should also be applied for GNSS observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary summary: we have load all data we need for processing:\n",
    "- `atr`: metadata, including incident angle, longitude and latitude step width, etc;\n",
    "- `insar_displacement`: LOS measurement from InSAR;\n",
    "- `insar_coherence`: coherence value of the interferograms:\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prevent reruning the above preparing again, we save the data into disk. They can be loaded easily next time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(work_dir/'base.pkl','wb') as f:\n",
    "    pickle.dump((atr,insar_displacement,insar_coherence,ifgs_date),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(work_dir/'base.pkl','rb') as f:\n",
    "    atr,insar_displacement,insar_coherence,ifgs_date = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Make GNSS LOS Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Find Collocated GNSS Stations\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get space and time range for searching GNSS station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length, width = int(atr['LENGTH']), int(atr['WIDTH'])\n",
    "lat_step = float(atr['Y_STEP'])\n",
    "lon_step = float(atr['X_STEP'])\n",
    "N = float(atr['Y_FIRST'])\n",
    "W = float(atr['X_FIRST'])\n",
    "S = N+lat_step*(length-1)\n",
    "E = W+lon_step*(width-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_gnss = ifgs_date[0,0]\n",
    "end_date_gnss = ifgs_date[-1,-1]\n",
    "\n",
    "inc_angle = int(float(atr.get('incidenceAngle', None)))\n",
    "az_angle = int(float(atr.get('azimuthAngle', None))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for collocated GNSS stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_names, site_lats, site_lons = gps.search_gps(SNWE=(S,N,W,E), \n",
    "                                                  start_date=start_date_gnss.strftime('%Y%m%d'),\n",
    "                                                  end_date=end_date_gnss.strftime('%Y%m%d'))\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Make GNSS LOS Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the 3-D GNSS observations are projected into LOS direction. The InSAR observations are averaged 3 by 3 near the station positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** the number of pixels used in calculating the averaged phase values at the GPS location depends on the resolution of input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get daily position solutions for GNSS stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(mint_dir)\n",
    "displacement = {}\n",
    "gnss_time_series = {}\n",
    "gnss_time_series_std = {}\n",
    "bad_stn = {}  #stations to toss\n",
    "pixel_radius = 1   #number of InSAR pixels to average for comparison with GNSS\n",
    "\n",
    "for counter,stn in enumerate(site_names):\n",
    "    gps_obj = GPS(site = stn, data_dir = str(mint_dir/'GPS'))\n",
    "    gps_obj.open()\n",
    "        \n",
    "    # count number of dates in time range\n",
    "    gps_obj.read_displacement\n",
    "    dates = gps_obj.dates\n",
    "    for i in range(insar_displacement.shape[0]):\n",
    "        start_date = ifgs_date[i,0]\n",
    "        end_date = ifgs_date[i,-1]\n",
    "        \n",
    "        range_days = (end_date - start_date).days\n",
    "        gnss_count = np.histogram(dates, bins=[start_date,end_date])\n",
    "        gnss_count = int(gnss_count[0])\n",
    "        #print(gnss_count)\n",
    "\n",
    "        # select GNSS stations based on data completeness, here we hope to select stations with data frequency of 1 day and no interruption\n",
    "        if range_days == gnss_count-1:\n",
    "        #if start_date in dates and end_date in dates:\n",
    "            _,disp_gnss_time_series,disp_gnss_time_series_std,site_latlon,_ = gps_obj.read_gps_los_displacement(atr,\n",
    "                                                                                                                    start_date=start_date.strftime('%Y%m%d'),\n",
    "                                                                                                                    end_date=end_date.strftime('%Y%m%d'))\n",
    "            x_value = round((site_latlon[1] - W)/lon_step)\n",
    "            y_value = round((site_latlon[0] - N)/lat_step)\n",
    "\n",
    "            #displacement from insar observation in the gnss station, averaged\n",
    "            #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "            disp_insar = insar_displacement[i,\n",
    "                                            y_value-pixel_radius:y_value+pixel_radius, \n",
    "                                            x_value-pixel_radius:x_value+pixel_radius]\n",
    "            if np.isfinite(disp_insar).sum() == 0:\n",
    "                break\n",
    "            disp_insar = np.nanmean(disp_insar)\n",
    "\n",
    "            disp_gnss_time_series = disp_gnss_time_series*1000 # convert unit from meter to mm\n",
    "            disp_gnss_time_series_std = disp_gnss_time_series_std*1000\n",
    "            gnss_time_series[(i,stn)] = disp_gnss_time_series\n",
    "            gnss_time_series_std[(i,stn)] = disp_gnss_time_series_std\n",
    "            displacement[(i,stn)] = list(site_latlon)\n",
    "            disp_gnss = disp_gnss_time_series[-1] - disp_gnss_time_series[0]\n",
    "\n",
    "            displacement[(i,stn)].append(disp_gnss)\n",
    "            displacement[(i,stn)].append(disp_insar)\n",
    "        else:\n",
    "            try:\n",
    "                bad_stn[i].append(stn)\n",
    "            except:\n",
    "                bad_stn[i] = [stn]\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some data structure transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series = dict(sorted(gnss_time_series.items()))\n",
    "gnss_time_series_std = dict(sorted(gnss_time_series_std.items()))\n",
    "displacement = dict(sorted(displacement.items()))\n",
    "bad_stn = dict(sorted(bad_stn.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series = pd.DataFrame.from_dict(gnss_time_series)\n",
    "gnss_time_series_std = pd.DataFrame.from_dict(gnss_time_series_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement = pd.DataFrame.from_dict(displacement,orient='index',\n",
    "                                      columns=['lat','lon','gnss_disp','insar_disp'])\n",
    "displacement.index = pd.MultiIndex.from_tuples(displacement.index,names=['ifg index','station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are less than 3 GNSS stations, don't conduct comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_index = []\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    if len(displacement.loc[i]) < 3:\n",
    "        drop_index.append(i)\n",
    "displacement=displacement.drop(drop_index)\n",
    "# ifgs_date after drop for approach 1\n",
    "ifgs_date_ap1=np.delete(ifgs_date,drop_index,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "- `Mintpy get_los_displacement()` only use the central incidence angle which may involve noticable bias for large area. We next will use pixel-dependent look vector.\n",
    "- A more general critterion is needed for GNSS station selection. Here the stations with uninterrupted data are selected while, in Secular Requirement Validation, stations are selected by data completeness and standard variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Make GNSS and InSAR Relative Displacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I select ref site randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference GNSS stations to GNSS reference site\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    gps_ref_site_name = random.choice(displacement.loc[i].index.unique())\n",
    "    displacement.loc[i,'gnss_disp'] = displacement.loc[i,'gnss_disp'].values - displacement.loc[(i,gps_ref_site_name),'gnss_disp']\n",
    "    displacement.loc[i,'insar_disp'] = displacement.loc[i,'insar_disp'].values - displacement.loc[(i,gps_ref_site_name),'insar_disp']\n",
    "    ref_x_value = round((displacement.loc[(i,gps_ref_site_name),'lon'] - W)/lon_step)\n",
    "    ref_y_value = round((displacement.loc[(i,gps_ref_site_name),'lat'] - N)/lat_step)\n",
    "\n",
    "    ref_disp_insar = insar_displacement[i,\n",
    "                                        ref_y_value-pixel_radius:ref_y_value+1+pixel_radius, \n",
    "                                        ref_x_value-pixel_radius:ref_x_value+1+pixel_radius]\n",
    "    ref_disp_insar = np.nanmean(ref_disp_insar)\n",
    "    insar_displacement[i] -= ref_disp_insar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the data to be validated next. The measurements are in milimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot GNSS stations on InSAR velocity field\n",
    "cmap = copy.copy(plt.get_cmap('RdBu'))\n",
    "#cmap.set_bad(color='black')\n",
    "vmin, vmax = np.nanmin(insar_displacement), np.nanmax(insar_displacement)\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    fig, ax = plt.subplots()\n",
    "    img1 = ax.imshow(insar_displacement[i], cmap=cmap,vmin=vmin,vmax=vmax, interpolation='nearest', extent=(W, E, S, N))\n",
    "    ax.set_title(ifgs_date[i,0].strftime('%Y%m%d')+'-'+ifgs_date[i,1].strftime('%Y%m%d'))\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('LOS displacement [mm]')\n",
    "\n",
    "    for stn in displacement.loc[i].index:\n",
    "        lon,lat = displacement.loc[(i,stn),'lon'],displacement.loc[(i,stn),'lat']\n",
    "        color = cmap((displacement.loc[(i,stn),'gnss_disp']-vmin)/(vmax-vmin))\n",
    "        ax.scatter(lon,lat,s=8**2,color=color,edgecolors='k')\n",
    "        ax.annotate(stn,(lon,lat),color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5 NISAR Validation: GNSS-InSAR Direct Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_disp = {}\n",
    "gnss_disp = {}\n",
    "ddiff_dist = {}\n",
    "ddiff_disp = {}\n",
    "abs_ddiff_disp = {}\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    displacement_i = displacement.loc[i]\n",
    "    insar_disp_i = []\n",
    "    gnss_disp_i = []\n",
    "    ddiff_dist_i = []\n",
    "    ddiff_disp_i = []\n",
    "\n",
    "    for sta1 in displacement_i.index:\n",
    "        for sta2 in displacement_i.index:\n",
    "            if sta2 == sta1:\n",
    "                break\n",
    "            insar_disp_i.append(displacement_i.loc[sta1,'insar_disp']-displacement_i.loc[sta2,'insar_disp'])\n",
    "            gnss_disp_i.append(displacement_i.loc[sta1,'gnss_disp']-displacement_i.loc[sta2,'gnss_disp'])\n",
    "            ddiff_disp_i.append(gnss_disp_i[-1]-insar_disp_i[-1])\n",
    "            g = pyproj.Geod(ellps=\"WGS84\")\n",
    "            _,_,distance = g.inv(displacement_i.loc[sta1,'lon'],displacement_i.loc[sta1,'lat'],\n",
    "                                 displacement_i.loc[sta2,'lon'],displacement_i.loc[sta2,'lat'])\n",
    "            distance = distance/1000 # convert unit from m to km\n",
    "            ddiff_dist_i.append(distance)\n",
    "    insar_disp[i]=np.array(insar_disp_i)\n",
    "    gnss_disp[i]=np.array(gnss_disp_i)\n",
    "    ddiff_dist[i]=np.array(ddiff_dist_i)\n",
    "    ddiff_disp[i]=np.array(ddiff_disp_i)\n",
    "    abs_ddiff_disp[i]=abs(np.array(ddiff_disp_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Compare Displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    plt.figure(figsize=(11,7))\n",
    "    disp_range = (min([*insar_disp[i],*gnss_disp[i]]),max([*insar_disp[i],*gnss_disp[i]]))\n",
    "    plt.hist(insar_disp[i],bins=100,range=disp_range,color = \"green\",label='D_InSAR')\n",
    "    plt.hist(gnss_disp[i],bins=100,range=disp_range,color=\"orange\",label='D_GNSS', alpha=0.5)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Displacements \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')} \\n Number of station pairs used: {len(insar_disp[i])}\")\n",
    "    plt.xlabel('LOS Displacement (mm)')\n",
    "    plt.ylabel('Number of Station Pairs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Plot Displacement Residuals Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    plt.figure(figsize=(11,7))\n",
    "    plt.hist(ddiff_disp[i],bins = 100, color=\"darkblue\",linewidth=1,label='D_gnss - D_InSAR')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Residuals \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')} \\n Number of stations pairs used: {len(ddiff_disp[i])}\")\n",
    "    plt.xlabel('Displacement Residual (mm)')\n",
    "    plt.ylabel('N Stations')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Plot Absolute Displacement Residuals As a Function of Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    dist_th = np.linspace(min(ddiff_dist[i]),max(ddiff_dist[i]),100)\n",
    "    acpt_error = 3*(1+np.sqrt(dist_th))\n",
    "    plt.figure(figsize=(11,7))\n",
    "    plt.scatter(ddiff_dist[i],abs_ddiff_disp[i],s=1)\n",
    "    plt.plot(dist_th, acpt_error, 'r')\n",
    "    plt.xlabel(\"Distance (km)\")\n",
    "    plt.ylabel(\"Amplitude of Displacement Residuals (mm)\")\n",
    "    plt.title(f\"Residuals \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')} \\n Number of stations pairs used: {len(ddiff_dist[i])}\")\n",
    "    plt.legend([\"Mission Reqiurement\",\"Measuement\"])\n",
    "    #plt.xlim(0,5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data used for approach 1:\n",
    "- `ddiff_dist`: distance of GNSS pairs,\n",
    "- `abs_ddiff_disp`: absolute value of measurement redisuals,\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddiff_dist_ap1 = list(ddiff_dist.values())\n",
    "abs_ddiff_disp_ap1 = list(abs_ddiff_disp.values())\n",
    "with open(work_dir/'approach1.pkl','wb') as f:\n",
    "    pickle.dump((ddiff_dist_ap1,abs_ddiff_disp_ap1,ifgs_date_ap1),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This validation (Approach 1) is implemented in `Transient_approach1.ipynb`, where the number of GNSS pairs which meet the mission requirement as a percentage of the total number of GNSS pairs are calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 NISAR Validation: Noise Level Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this validation (Approach #2), we evaluate the estimated secular deformation rate (Requirements 658) or co-seismic displacement (Requirement 660) from time series processing or the individual unwrapped interferogram (Requirement 663) over selected cal/val areas with negligible deformation. Any estimated deformation should thus be treated as noise and our goal is to evaluate the significance of this noise. In general, noise in the modeled displacement or the unwrapped interferogram is anisotropic, but here we neglect this anisotropy. Also, we assume the noise is stationary.\n",
    "\n",
    "We first randomly sample measurements and pair up sampled pixel measurements. For each pixel-pair, the difference of their measurement becomes:\n",
    "$$d\\left(r\\right)=|(f\\left(x\\right)-f\\left(x-r\\right))|$$\n",
    "Estimates of $d(r)$ from all pairs are binned according to the distance r. In each bin, $d(r)$ is assumed to be a normal distribution. We use two methods to validate the noise level as shown in `Transient_approach2.1.ipynb` and `Transient_approach2.2.ipynb`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Now we simply assume there is no deformation in this study area and time interval. But in fact, it is hard to find a enough large area without any deformation. An more realistic solution is to apply a mask to mask out deformed regions. But this may introduce bias for emperical variation estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Mask Pixels with Low Coherence (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ifgs = insar_displacement.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insar_displacement[insar_coherence <0.6] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coherence and InSAR measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('gray')\n",
    "\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.imshow(insar_coherence[i],cmap=cmap, interpolation='nearest',extent=(W, E, S, N))\n",
    "    ax.set_title(f\"Coherence \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('coherence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('RdBu')\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.imshow(insar_displacement[i], cmap=cmap, interpolation='nearest', extent=(W, E, S, N))\n",
    "    ax.set_title(f\"Interferogram \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('LOS displacement [mm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Randomly Sample Pixels and Pair Them Up with Option to Remove Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coordinate for every pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X0,Y0 = load_geo(atr)\n",
    "X0_2d,Y0_2d = np.meshgrid(X0,Y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each interferogram, randomly selected pixels need to be paired up. In order to keep measurements independent, different pixel pairs can not share same pixel. This is achieved by pairing up in sequence, i.e., pairing up pixel number 1 and number 2, 3 and 4..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = []; rel_measure = []\n",
    "for i in range(n_ifgs):\n",
    "    dist_i, rel_measure_i = samp_pair(X0_2d,Y0_2d,insar_displacement[i],num_samples=1000000)\n",
    "    dist.append(dist_i)\n",
    "    rel_measure.append(rel_measure_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the statistical property of selected pixel pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(dist[i], bins=100)\n",
    "    ax.set_title(f\"Histogram of distance \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_xlabel(r'Distance ($km$)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlim(0,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(rel_measure[i], bins=100)\n",
    "    ax.set_title(f\"Histogram of Relative Measurement \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_xlabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist_th = np.linspace(0,50,100)\n",
    "rqmt = 3*(1+np.sqrt(dist_th))\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    ax.plot(dist_th, rqmt, 'r')\n",
    "    ax.scatter(dist[i], rel_measure[i], s=1, alpha=0.25)\n",
    "    ax.set_title(f\"Comparation between Relative Measurement and Requirement Curve \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_ylabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_xlabel('Distance (km)')\n",
    "    ax.set_xlim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data used of approach 2:\n",
    "- `dist`: distance of pixel pairs,\n",
    "- `rel_measure`: relative measurement of pixel pairs,\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(work_dir/'approach2.pkl','wb') as f:\n",
    "    pickle.dump((dist,rel_measure,ifgs_date),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation is implemented in `Transient_approach2.1.ipynb` and `Transient_approach2.2.ipynb`.\n",
    "\n",
    "`Transient_approach2.1.ipynb` counts the number of pixel pairs which meet the mission requirement as a percentage of the total number of pixel pairs selected.\n",
    "\n",
    "`Transient_approach2.2.ipynb` models the noise as random variable with a normal distribution so the standard deviation and its lower confidence bound of this distribution is estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: GPS Position Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(gnss_time_series)):\n",
    "    print(ifgs_date[i,0].strftime('%Y%m%d')+'-'+ifgs_date[i,1].strftime('%Y%m%d'))\n",
    "    for stn in gnss_time_series[i].columns:\n",
    "        series = gnss_time_series[i,str(stn)]\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.title(f\"station name: {stn}\")\n",
    "        plt.scatter(pd.date_range(start=ifgs_date[i,0], end=ifgs_date[i,1]),series)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Displacement (mm)')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
