{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Algorithm Theoretical Basis Document: Algorithms to Validate NISAR L2 Transient Displacement Requirements\n",
    "\n",
    "**Original code authored by:** NISAR Science Team Members and Affiliates  \n",
    "\n",
    "*March 08, 2022*\n",
    "\n",
    "*NISAR Solid Earth Team*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr/>\n",
    "\n",
    "## 1.Transient Deformation\n",
    "Detecting and quantifying transient deformation is essential for improving our understanding of fundamental processes associated with tectonics, subsurface movement of magma, volcanic eruptions, landslides, response to changing surface loads and a wide variety of anthropogenic phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Theoretical Basis of Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Requirements\n",
    "**L2 Requirement 663 â€“ Deformation Transients:** *The NISAR project shall measure at least two components of the point-to-point vector displacements over at least 70% of targeted sites with accuracy of $3(1+ L^{1/2})$ mm or better, over length scales 0.1 km < L < 50 km, at 100 m resolution, and over 12-day time scales.*\n",
    "\n",
    "NISAR must be able to constrain displacements in two look directions (left-looking on both ascending and descending tracks) for the active area of 70% of target sites with an accuracy that scales with baseline distance L between any two locations within a scene. This pertains to all directly measured or inferred 12-day interferograms over the duration of the mission. Here, accuracy is calculated using L in kilometers but with the units removed, at or better than 100 m resolution. The 12-day time scale corresponds to a repeat NISAR pass on each of the ascending and descending satellite tracks, from which interferograms can be made and displacements estimated. The NISAR mission has compiled a list of 2000 global targets covering areas of known or potential transient deformation related to the processes specified in the requirement (NISAR Handbook Appendix H, 2018). These targets include all Earth's active volcanoes above sea-level, areas of rapid glacial mass changes, selected deforming reservoirs of water, oil, gas, CO2 and steam and landslide-prone areas near major population centers, as well as sites where selected disaster-related events have occurred. \n",
    "\n",
    "### 2.2 Approach to validating the L2 requirements\n",
    "We use [two] approaches [in this notebook] for validating the NISAR solid earth L2 requirements. [Both] approaches require the generation of a standard set of NISAR L3 data products consisting of surface displacement for selected areas that sample a range of vegetation types, topographic relief, and strain rates. Generation of these products, as discussed in Section 3, requires a set of temporally contiguous/overlapping SAR interferograms over all time periods of interest (see description of inputs and potential preprocessing steps in Sections 3 and 5).\n",
    "\n",
    "All the Solid Earth Science requirements specify a minimum spatial coverage component, whose validation will rely on a combination of assessing the coverage of InSAR-quality data and ensuring that the required measurement accuracy is achieved in a suite of locations that comprehensively sample different types of regions with respect to surface properties and vegetation land cover. Many of these regions will be automatically evaluated as part of the targeted sites for the transient deformation requirement.\n",
    "\n",
    "#### 2.2.1\tL2 Requirement 663 - Transient Displacements\n",
    "\n",
    "To validate the L2 requirements on transient displacements, we will produce 12-day interferograms from both descending and ascending tracks over diverse target sites where GNSS observations are available. The two components of vector displacement, ascending and descending, will be validated separately.\n",
    "\n",
    "For **Validation Approach 1**, we will use unwrapped interferograms at 100-m resolution to produce point-to-point relative LOS measurements (and their associated uncertainties) between GNSS sites. Position observations from the same set of GNSS sites and at the InSAR acquisition times will be projected into the LOS direction and differenced pairwise. These will be compared to the point-to-point InSAR LOS measurements using a methodology similar to that described in Section 2.2.2., except that the accuracy specification is $3(1+L^{1/2})$ mm over 0.1 km < L < 50 km. \n",
    "\n",
    "For **Validation Approach 2**, we will utilize interferograms over the non-deforming areas discussed in Section 2.2.1. In practice, characterization of transient deformation will usually be improved by examining longer time series of interferograms. The approach described here validates the requirement that short timescale or temporally complex transients can be characterized with a single interferogram. \n",
    "\n",
    "For **Validation Approach 3**, which can be blended with the first two approaches, we use UAVSAR to validate the InSAR-derived motions at scales smaller than the characteristic spacing between the GNSS stations. UAVSAR has the advantage of filling in the spatial sampling between GNSS stations at a resolution approximately 10X higher than the resolution of NISAR images. Ideally, UAVSAR data will be collected during NISAR passes. Corner reflectors will further validate the accuracy and can be used to assess UAVSAR motion compensation errors. Assessment of UAVSAR motion compensation errors should be carried out prior to NISAR launch from experiments using corner reflectors that are moved between passes.\n",
    "\n",
    "Comprehensive validation requires transient sites possessing different deformation characteristics (e.g., volcanoes, landslides, aquifers, hydrocarbons, etc.), vegetation covers (forest, shrub, bare surface, etc.), seasonality (leaf on/off, snow, etc.), and terrain slopes. The NISAR Science Team will select a set of cal/val regions to be used for this requirement and will list those sites in the NISAR cal/val plan.\n",
    "\n",
    "### 2.3.\tTechnical Framework for Validating Requirements\n",
    "#### 2.3.1.\tComparison of GNSS and InSAR measurements\n",
    "The InSAR and GNSS comparisons for Requirement [663] will be performed based on the basis of interferogram by interferogram.\n",
    "\n",
    "#### 2.3.2.\tSpatial Analysis of InSAR scenes\n",
    "Individual interferogram analysis in non-deforming regions will be conducted based on unwrapped interferograms at the required spatial resolutions. We first estimate the covariances or semi-variogram of phase observations between points of varying distances by constructing the structure function (e.g., Lohman & Simons, 2005) (see Section 4.2). We then compare the spatial spectrum of the covariance function to the requirement(s) at distances between 0.1 and 50 km  to validate that the observed noise is smaller than the threshold in the various requirements. We use ensemble statistics over many interferograms and over different terrains and seasons for this validation approach. SES members have done relevant work in the past to validate the NISAR performance tool (Hensley et al., 2016) using the  70-km swath of ALOS interferograms. Before the NISAR launch, we can use ALOS-2 wide-swath or Sentinel-1 scenes to conduct this validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Interferogram Preparation\n",
    "In this initial processing step, all the necessary Level-2 unwrapped interferogram products are gathered. Ascending and descending interferograms will be prepared for independent analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1  Introduction\n",
    "The project will provide sets of ascending and descending unwrapped L2 interferograms over regions of interest listed in the NISAR Solid Earth calval document. For the purpose of testing calval algorithms prior to NISAR launch, the NISAR SE team will make interferograms using SAR data from complementary missions (e.g. Sentinel-1 or ALOS-2). These will include at a minimum nearest-neighbor interferograms.\n",
    "\n",
    "As part of L2 processing, the project will calculate and apply required and optional corrections to minimize errors due to non-geophysical sources. An example of a required correction is the removal of ionospheric propagation delays using split-band processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  Configure Local Processing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Packages\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from mintpy.utils import readfile, utils as ut\n",
    "from mintpy.objects import gps\n",
    "from mintpy.objects.gps import GPS\n",
    "from scipy import special\n",
    "from scipy.stats import binned_statistic\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import pyproj\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "import warnings\n",
    "#from scipy.spatial.distance import pdist\n",
    "#from itertools import combinations,chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Custom Parameters in the cell below. Users only need to input the following parameters:\n",
    "- `calval_location`: name of the study area (e.g., 'oklahoma');\n",
    "- `download_region`: lat-lon box that falls into the study region (e.g., '\"35.00 36.00 -100.00 -98.00\"');\n",
    "- `sentinel_track`: track number of sentinel-1 data (e.g., '135');\n",
    "- `download_start_date` and `download_end_date`: data in this time interval will be downloaded and analysed (e.g., '20000101', '20230101')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: the `download_start_date` and `download_end_date` are set to download all available data by default, users can modified it to select the data they want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calval_location = 'central_valley'\n",
    "# download_region = '\"36.18 36.26 -119.91 -119.77\"' #download box in S,N,W,E format\n",
    "# sentinel_track = '144'\n",
    "\n",
    "# calval_location = 'texas'\n",
    "# download_region = '\"31.00 33.00 -106.00 -103.00\"' #download box in S,N,W,E format\n",
    "# sentinel_track = '151'\n",
    "\n",
    "# calval_location = 'oklahoma'\n",
    "# download_region = '\"35.00 36.00 -100.00 -98.00\"' #download box in S,N,W,E format\n",
    "# sentinel_track = '107'\n",
    "\n",
    "calval_location = 'purtorico'\n",
    "download_region = '\"18.00 18.10 -67.00 -66.00\"' #download box in S,N,W,E format\n",
    "sentinel_track = '135'\n",
    "download_start_date = '20000101'\n",
    "download_end_date = '20220801'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up directories structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calval_dir = Path.cwd()/'calval'\n",
    "\n",
    "#Set Directories\n",
    "print(\"calval directory: \", str(calval_dir))\n",
    "calval_dir.mkdir(exist_ok=True)\n",
    "work_dir = calval_dir/calval_location\n",
    "print(\"Work directory: \", str(work_dir))\n",
    "work_dir.mkdir(exist_ok=True)\n",
    "mint_dir = work_dir/'Mintpy'\n",
    "print(\"Mintpy directory: \", str(mint_dir))\n",
    "mint_dir.mkdir(exist_ok=True)\n",
    "gunw_dir = work_dir/'products'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3  Download Interferogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all interferograms that intersect download_region over specified time range by ARIA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(work_dir)\n",
    "command = 'ariaDownload.py --bbox ' + download_region + ' --start ' + download_start_date + ' --end ' + download_end_date \n",
    "if sentinel_track != '':\n",
    "    command = command + ' --track ' + sentinel_track\n",
    "\n",
    "result = subprocess.run(command,capture_output=True,text=True,shell=True)\n",
    "print(result.stdout)\n",
    "\n",
    "#delete unnecessary files\n",
    "(gunw_dir/\"avg_rates.csv\").unlink(missing_ok=True)\n",
    "(gunw_dir/\"ASFDataDload0.py\").unlink(missing_ok=True)\n",
    "(gunw_dir/\"AvgDlSpeed.png\").unlink(missing_ok=True)\n",
    "(gunw_dir/\"error.log\").unlink(missing_ok=True)\n",
    "(work_dir/\"error.log\").unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up ARIA product and mask data with GSHHS water mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'ariaTSsetup.py -f \"products/*.nc\" --mask Download'\n",
    "result = subprocess.run(command,capture_output=True,text=True,shell=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4  Set Up MintPy Configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series analysis is not needed for validating Transient Requirement. We use Mintpy here only for data loading. The configuration file required by Mintpy needs to be created and written into `mint_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_content = 'mintpy.load.processor = aria\\n'\n",
    "config_file_content += 'mintpy.load.unwFile = ../stack/unwrapStack.vrt\\n'\n",
    "config_file_content += 'mintpy.load.corFile = ../stack/cohStack.vrt\\n'\n",
    "config_file_content += 'mintpy.load.connCompFile = ../stack/connCompStack.vrt\\n'\n",
    "config_file_content += 'mintpy.load.demFile = ../DEM/SRTM_3arcsec.dem\\n'\n",
    "config_file_content += 'mintpy.load.incAngleFile = ../incidenceAngle/*.vrt\\n'\n",
    "config_file_content += 'mintpy.load.azAngleFile = ../azimuthAngle/*.vrt\\n'\n",
    "config_file_content += 'mintpy.load.waterMaskFile = ../mask/watermask.msk\\n'\n",
    "config_file_content += 'mintpy.reference.lalo = auto\\n'\n",
    "config_file_content += 'mintpy.topographicResidual.pixelwiseGeometry = no\\n'\n",
    "config_file_content += 'mintpy.troposphericDelay.method = no\\n'\n",
    "config_file_content += 'mintpy.topographicResidual = no\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = mint_dir/(calval_location+'.cfg')\n",
    "config_file.write_text(config_file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5  Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(mint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep load_data'\n",
    "result = subprocess.run(command,capture_output=True,text=True,shell=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The output of this step is an \"inputs\" directory containing two HDF5 files:\n",
    "- ifgramStack.h5: This file contains 6 dataset cubes (e.g. unwrapped phase, coherence, connected components etc.) and multiple metadata\n",
    "- geometryGeo.h5: This file contains geometrical datasets (e.g., incidence/azimuth angle, masks, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_file = mint_dir/'inputs/ifgramStack.h5'\n",
    "geom_file = mint_dir/'inputs/geometryGeo.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the interferogram has a resolution lower than 100 m, we need multi-look the interferogram phase values before calculating the empirical semivarigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the date of interferograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_date = readfile.read(ifgs_file,datasetName='date')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ifgs_date = np.empty_like(ifgs_date,dtype=dt)\n",
    "for i in range(ifgs_date.shape[0]):\n",
    "    start_date = str(int(ifgs_date[i,0]))\n",
    "    end_date = str(int(ifgs_date[i,1]))\n",
    "    start_date = dt.strptime(start_date, \"%Y%m%d\")\n",
    "    end_date = dt.strptime(end_date, \"%Y%m%d\")\n",
    "    _ifgs_date[i] = [start_date,end_date]\n",
    "    \n",
    "ifgs_date = _ifgs_date\n",
    "del _ifgs_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove interferograms with time interval other than 12 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_row_index = []\n",
    "for i in range(ifgs_date.shape[0]):\n",
    "    time_interval = (ifgs_date[i,1]-ifgs_date[i,0]).days\n",
    "    if time_interval != 12:\n",
    "        del_row_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_date = np.delete(ifgs_date,del_row_index,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify independent interferograms (i.e., selected inteferograms do NOT share common dates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_row_index = []\n",
    "i = 0\n",
    "while i<ifgs_date.shape[0]-1:\n",
    "    if ifgs_date[i,1]==ifgs_date[i+1,0]:\n",
    "        del_row_index.append(i+1)\n",
    "        i = i+2\n",
    "    else:\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_date = np.delete(ifgs_date,del_row_index,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the phase and coherence of selected interferograms, geometrical datasets, and attribution of them are loaded into numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrapPhaseName = ['unwrapPhase-'+i[0].strftime('%Y%m%d')+'_'+i[1].strftime('%Y%m%d') for i in ifgs_date]\n",
    "coherenceName = ['coherence-'+i[0].strftime('%Y%m%d')+'_'+i[1].strftime('%Y%m%d') for i in ifgs_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_unw,atr = readfile.read(ifgs_file,datasetName=unwrapPhaseName)\n",
    "insar_displacement = -ifgs_unw*float(atr['WAVELENGTH'])/(4*np.pi)*1000 # unit in mm\n",
    "\n",
    "insar_coherence = readfile.read(ifgs_file,datasetName=coherenceName)[0]\n",
    "del ifgs_unw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change default missing phase values in interferograms from 0.0 to `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_displacement[insar_displacement==0.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Optional interferograms correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Phase distortions related to solid earth and ocean tidal effects as well as those due to temporal variations in the vertical stratification of the atmosphere can be mitigated using the approaches described below. At this point, it is expected that these corrections will not be needed to validate the mission requirements, but they may be used to produce the highest quality data products. Typically, these are applied to the estimated time series product rather than to the individual interferograms, since they are a function of the time of each radar acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.A Solid Earth Tide Correction\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.B Tropospheric Delay Correction\n",
    "Optional atmospheric correction utilizes the PyAPS (Jolivet et al., 2011, Jolivet and Agram, 2012) module within GIAnT (or eventually a merged replacement for GIAnT and MintPy). PyAPS is well documented, maintained and can be freely downloaded. PyAPS is included in GIAnT distribution). PyAPS currently includes support for ECMWFâ€™s ERA-Interim, NOAAâ€™s NARR and NASAâ€™s MERRA weather models. A final selection of atmospheric models to be used for operational NISAR processing will be done during Phase C.\n",
    "\n",
    "[T]ropospheric delay maps are produced from atmospheric data provided by Global Atmospheric Models. This method aims to correct differential atmospheric delay correlated with the topography in interferometric phase measurements. Global Atmospheric Models (hereafter GAMs)... provide estimates of the air temperature, the atmospheric pressure and the humidity as a function of elevation on a coarse resolution latitude/longitude grid. In PyAPS, we use this 3D distribution of atmospheric variables to determine the atmospheric phase delay on each pixel of each interferogram.\n",
    "\n",
    "The absolute atmospheric delay is computed at each SAR acquisition date. For a pixel a_i at an elevation z at acquisition date i, the four surrounding grid points are selected and the delays for their respective elevations are computed. The resulting delay at the pixel a_i is then the bilinear interpolation between the delays at the four grid points. Finally, we combine the absolute delay maps of the InSAR partner images to produce the differential delay maps used to correct the interferograms.\n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.C Topographic Residual Correction \n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Phase deramping is not appplied here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the solid earth tide correction for interferogram is applied, it should also be applied for GNSS observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary summary: we have load all data we need for processing:\n",
    "- `atr`: metadata, including incident angle, longitude and latitude step width, etc;\n",
    "- `insar_displacement`: LOS measurement from InSAR;\n",
    "- `insar_coherence`: coherence value of the interferograms:\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prevent reruning the above preparing again, we save the data into disk. They can be loaded easily next time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(work_dir/'base.pkl','wb') as f:\n",
    "    pickle.dump((atr,insar_displacement,insar_coherence,ifgs_date),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(work_dir/'base.pkl','rb') as f:\n",
    "    atr,insar_displacement,insar_coherence,ifgs_date = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Make GNSS LOS Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Find Collocated GNSS Stations\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSFâ€™s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSFâ€™s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get space and time range for searching GNSS station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length, width = int(atr['LENGTH']), int(atr['WIDTH'])\n",
    "lat_step = float(atr['Y_STEP'])\n",
    "lon_step = float(atr['X_STEP'])\n",
    "N = float(atr['Y_FIRST'])\n",
    "W = float(atr['X_FIRST'])\n",
    "S = N+lat_step*(length-1)\n",
    "E = W+lon_step*(width-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_gnss = ifgs_date[0,0]\n",
    "end_date_gnss = ifgs_date[-1,-1]\n",
    "\n",
    "inc_angle = int(float(atr.get('incidenceAngle', None)))\n",
    "az_angle = int(float(atr.get('azimuthAngle', None))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for collocated GNSS stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_names, site_lats, site_lons = gps.search_gps(SNWE=(S,N,W,E), \n",
    "                                                  start_date=start_date_gnss.strftime('%Y%m%d'),\n",
    "                                                  end_date=end_date_gnss.strftime('%Y%m%d'))\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Make GNSS LOS Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the 3-D GNSS observations are projected into LOS direction. The InSAR observations are averaged 3 by 3 near the station positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** the number of pixels used in calculating the averaged phase values at the GPS location depends on the resolution of input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get daily position solutions for GNSS stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "displacement = {}\n",
    "gnss_time_series = {}\n",
    "gnss_time_series_std = {}\n",
    "bad_stn = {}  #stations to toss\n",
    "pixel_radius = 1   #number of InSAR pixels to average for comparison with GNSS\n",
    "\n",
    "for counter,stn in enumerate(site_names):\n",
    "    gps_obj = GPS(site = stn, data_dir = str(mint_dir/'GPS'))\n",
    "    gps_obj.open()\n",
    "        \n",
    "    # count number of dates in time range\n",
    "    gps_obj.read_displacement\n",
    "    dates = gps_obj.dates\n",
    "    for i in range(insar_displacement.shape[0]):\n",
    "        start_date = ifgs_date[i,0]\n",
    "        end_date = ifgs_date[i,-1]\n",
    "        \n",
    "        range_days = (end_date - start_date).days\n",
    "        gnss_count = np.histogram(dates, bins=[start_date,end_date])\n",
    "        gnss_count = int(gnss_count[0])\n",
    "        #print(gnss_count)\n",
    "\n",
    "        # select GNSS stations based on data completeness, here we hope to select stations with data frequency of 1 day and no interruption\n",
    "        if range_days == gnss_count-1:\n",
    "        #if start_date in dates and end_date in dates:\n",
    "            _,disp_gnss_time_series,disp_gnss_time_series_std,site_latlon,_ = gps_obj.read_gps_los_displacement(atr,\n",
    "                                                                                                                    start_date=start_date.strftime('%Y%m%d'),\n",
    "                                                                                                                    end_date=end_date.strftime('%Y%m%d'))\n",
    "            x_value = round((site_latlon[1] - W)/lon_step)\n",
    "            y_value = round((site_latlon[0] - N)/lat_step)\n",
    "\n",
    "            #displacement from insar observation in the gnss station, averaged\n",
    "            #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "            disp_insar = insar_displacement[i,\n",
    "                                            y_value-pixel_radius:y_value+pixel_radius, \n",
    "                                            x_value-pixel_radius:x_value+pixel_radius]\n",
    "            if np.isfinite(disp_insar).sum() == 0:\n",
    "                break\n",
    "            disp_insar = np.nanmean(disp_insar)\n",
    "\n",
    "            disp_gnss_time_series = disp_gnss_time_series*1000 # convert unit from meter to mm\n",
    "            disp_gnss_time_series_std = disp_gnss_time_series_std*1000\n",
    "            gnss_time_series[(i,stn)] = disp_gnss_time_series\n",
    "            gnss_time_series_std[(i,stn)] = disp_gnss_time_series_std\n",
    "            displacement[(i,stn)] = list(site_latlon)\n",
    "            disp_gnss = disp_gnss_time_series[-1] - disp_gnss_time_series[0]\n",
    "\n",
    "            displacement[(i,stn)].append(disp_gnss)\n",
    "            displacement[(i,stn)].append(disp_insar)\n",
    "        else:\n",
    "            try:\n",
    "                bad_stn[i].append(stn)\n",
    "            except:\n",
    "                bad_stn[i] = [stn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some data structure transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series = dict(sorted(gnss_time_series.items()))\n",
    "gnss_time_series_std = dict(sorted(gnss_time_series_std.items()))\n",
    "displacement = dict(sorted(displacement.items()))\n",
    "bad_stn = dict(sorted(bad_stn.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series = pd.DataFrame.from_dict(gnss_time_series)\n",
    "gnss_time_series_std = pd.DataFrame.from_dict(gnss_time_series_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement = pd.DataFrame.from_dict(displacement,orient='index',\n",
    "                                      columns=['lat','lon','gnss_disp','insar_disp'])\n",
    "displacement.index = pd.MultiIndex.from_tuples(displacement.index,names=['ifg index','station'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are less than 3 GNSS stations, don't conduct comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_index = []\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    if len(displacement.loc[i]) < 3:\n",
    "        drop_index.append(i)\n",
    "displacement=displacement.drop(drop_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "- `Mintpy get_los_displacement()` only use the central incidence angle which may involve noticable bias for large area. We next will use pixel-dependent look vector.\n",
    "- A more general critterion is needed for GNSS station selection. Here the stations with uninterrupted data are selected while, in Secular Requirement Validation, stations are selected by data completeness and standard variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Make GNSS and InSAR Relative Displacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I select ref site randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference GNSS stations to GNSS reference site\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    gps_ref_site_name = random.choice(displacement.loc[i].index.unique())\n",
    "    displacement.loc[i,'gnss_disp'] = displacement.loc[i,'gnss_disp'].values - displacement.loc[(i,gps_ref_site_name),'gnss_disp']\n",
    "    displacement.loc[i,'insar_disp'] = displacement.loc[i,'insar_disp'].values - displacement.loc[(i,gps_ref_site_name),'insar_disp']\n",
    "    ref_x_value = round((displacement.loc[(i,gps_ref_site_name),'lon'] - W)/lon_step)\n",
    "    ref_y_value = round((displacement.loc[(i,gps_ref_site_name),'lat'] - N)/lat_step)\n",
    "\n",
    "    ref_disp_insar = insar_displacement[i,\n",
    "                                        ref_y_value-pixel_radius:ref_y_value+1+pixel_radius, \n",
    "                                        ref_x_value-pixel_radius:ref_x_value+1+pixel_radius]\n",
    "    ref_disp_insar = np.nanmean(ref_disp_insar)\n",
    "    insar_displacement[i] -= ref_disp_insar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the data to be validated next. The measurements are in milimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot GNSS stations on InSAR velocity field\n",
    "cmap = copy.copy(plt.get_cmap('RdBu'))\n",
    "#cmap.set_bad(color='black')\n",
    "vmin, vmax = np.nanmin(insar_displacement), np.nanmax(insar_displacement)\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    fig, ax = plt.subplots()\n",
    "    img1 = ax.imshow(insar_displacement[i], cmap=cmap,vmin=vmin,vmax=vmax, interpolation='nearest', extent=(W, E, S, N))\n",
    "    ax.set_title(ifgs_date[i,0].strftime('%Y%m%d')+'-'+ifgs_date[i,1].strftime('%Y%m%d'))\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('LOS displacement [mm]')\n",
    "\n",
    "    for stn in displacement.loc[i].index:\n",
    "        lon,lat = displacement.loc[(i,stn),'lon'],displacement.loc[(i,stn),'lat']\n",
    "        color = cmap((displacement.loc[(i,stn),'gnss_disp']-vmin)/(vmax-vmin))\n",
    "        ax.scatter(lon,lat,s=8**2,color=color,edgecolors='k')\n",
    "        ax.annotate(stn,(lon,lat),color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5 NISAR Validation: GNSS-InSAR Direct Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_disp = {}\n",
    "gnss_disp = {}\n",
    "ddiff_dist = {}\n",
    "ddiff_disp = {}\n",
    "abs_ddiff_disp = {}\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    displacement_i = displacement.loc[i]\n",
    "    insar_disp_i = []\n",
    "    gnss_disp_i = []\n",
    "    ddiff_dist_i = []\n",
    "    ddiff_disp_i = []\n",
    "\n",
    "    for sta1 in displacement_i.index:\n",
    "        for sta2 in displacement_i.index:\n",
    "            if sta2 == sta1:\n",
    "                break\n",
    "            insar_disp_i.append(displacement_i.loc[sta1,'insar_disp']-displacement_i.loc[sta2,'insar_disp'])\n",
    "            gnss_disp_i.append(displacement_i.loc[sta1,'gnss_disp']-displacement_i.loc[sta2,'gnss_disp'])\n",
    "            ddiff_disp_i.append(gnss_disp_i[-1]-insar_disp_i[-1])\n",
    "            g = pyproj.Geod(ellps=\"WGS84\")\n",
    "            _,_,distance = g.inv(displacement_i.loc[sta1,'lon'],displacement_i.loc[sta1,'lat'],\n",
    "                                 displacement_i.loc[sta2,'lon'],displacement_i.loc[sta2,'lat'])\n",
    "            distance = distance/1000 # convert unit from m to km\n",
    "            ddiff_dist_i.append(distance)\n",
    "    insar_disp[i]=np.array(insar_disp_i)\n",
    "    gnss_disp[i]=np.array(gnss_disp_i)\n",
    "    ddiff_dist[i]=np.array(ddiff_dist_i)\n",
    "    ddiff_disp[i]=np.array(ddiff_disp_i)\n",
    "    abs_ddiff_disp[i]=abs(np.array(ddiff_disp_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Compare Displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    plt.figure(figsize=(11,7))\n",
    "    disp_range = (min([*insar_disp[i],*gnss_disp[i]]),max([*insar_disp[i],*gnss_disp[i]]))\n",
    "    plt.hist(insar_disp[i],bins=100,range=disp_range,color = \"green\",label='D_InSAR')\n",
    "    plt.hist(gnss_disp[i],bins=100,range=disp_range,color=\"orange\",label='D_GNSS', alpha=0.5)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Displacements \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')} \\n Number of station pairs used: {len(insar_disp[i])}\")\n",
    "    plt.xlabel('LOS Displacement (mm)')\n",
    "    plt.ylabel('Number of Station Pairs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Plot Displacement Residuals Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    plt.figure(figsize=(11,7))\n",
    "    plt.hist(ddiff_disp[i],bins = 100, color=\"darkblue\",linewidth=1,label='D_gnss - D_InSAR')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Residuals \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')} \\n Number of stations pairs used: {len(ddiff_disp[i])}\")\n",
    "    plt.xlabel('Displacement Residual (mm)')\n",
    "    plt.ylabel('N Stations')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Plot Absolute Displacement Residuals As a Function of Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    dist_th = np.linspace(min(ddiff_dist[i]),max(ddiff_dist[i]),100)\n",
    "    acpt_error = 3*(1+np.sqrt(dist_th))\n",
    "    plt.figure(figsize=(11,7))\n",
    "    plt.scatter(ddiff_dist[i],abs_ddiff_disp[i],s=1)\n",
    "    plt.plot(dist_th, acpt_error, 'r')\n",
    "    plt.xlabel(\"Distance (km)\")\n",
    "    plt.ylabel(\"Amplitude of Displacement Residuals (mm)\")\n",
    "    plt.title(f\"Residuals \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')} \\n Number of stations pairs used: {len(ddiff_dist[i])}\")\n",
    "    plt.legend([\"Mission Reqiurement\",\"Measuement\"])\n",
    "    #plt.xlim(0,5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data used for approach 1:\n",
    "- `ddiff_dist`: distance of GNSS pairs,\n",
    "- `abs_ddiff_disp`: absolute value of measurement redisuals,\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(work_dir/'approach1.pkl','wb') as f:\n",
    "    pickle.dump((ddiff_dist,abs_ddiff_disp,ifgs_date),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation approach 1 is implemented in `Transient_approach1.ipynb`.\n",
    "\n",
    "In that notebook, the number of GNSS pairs which meet the mission requirement as a percentage of the total number of GNSS pairs are calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 NISAR Validation: Noise Level Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Now we simply assume there is no deformation in this study area and time interval. But in fact, it is hard to find a enough large area without any deformation. An more realistic solution is to apply a mask to mask out deformed regions. But this may introduce bias for emperical variation estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_geo(attr_geo) -> np.array:\n",
    "    \"\"\"This program calculate the coordinate of the geocoded files \n",
    "    and perform coordinate transformation from longitude and latitude to local coordinate in kilometers.\n",
    "    \n",
    "    Parameters:\n",
    "    ######\n",
    "    geo_attr:attribute of the geocoded data\n",
    "\n",
    "    Returns:\n",
    "    ######\n",
    "    X:coordinates in east direction in km\n",
    "    Y:coordinates in north direction in km\n",
    "    \"\"\"\n",
    "    \n",
    "    X0=float(attr_geo['X_FIRST'])\n",
    "    Y0=float(attr_geo['Y_FIRST'])\n",
    "    X_step=float(attr_geo['X_STEP'])\n",
    "    Y_step=float(attr_geo['Y_STEP'])\n",
    "    length=int(attr_geo['LENGTH'])\n",
    "    width=int(attr_geo['WIDTH'])\n",
    "    \n",
    "    # \n",
    "    Y_local_end_ix = math.floor(length/2)\n",
    "    Y_local_first_ix = -(length-Y_local_end_ix-1)\n",
    "    X_local_end_ix = math.floor(width/2)\n",
    "    X_local_first_ix = -(width-X_local_end_ix-1)\n",
    "    \n",
    "    Y_origin = Y0+Y_step*(-Y_local_first_ix)\n",
    "    \n",
    "    X_step_local = math.cos(math.radians(Y_origin))*X_step*111.1\n",
    "    Y_step_local = Y_step*111.1\n",
    "    \n",
    "\n",
    "    X=np.linspace(X_local_first_ix*X_step_local,X_local_end_ix*X_step_local,width)\n",
    "    Y=np.linspace(Y_local_first_ix*Y_step_local,Y_local_end_ix*Y_step_local,length)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "def rand_samp(data:np.ndarray,X:np.ndarray,Y:np.ndarray,num_samples:int=10000):\n",
    "    \"\"\"\n",
    "    This function select points used for calculation of structure function using a random point sampling \n",
    "    \n",
    "    Parameters:\n",
    "    data: np.ndarray\n",
    "        input data array\n",
    "    X: np.ndarray\n",
    "        input X location of the data points\n",
    "    Y: np.ndarray\n",
    "        input Y location of the data points\n",
    "    num_samples: int\n",
    "        number of points to be sampled\n",
    "    percent_samples: float\n",
    "        percentage of points to be samples\n",
    "        \n",
    "    Returns: \n",
    "    tuple [np.ndarray, np.ndarray, np.ndarray]\n",
    "        (data, X, Y)\n",
    "    \"\"\"\n",
    "    length=np.size(data)\n",
    "    if length<num_samples:\n",
    "        n_points=length\n",
    "        warnings.warn(f'Using all data points: {n_points}')\n",
    "    else:\n",
    "        n_points=num_samples\n",
    "        \n",
    "    ind=np.random.choice(length,n_points,replace=False)\n",
    "    sampled_data=data[ind]\n",
    "    sampled_X=X[ind]\n",
    "    sampled_Y=Y[ind]\n",
    "\n",
    "    return sampled_data, sampled_X, sampled_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Mask Pixels with Low Coherence (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insar_displacement[insar_coherence <0.6] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coherence and InSAR measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ifgs = insar_displacement.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('gray')\n",
    "\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.imshow(insar_coherence[i],cmap=cmap, interpolation='nearest',extent=(W, E, S, N))\n",
    "    ax.set_title(f\"Coherence \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('coherence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('RdBu')\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.imshow(insar_displacement[i], cmap=cmap, interpolation='nearest', extent=(W, E, S, N))\n",
    "    ax.set_title(f\"Interferogram \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('LOS displacement [mm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coordinate for every pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X0,Y0 = load_geo(atr)\n",
    "X0_2d,Y0_2d = np.meshgrid(X0,Y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape data to 1d and remove value of `nan`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    mask = np.isnan(insar_displacement[i])\n",
    "    data_i = insar_displacement[i][~mask]\n",
    "    X_i = X0_2d[~mask]\n",
    "    Y_i = Y0_2d[~mask]\n",
    "    data.append(data_i)\n",
    "    X.append(X_i)\n",
    "    Y.append(Y_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Remove Trend (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(n_ifgs):\n",
    "#    data[i]=variogram.remove_trend(X[i],Y[i],data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Randomly Sample Pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    data[i],X[i],Y[i]=rand_samp(data[i],X[i],Y[i],num_samples=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Pair up sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each interferogram, randomly selected pixels need to be paired up. In order to keep measurements independent, different pixel pairs can not share same pixel. This is achieved by pairing up in sequence, i.e., pairing up pixel number 1 and number 2, 3 and 4..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = [] # distance\n",
    "rel_measure = [] # relative measurement\n",
    "for i in range(n_ifgs):\n",
    "    x_odd = X[i][1::2]\n",
    "    y_odd = Y[i][1::2]\n",
    "    data_odd = data[i][1::2]\n",
    "    if (X[i].shape[0] % 2) == 0:\n",
    "        x_even = X[i][0::2]\n",
    "        y_even = Y[i][0::2]\n",
    "        data_even = data[i][0::2]\n",
    "    else:\n",
    "        x_even = X[i][0:-1:2]\n",
    "        y_even = Y[i][0:-1:2]\n",
    "        data_even = data[i][0:-1:2]\n",
    "        \n",
    "    distance = np.sqrt((x_odd-x_even)**2+(y_odd-y_even)**2)\n",
    "    rel_measure_i = abs(data_odd-data_even)\n",
    "    \n",
    "    dist.append(distance)\n",
    "    rel_measure.append(rel_measure_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dist = [] # distance\n",
    "# rel_measure = [] # relative measurement\n",
    "# for i in range(n_ifgs):\n",
    "#     x = X[i]\n",
    "#     y = Y[i]\n",
    "#     data_i = data[i]\n",
    "#     xy = np.stack((x,y),axis=-1)\n",
    "#     distance = pdist(xy)\n",
    "#     index_iter = chain.from_iterable(combinations(range(len(x)),2))\n",
    "#     index = np.fromiter(index_iter,int).reshape(-1,2)\n",
    "#     rel_measure_i = abs(data_i[index[:,0]] - data_i[index[:,1]])\n",
    "    \n",
    "#     dist.append(distance)\n",
    "#     rel_measure.append(rel_measure_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the statistical property of selected pixel pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(dist[i], bins=100)\n",
    "    ax.set_title(f\"Histogram of distance \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_xlabel(r'Distance ($km$)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlim(0,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(rel_measure[i], bins=100)\n",
    "    ax.set_title(f\"Histogram of Relative Measurement \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_xlabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist_th = np.linspace(0,50,100)\n",
    "rqmt = 3*(1+np.sqrt(dist_th))\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    ax.plot(dist_th, rqmt, 'r')\n",
    "    ax.scatter(dist[i], rel_measure[i], s=1, alpha=0.25)\n",
    "    ax.set_title(f\"Comparation between Relative Measurement and Requirement Curve \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_ylabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_xlabel('Distance (km)')\n",
    "    ax.set_xlim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data used of approach 2:\n",
    "- `dist`: distance of pixel pairs,\n",
    "- `rel_measure`: relative measurement of pixel pairs,\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(work_dir/'approach2.pkl','wb') as f:\n",
    "    pickle.dump((dist,rel_measure,ifgs_date),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation is implemented in `Transient_approach2.1.ipynb` and `Transient_approach2.2.ipynb`.\n",
    "\n",
    "`Transient_approach2.1.ipynb` counts the number of pixel pairs which meet the mission requirement as a percentage of the total number of pixel pairs selected.\n",
    "\n",
    "`Transient_approach2.2.ipynb` models the noise as random variable with a normal distribution so the standard deviation and its lower confidence bound of this distribution is estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: GPS Position Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(gnss_time_series)):\n",
    "    print(ifgs_date[i,0].strftime('%Y%m%d')+'-'+ifgs_date[i,1].strftime('%Y%m%d'))\n",
    "    for stn in gnss_time_series[i].columns:\n",
    "        series = gnss_time_series[i,str(stn)]\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.title(f\"station name: {stn}\")\n",
    "        plt.scatter(pd.date_range(start=ifgs_date[i,0], end=ifgs_date[i,1]),series)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Displacement (mm)')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
